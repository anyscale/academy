{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - RecSys: Recommender System\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This section explores one approach for using *reinforcement learning* with [Ray RLlib](https://rllib.io/) to build a [*recommender system*](https://en.wikipedia.org/wiki/Recommender_system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the GitHub public repo for this code is available at <https://github.com/anyscale/academy/blob/main/ray-rllib/recsys> and full source code for this example recommender system is also in the `recsys.py` script. You can run that with default settings to exercise the code:\n",
    "\n",
    "```shell\n",
    "python recsys.py\n",
    "```\n",
    "\n",
    "To see the available command line options use:\n",
    "\n",
    "```shell\n",
    "python recsys.py --help\n",
    "```\n",
    "\n",
    "A full run takes about 5-10 minutes on a recent model MacBook Pro laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training Data\n",
    "\n",
    "The approach given here for building a recommender system could be applied to just about any dataset where *users* are *rating* a set of *items*. It's bounded in terms of memory requirements as the number of users grows. It could be re-engineered to handle a very large set of items.\n",
    "\n",
    "We'll be using the [Jester collaborative filtering dataset](https://goldberg.berkeley.edu/jester-data/) – which is known for having a high *density*, i.e., where many users have rated many of the items: https://goldberg.berkeley.edu/jester-data/.\n",
    "\n",
    "Jester was an online joke recommender hosted a UC Berkeley which collected data from April 1999 through May 2003. See the discussion of \"universal queries\" in:\n",
    "\n",
    "> \"Eigentaste: A Constant Time Collaborative Filtering Algorithm\"  \n",
    "Ken Goldberg, Theresa Roeder, Dhruv Gupta, Chris Perkins  \n",
    "*Information Retrieval*, 4(2), 133-151 (July 2001)  \n",
    "<https://goldberg.berkeley.edu/pubs/eigentaste.pdf>\n",
    "\n",
    "The data is split into three downloadable files, and the first file contains anonymous ratings from 24,983 users who have rated 36 or more jokes.\n",
    "\n",
    "Ratings data is organized as a matrix with dimensions `24983 X 101`\n",
    "\n",
    "  * one row per user\n",
    "  * first column gives the number of jokes rated by that user\n",
    "  * the next 100 columns give the ratings for jokes `01` through `100`\n",
    "  * ratings are real values ranging from `-10.00` to `+10.00`\n",
    "  * the value `\"99\"` corresponds to `None` = \"not rated\"\n",
    "  \n",
    "Here's a function to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "NO_RATING = \"99\"\n",
    "MAX_RATING = 10.0\n",
    "\n",
    "def load_data (data_path):\n",
    "    rows = []\n",
    "\n",
    "    with open(data_path, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
    "\n",
    "        for row in csvreader:\n",
    "            conv = [None] * (len(row) - 1)\n",
    "\n",
    "            for i in range(1, len(row)):\n",
    "                if row[i] != NO_RATING:\n",
    "                    rating = float(row[i]) / MAX_RATING\n",
    "                    conv[i - 1] = rating\n",
    "\n",
    "            rows.append(conv)\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Discovery\n",
    "\n",
    "Let's load the dataset into a `pandas` dataframe, then take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(os.getcwd()) / Path(\"jester-data-1.csv\")\n",
    "sample = load_data(DATA_PATH)\n",
    "\n",
    "df = pd.DataFrame(sample)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above how there are many `NaN` values in the dataset.\n",
    "Those are *missing values*.\n",
    "Before we analyze this data any further, we'll need to [*impute*](https://en.wikipedia.org/wiki/Imputation_(statistics)) those missing values.\n",
    "In other words, substitute each missing value with some appropriate estimate, such as a column mean.\n",
    "The `SimpleImputer` class in `scikit-learn` provides a good way to handle that requirement:\n",
    "<https://scikit-learn.org/stable/modules/impute.html>.\n",
    "\n",
    "We'll save the imputed sample data in the `X` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imp.fit(df.values)\n",
    "\n",
    "X = imp.transform(df.values).T\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use *K-means clustering* as an unsupervised learning approach to understand more about how the dataset is structured. In other words, let's re-arrange the dataset into `k` clusters of items, based on how many different users have rated them.\n",
    "\n",
    "Since `k` is a *hyperparameter* we'll need some way to select an appropriate value for it. One approach that's popular for K-means is to evaluate the *inertia* of decreasing error in the clustering models as `k` grows from zero to some limit.\n",
    "\n",
    " * <https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html>\n",
    " * <https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c>\n",
    " \n",
    "We really don't want to work with more than a few clusters – since many small clusters would lead to less *predictive power* in the _recsys_. Given that there are 100 items, let's measure inertia up to `k=20` clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "distortions = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "    km = KMeans(\n",
    "        n_clusters=i,\n",
    "        init=\"random\",\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        tol=1e-04,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the curve of inertia, using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, 20), distortions, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of cluster analysis has stochastic aspects, so results may differ on different runs. Generally, the plot shows a \"knee\" in the curve near `k=7` as the decrease in error begins to level out. That's a reasonable number of clusters, such that each cluster will tend to have ~14% of the items. That choice has an inherent trade-off:\n",
    "\n",
    "  * too few clusters → poor predictions (less precision)\n",
    "  * too many clusters → poor predictive power (less recall)\n",
    "\n",
    "Now we can run K-means in `scikit-learn` with that hyperparameter `k=7` to get the clusters that we'll use in our RL environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_CLUSTERS = 7\n",
    "\n",
    "km = KMeans(n_clusters=K_CLUSTERS)\n",
    "km.fit(X)\n",
    "\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this analysis we'll need the *labels* for each item, i.e., the number of the cluster to which each item gets assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labels = km.labels_\n",
    "clusters = defaultdict(set)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    clusters[labels[i]].add(i)\n",
    "\n",
    "CLUSTERS = dict(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also use the cluster *centers*, which provide the vector positions (the per-dimension \"means\") for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = km.cluster_centers_\n",
    "CENTERS = centers.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW, let's take a look at the K clusters from this analysis…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    X[y_km == 0, 0], X[y_km == 0, 1],\n",
    "    s=50, c=\"lightgreen\",\n",
    "    marker=\"s\", edgecolor=\"black\",\n",
    "    label=\"cluster 0\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 1, 0], X[y_km == 1, 1],\n",
    "    s=50, c=\"orange\",\n",
    "    marker=\"o\", edgecolor=\"black\",\n",
    "    label=\"cluster 1\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 2, 0], X[y_km == 2, 1],\n",
    "    s=50, c=\"lightblue\",\n",
    "    marker=\"v\", edgecolor=\"black\",\n",
    "    label=\"cluster 2\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 3, 0], X[y_km == 3, 1],\n",
    "    s=50, c=\"blue\",\n",
    "    marker=\"^\", edgecolor=\"black\",\n",
    "    label=\"cluster 3\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 4, 0], X[y_km == 4, 1],\n",
    "    s=50, c=\"yellow\",\n",
    "    marker=\"<\", edgecolor=\"black\",\n",
    "    label=\"cluster 4\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 5, 0], X[y_km == 5, 1],\n",
    "    s=50, c=\"purple\",\n",
    "    marker=\">\", edgecolor=\"black\",\n",
    "    label=\"cluster 5\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X[y_km == 6, 0], X[y_km == 6, 1],\n",
    "    s=50, c=\"brown\",\n",
    "    marker=\"X\", edgecolor=\"black\",\n",
    "    label=\"cluster 6\"\n",
    ")\n",
    "\n",
    "# plot the centroids\n",
    "plt.scatter(\n",
    "    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "    s=250, marker=\"*\",\n",
    "    c=\"red\", edgecolor=\"black\",\n",
    "    label=\"centers\"\n",
    ")\n",
    "\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, based on the centers these clusters show some separation – at least when we plot in 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thesis for Building an RL Recommender System\n",
    "\n",
    "We're using a thesis here that people tend to like similar \"kinds\" of items, in this case jokes. While we could go to extra effort to build a classifier model for jokes, it's much simpler to measure ratings for them across a large sample of users' likes. In a classic [*collaborative filtering*](https://en.wikipedia.org/wiki/Collaborative_filtering) approach, we could base our recommendations on other users who have similar patterns of rating items. \n",
    "\n",
    "On the one hand, that may be expensive to scale as the number of users grows. Instead, an [*item-based collaborative filter*](https://en.wikipedia.org/wiki/Item-item_collaborative_filtering) approach which was invented by [Amazon](https://ieeexplore.ieee.org/document/1167344) for \"people who bought x also bought y\". This scales to massive datasets by clustering the items based on user ratings.\n",
    "\n",
    "On the other hand, people change their interests over time but a class CF model doesn't account for that.  For a thorough study of these effects, see [\"From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews\"](https://arxiv.org/abs/1303.4402) by Julian McAuley and Jure Leskovec (2013-03-18).\n",
    "\n",
    "Let's use reinforcement learning to build an item-based recommender system that can \"evolve\" its recommendations for each individual over time. We'll leverage *exploration* (gather info that leads toward better recommendations in the future) versus *exploitation* (use the best decision given the info known so far). We have clustered the items (jokes), so now we can create a custom environment where:\n",
    "\n",
    "  * *actions* taken by an agent – recommend items from specific clusters\n",
    "  * *observation space* – a vector distance from the user's rating history to the cluster centers\n",
    "  * *rewards* – user ratings for recommended items, with minor penalties for unrated items\n",
    "  \n",
    "Each *episode* for an agent will simulate one user's ratings vs. which jokes get recommdened based on our *policy* which is trained using RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other point: a *dense sub-matrix* – in which almost all users have rated the jokes – is defined by the following columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_SUBMATRIX = [ 5, 7, 8, 13, 15, 16, 17, 18, 19, 20 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One well-known issue encountered in production use of recommenders is the [*cold start*](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems)) problem. When we first begin to make recommendations for a given user, we have zero information about them. So it's not possible to provide personalized recommendations. However, we have the dense sub-matrix above, which almost all users have rated. We can sample from this subset – as \"warm\" items – to provide to each user at the start of an *episode*. That will give the recommender some initial information to leverage as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Custom Gym Environment\n",
    "\n",
    "Next we'll build a custom environment for training a policy with RLlib.  We'll use OpenAI [Gym](https://gym.openai.com/) as the basis for it. For details about how to build custom Gym environments for working with RLlib, see [\"Anatomy of a custom environment for RLlib\"](https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5) and its accompanying source code in the <https://github.com/DerwenAI/gym_example> Git repo. Some details are also covered in the _Explore RLlib_ lesson, [Custom Environments and Reward Shaping](../explore-rllib/03-Custom-Environments-Reward-Shaping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define a few globals… `MAX_STEPS` is the length of an episode, based on how many jokes can be recommended before repeating. Reward values are either the user's rating for the recommended item, or minor penalties for recommending either a \"depleted\" cluster or an item which the user never rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_LENGTH = 100\n",
    "MAX_STEPS = ROW_LENGTH - len(DENSE_SUBMATRIX)\n",
    "\n",
    "REWARD_DEPLETED = -0.1\t# item recommended from a depleted cluster (no-op)\n",
    "REWARD_UNRATED = -0.05\t# item was never rated by this user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the Gym environment. This is one class and should be defined within one cell – albeit this is a long cell to scroll through…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "\n",
    "class JokeRec (gym.Env):\n",
    "    def __init__ (self, config):\n",
    "        # NB: here we're passing strings via config; RLlib use of JSON\n",
    "        # parser was throwing exceptions due to config values\n",
    "        self.dense = eval(config[\"dense\"])\n",
    "        self.centers = eval(config[\"centers\"])\n",
    "        self.clusters = eval(config[\"clusters\"])\n",
    "\n",
    "        lo = np.array([np.float64(-1.0)] * K_CLUSTERS)\n",
    "        hi = np.array([np.float64(1.0)] * K_CLUSTERS)\n",
    "\n",
    "        self.observation_space = spaces.Box(lo, hi, shape=(K_CLUSTERS,), dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(K_CLUSTERS)\n",
    "\n",
    "        # load the dataset\n",
    "        self.dataset = load_data(config[\"dataset\"])\n",
    "\n",
    "\n",
    "    def _warm_start (self):\n",
    "        \"\"\"\n",
    "        attempt a warm start, sampling half the dense submatrix of most-rated items\n",
    "        \"\"\"\n",
    "        sample_size = round(len(self.dense) / 2.0)\n",
    "\n",
    "        for action, items in self.clusters.items():\n",
    "            for item in random.sample(self.dense, sample_size):\n",
    "                if item in items:\n",
    "                    state, reward, done, info = self.step(action)\n",
    "\n",
    "\n",
    "    def _get_state (self):\n",
    "        \"\"\"\n",
    "        calculate root-mean-square (i.e., normalized vector distance) for the agent's current \n",
    "        \"distance\" measure from each cluster center as the observation\n",
    "        \"\"\"\n",
    "        n = float(len(self.used))\n",
    "\n",
    "        if n > 0.0:\n",
    "            state = [ np.sqrt(x / n) for x in self.coords ]\n",
    "        else:\n",
    "            state = self.coords\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        \"\"\"\n",
    "        reset the item recommendation history, select a new user to simulate from among the \n",
    "        dataset rows, then run an initial 'warm-start' sequence of steps before handing step\n",
    "        control back to the agent\n",
    "        \"\"\"\n",
    "        self.count = 0\n",
    "        self.used = []\n",
    "        self.depleted = 0\n",
    "        self.coords = [np.float64(0.0)] * K_CLUSTERS\n",
    "\n",
    "        # select a random user to simulate\n",
    "        self.data_row = random.choice(self.dataset)\n",
    "\n",
    "        # attempt a warm start\n",
    "        self._warm_start()\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        \"\"\"\n",
    "        recommend one item, which may result in a no-op -- \n",
    "        in production, skip any repeated items per user\n",
    "        \"\"\"\n",
    "        assert action in self.action_space, action\n",
    "        assert_info = \"c[item] {}, rating {}, scaled_diff {}\"\n",
    "\n",
    "        # enumerate items from the cluster selected by the action that haven't been recommended\n",
    "        # previously to the simulated user\n",
    "        items = set(self.clusters[action]).difference(set(self.used))\n",
    "\n",
    "        if len(items) < 1:\n",
    "            # oops! items from the selected cluster have been depleted, i.e. all have been \n",
    "            # recommended previously to the simulated user; hopefully the agent will learn\n",
    "            # to switch to exploring among the other clusters\n",
    "            self.depleted += 1\n",
    "            item = None\n",
    "            reward = REWARD_DEPLETED\n",
    "        else:\n",
    "            # chose an item at random from the selected cluster\n",
    "            item = random.choice(list(items))\n",
    "            rating = self.data_row[item]\n",
    "\n",
    "            if not rating:\n",
    "                # no-op! this action resulted in an unrated item\n",
    "                reward = REWARD_UNRATED\n",
    "\n",
    "            else:\n",
    "                # success! this action resulted in an item rated by the simulated user\n",
    "                reward = rating\n",
    "                self.used.append(item)\n",
    "\n",
    "                # update the coords history: agent observes its distance to each cluster \"evolve\"\n",
    "                for i in range(len(self.coords)):\n",
    "                    c = self.centers[i]\n",
    "                    scaled_diff = abs(c[item] - rating) / 2.0\n",
    "                    self.coords[i] += scaled_diff ** 2.0\n",
    "\n",
    "        self.count += 1\n",
    "        done = self.count >= MAX_STEPS\n",
    "        info = { \"item\": item, \"count\": self.count, \"depleted\": self.depleted }\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "\n",
    "    def render (self, mode=\"human\"):\n",
    "        last_used = self.used[-10:]\n",
    "        last_used.reverse()\n",
    "        print(\">> used:\", last_used)\n",
    "        print(\">> dist:\", [round(x, 2) for x in self._get_state()])\n",
    "        print(\">> depl:\", self.depleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `JokeRec.step()` method contains the heart of the simulation logic here. It determines the item to be recommend based on the input `action`, then determines the `reward` and the agent's updated vector distance to the cluster centers as the `observation space`.\n",
    "\n",
    "Overall, this approach is relatively well-behaved and bounded for its memory use as the number of items grows. We're keeping most of the data in memory, but could readily use a distributed key/value store for selecting items, maintaining user history, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Now we'll use the results of K-means clustering on the data sample to prepare a configuration for our custom environment.\n",
    "\n",
    "We'll use [*proximal policy optimization*](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=ppo#proximal-policy-optimization-ppo) (PPO) for training a policy in RLlib, taking the default PPO configuration as the foundation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "CONFIG = ppo.DEFAULT_CONFIG.copy()\n",
    "\n",
    "CONFIG[\"log_level\"] = \"WARN\"\n",
    "CONFIG[\"num_workers\"] = 3    # set to `0` for debug\n",
    "\n",
    "CONFIG[\"env_config\"] = {\n",
    "    \"dataset\": DATA_PATH,\n",
    "    \"dense\": str(DENSE_SUBMATRIX),\n",
    "    \"clusters\": repr(CLUSTERS),\n",
    "    \"centers\": repr(CENTERS),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an instance of our custom Gym environment and call the `reset()` method, which will initialize an episode (simulating one user's ratings), run a \"warm start\", then return the initial observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JokeRec(CONFIG[\"env_config\"])\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly select an action, i.e., the label of an item cluster to recommend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(\"action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use that action to take one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action)\n",
    "print(\"obs:\", state)\n",
    "print(\"reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `render()` method to describe more about the agent's state within its environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there are already some other \"used\" items, due to the warm start.\n",
    "\n",
    "Next, we'll define a function that runs through one entire episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_episode (env, naive=False, verbose=False):\n",
    "    \"\"\"\n",
    "    step through one episode, using either a naive strategy or random actions\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    sum_reward = 0\n",
    "\n",
    "    action = None\n",
    "    avoid_actions = set([])\n",
    "    depleted = 0\n",
    "\n",
    "    for i in range(MAX_STEPS):\n",
    "        if not naive or not action:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"action:\", action)\n",
    "            print(\"obs:\", i, state, reward, done, info)\n",
    "\n",
    "        # naive strategy: select items from the nearest non-depleted cluster\n",
    "        if naive:\n",
    "            if info[\"depleted\"] > depleted:\n",
    "                depleted = info[\"depleted\"]\n",
    "                avoid_actions.add(action)\n",
    "\n",
    "            obs = []\n",
    "\n",
    "            for a in range(len(state)):\n",
    "                if a not in avoid_actions:\n",
    "                    dist = round(state[a], 2)\n",
    "                    obs.append([dist, a])\n",
    "\n",
    "            action = min(obs)[1]\n",
    "\n",
    "        sum_reward += reward\n",
    "\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(\"DONE @ step {}\".format(i))\n",
    "\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"CUMULATIVE REWARD: \", round(sum_reward, 3))\n",
    "\n",
    "    return sum_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, another function which uses `run_one_episode()` to measure the baseline performance of a \"naïve\" strategy, i.e., without use of reinforcement learning to train a policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def measure_baseline (env, n_iter=1, naive=False, verbose=False):\n",
    "    history = []\n",
    "\n",
    "    for episode in tqdm(range(n_iter), ascii=True, desc=\"measure baseline\"):\n",
    "        sum_reward = run_one_episode(env, naive=naive, verbose=verbose)\n",
    "        history.append(sum_reward)\n",
    "\n",
    "    baseline = sum(history) / len(history)\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this to measure how well our recommender system runs without leveraging RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = measure_baseline(env, n_iter=1000, naive=True)\n",
    "print(\"BASELINE CUMULATIVE REWARD\", round(baseline, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Ray and RLlib\n",
    "\n",
    "At this point we're ready to train a policy with RLlib. First we'll initialize the directory in which to save *checkpoints*, and the directory in which to log results…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "CHECKPOINT_ROOT = \"tmp/rec\"\n",
    "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
    "\n",
    "ray_results = \"{}/ray_results/\".format(os.getenv(\"HOME\"))\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start Ray, register our custom environment, and create an agent. BTW, if you see lots of \"deprecation\" warnings from [Tensorflow](https://www.tensorflow.org/) just ignore those…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register our environment so we can reference it by name. Then create a `PPOTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "env_key = \"JokeRec-v0\"\n",
    "register_env(env_key, lambda config_env: JokeRec(config_env))\n",
    "AGENT = ppo.PPOTrainer(CONFIG, env=env_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a policy using the PPO optimizer in RLlib. As the following code steps through each training iteration, watch how the measured improvements in the min, mean, and max rewards per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_ITER = 20\n",
    "\n",
    "df = pd.DataFrame(columns=[ \"min_reward\", \"avg_reward\", \"max_reward\", \"steps\", \"checkpoint\"])\n",
    "status = \"reward {:6.2f} {:6.2f} {:6.2f}  len {:4.2f}  saved {}\"\n",
    "\n",
    "for i in range(TRAIN_ITER):\n",
    "    result = AGENT.train()\n",
    "    checkpoint_file = AGENT.save(CHECKPOINT_ROOT)\n",
    "\n",
    "    row = [\n",
    "        result[\"episode_reward_min\"],\n",
    "        result[\"episode_reward_mean\"],\n",
    "        result[\"episode_reward_max\"],\n",
    "        result[\"episode_len_mean\"],\n",
    "        checkpoint_file,\n",
    "        ]\n",
    "\n",
    "    df.loc[len(df)] = row\n",
    "    print(status.format(*row))\n",
    "    \n",
    "BEST_CHECKPOINT = checkpoint_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout to Emulate a Use Case Deployment\n",
    "\n",
    "Now let's define a function to run a *rollout* using a checkpointed policy.\n",
    "Each rollout iteration will emuluate a deployed use of our recommender system for one user, and we'll measure the average rewards across many iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rollout (agent, env, n_iter=1, verbose=False):\n",
    "    \"\"\"\n",
    "    iterate through `n_iter` episodes in a rollout to emulate deployment in a production use case\n",
    "    \"\"\"\n",
    "    for episode in range(n_iter):\n",
    "        state = env.reset()\n",
    "        sum_reward = 0\n",
    "\n",
    "        for step in range(MAX_STEPS):\n",
    "            try:\n",
    "                action = agent.compute_action(state)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                sum_reward += reward\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"reward {:6.3f}  sum {:6.3f}\".format(reward, sum_reward))\n",
    "                    env.render()\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "\n",
    "            if done:\n",
    "                # report at the end of each episode\n",
    "                print(\"CUMULATIVE REWARD:\", round(sum_reward, 3), \"\\n\")\n",
    "                yield sum_reward\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the best trained policy in a rollout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AGENT.restore(BEST_CHECKPOINT)\n",
    "history = []\n",
    "\n",
    "for episode_reward in run_rollout(AGENT, env, n_iter=500, verbose=False):\n",
    "    history.append(episode_reward)\n",
    "    \n",
    "print(\"average reward:\", round(sum(history) / len(history), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reported *average reward* from many rollouts (using RLlib to train a policy) compare with the *baseline cumulative reward* above based on a naïve strategy and no learning?  How does it compare with the predicted *mean reward per episode* from training? \n",
    "\n",
    "The baseline reward from a naïve strategy should be much lower (worse user ratings) than the other two measures.\n",
    "\n",
    "These measures are an estimate for how a user would rate their recommended items.\n",
    "Of course, not all users will like the jokes, so there will be some rollouts with negative rewards.\n",
    "Overall we want the average reward to be *positive*, with `MAX_STEPS` as an upper bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the trained policy\n",
    "\n",
    "Use the following code to examine the trained policy that was optimized using PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = AGENT.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "print(\"\\n\", model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate learning with TensorBoard\n",
    "\n",
    "You also can run [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize the RL training metrics from the log files. The results during training were written to a directory under `$HOME/ray_results`\n",
    "\n",
    "If you are viewing this lesson on the Anyscale hosted platform, use the provided link to open TensorBoard.\n",
    "\n",
    "If you are viewing this lesson on a laptop, open a terminal and run the following command, then open the URL shown in the output. (You can open a terminal using the `+` in the upper left-hand corner of Jupyter Lab.)\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```\n",
    "\n",
    "Open the URL printed to view the TensorBoard GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Compare use of the other datasets `\"jester-data-2.csv\"` and `\"jester-data-3.csv\"` by substituting them during the rollout.\n",
    "\n",
    "How do the mean cumulative reward differ from the metrics in the lesson?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Compare the effect of using a larger `K` value for the number of clusters.\n",
    "\n",
    "Show the difference, if any, by comparing:\n",
    "\n",
    "  * baseline with random actions \n",
    "  * baseline with the naïve strategy\n",
    "  * predicted average reward from training\n",
    "  * stats from the rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "  1. In what ways could the \"warm start\" be improved?\n",
    "  2. How could this code be modified to scale to millions of users?  Or to thousands of items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Finally, let's shutdown Ray gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
