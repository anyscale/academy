{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune - Understanding Hyperparameter Tuning\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the [Ray RLlib](../ray-rllib/00-Ray-RLlib-Overview.ipynb) lessons used [Ray Tune](http://tune.io) to train policies. This meant we trained _parameters_ that defined the policies. Now we'll learn that Ray Tune was actually designed to determine the best _hyperparameters_ for the problem, before training to determine the _parameters_.\n",
    "\n",
    "This lesson introduces the concepts of _Hyperparameter Tuning or Optimization_ (HPO) and works through a nontrivial example using Tune. \n",
    "\n",
    "See also the [Hyperparameter Tuning References](References-Hyperparameter-Tuning.ipynb) notebook and the [Tune documentation](http://tune.io), in particular, the [API reference](https://docs.ray.io/en/latest/tune/api_docs/overview.html). \n",
    "\n",
    "A [Ray Summit Connect](https://anyscale.com/blog/videos-and-slides-for-the-second-ray-summit-connect-june-17-2020/) talk by the creator of Tune, Richard Liaw, provides an excellent overview of the challenges of hyperparameter tuning and how Tune addresses these challenges. Another recent webinar [Fast and efficient hyperparameter tuning with Ray Tune](https://www.anyscale.com/events/2021/10/20/fast-scalable-hyperparameter-tuning-ray-tune) complements the discussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Hyperparameters?\n",
    "\n",
    "In _supervised learning_, we train a model with labeled data so the model can properly label new data values. Everything about the model is defined by a set of _parameters_, such as the weights in a linear regression. \n",
    "\n",
    "In contrast, the _hyperparameters_<sup>1</sup> define structural details about the kind of model itself, like whether or not we are using a linear regression or what architecture is best for a neural network, etc. Other quantities considered hyperparameters include learning rates, discount rates, etc. If we want our training process and resulting model to work well, we first need to determine the optimal or near-optimal set of hyperparameters.\n",
    "\n",
    "How do we determine the optimal hyperparameters? The most straightfoward approach is to perform a loop where we pick a candidate set of values from some reasonably inclusive list of possible values, train a model, compare the results achieved with previous loop iterations, and pick the set that performed best. This process is called _Hyperparameter Tuning_ or _Optimization_ (HPO).\n",
    "\n",
    "This simple algorithm can quickly become very expensive, however. Training a single neural networks can be compute intensive and the space of all possible architectures is huge. Hence, much of the research in hyperparameter tuning, especially for neural networks, focuses on ways to optimize HPO, such as early stopping and pruning the search space when some combinations appear to perform poorly.\n",
    "\n",
    "1. _Hyperparameter_ is often spelled _hyper parameter_ or _hyper-parameter_, but we'll use the spelling with no space or dash.\n",
    "\n",
    "![](../images/tune/what-are-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: $k$-Means \n",
    "\n",
    "Let's start with a very simple example of HPO, finding $k$ in $k$-means. \n",
    "\n",
    "The $k$-means algorithm finds clusters in a data set. It's a canonical example of _unsupervised learning_, where information is extracted from a data set, rather than using labeled data to train a model for labelling new data, as in _supervised learning_. We won't discuss the algorithm details, but the essense of it involves a \"guess\" for the expected number of clusters, the $k$ value, then calculating $k$ centroids (the coordinates at the center), one per cluster, along with determining to which cluster each data point belongs. The details are in [$k$-means Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering). The following animation shows the algorithm in action for a two-dimensional data set where three clusters are evident.\n",
    "\n",
    "![K-Means Convergence](../images/tune/K-means_convergence.gif)\n",
    "\n",
    "(source: [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering). [Larger Image](https://en.wikipedia.org/wiki/K-means_clustering#/media/File:K-means_convergence.gif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is easy to see the clusters in this two-dimensional data set, that won't be for arbitrary datasets, especially those with more than three dimensions. Hence, we should determine the best $k$ value by trying many values and picking the value that appears to be best. In this case, \"best\" would mean that we minimize the distances between the datapoints and centroids. \n",
    "\n",
    "With just one hyperparameter, this problem is comparatively simple and brute force calculations to find the optimal $k$ is usually good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO for Neural Networks\n",
    "\n",
    "Where HPO really becomes a challenge is finding the right neural network architecture for your problem. Why are neural networks a challenge? Consider this image of a typical architecture:\n",
    "\n",
    "![Typical Neural Network](../images/tune/hpo-neural-network-example.png)\n",
    "\n",
    "Every number you see is a hyperparameter! So are the decisions about how many layers to have, what kind of layer to use for each layer, etc. The space of possible hyperparameters is enormous, too big to explore naively.\n",
    "\n",
    "So called _neural architecture search_ (NAS) has become a research field in its own right, along with general research in optimizing HPO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ray Tune\n",
    "\n",
    "[Ray Tune](http://tune.io) is the Ray-based native library for hyperparameter tuning. Tune makes it nearly as easy to run distributed, parallelized HPO as it is to run trials on a single machine manually, one after the other. \n",
    "\n",
    "Tune is built as an extensible, pluggable framework, with built-in integrations for many frameworks, including [OpenAI Gym environments](https://gym.openai.com/envs/), [PyTorch](https://pytorch.org), [TensorFlow](http://tensorflow.org), and recently, [sci-kit learn](https://scikit-learn.org/stable/) (see [this recent blog post](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf)).\n",
    "\n",
    "## How Tune Works\n",
    "\n",
    "Before we get into using Tune, let's understand the some definitions, terms, and components. With this understanding, you will get an insight into what happens when you use Tune to search your hyperparameter space and optimize your process to select the best, optimized model.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "After a short preview of the [Tune basic steps and concepts](01-Ray-Tune-Warmup.ipynb) in our warm up tutorial, let's get an intuition of what those terms and steps mean.  \n",
    "\n",
    "#### Trainable\n",
    "This is your training function, with an objective function. As [trainable](https://docs.ray.io/en/latest/tune/api_docs/trainable.html?highlight=trainable#ray.tune.Trainable), it's one of the argument to `tune.run(...)` method. Tune offers two iterface APIs for trainable: functional and class.\n",
    "\n",
    "\n",
    "#### Trial\n",
    "\n",
    "A trial is an execution or run of a logical representation of a single hyperparameter configuration. Each trial is associated with an instance of a Trainable. And a collection of trials comprise an experiment.\n",
    "\n",
    "#### Lifecycle of a trial¶\n",
    "A trial’s life cycle consists of 6 stages:\n",
    "\n",
    "Initialization (generation): A trial is first generated as a hyperparameter sample, and its parameters are configured according to what was provided in `tune.run` as part of the `config` arggument. Trials are then placed into a queue to be executed (with status PENDING).\n",
    "\n",
    "**PENDING**: A pending trial is a trial to be executed on the machine. Every trial is configured with resource values. Whenever the trial’s resource values are available, tune will run the trial (by starting a ray actor holding the config and the training function).\n",
    "\n",
    "**RUNNING**: A running trial is assigned a Ray Actor. There can be multiple running trials in parallel.\n",
    "\n",
    "**ERRORED**: If a running trial throws an exception, Tune will catch that exception and mark the trial as errored. Note that exceptions can be propagated from an actor to the main Tune driver process. If `max_retries` is set, Tune will set the trial back into “PENDING” and later start it from the last checkpoint.\n",
    "\n",
    "**TERMINATED**: A trial is terminated if it is stopped or finished by a Stopper/Scheduler. If using the Function API, the trial is also terminated when the function stops.\n",
    "\n",
    "**PAUSED**: A trial can be paused by a Trial scheduler. This means that the trial’s actor will be stopped too. A paused trial can later be resumed from the most recent checkpoint.\n",
    "\n",
    "\n",
    "#### Driver/worker process\n",
    "\n",
    "The driver process is the python process that calls `tune.run` (which calls ray.init() underneath the hood); therefore, you\n",
    "do not need to invoke `ray.init(...)` explicity. Tune does it for you during its inital run. The Tune's driver process runs on the node where you run your script (which calls `tune.run`), while Ray Tune trainable “actors” run on any node (either on the same node on multiple cores) or on worker nodes (with multiple cores on a distributed Ray cluster).\n",
    "\n",
    "#### Ray Actors\n",
    "\n",
    "Tune uses Ray Actors as worker node's processes to evaluate multiple Trainables in parallel.\n",
    "\n",
    "[Ray Actors](https://docs.ray.io/en/latest/actors.html#actor-guide) allow you to parallelize an instance of a class in Python. When you instantiate a class that is a Ray actor, Ray will start a instance of that class on a separate process either on the same machine (or another distributed machine, if running a Ray cluster). This actor can then asynchronously execute method calls and maintain its own internal state.\n",
    "\n",
    "### The execution of a trainable¶\n",
    "Tune uses Ray actors to parallelize the evaluation of multiple hyperparameter configurations. Each actor is a Python process that executes an instance of the user-provided Trainable. The definition of the user-provided Trainable will be [serialized via cloudpickle](https://docs.ray.io/en/latest/serialization.html#serialization-guide) and sent to each actor process. Each Ray actor will start an instance of the Trainable to be executed.\n",
    "\n",
    "If the Trainable is a class, it will be executed iteratively by calling train/step. After each invocation, the driver is notified that a “result dict” is ready. The driver will then pull the result via `ray.get`.\n",
    "\n",
    "If the trainable is a callable or a function, it will be executed on the Ray actor process on a separate execution thread. Whenever `tune.report` is called, the execution thread is paused and waits for the driver to pull a result. After pulling, the actor’s execution thread will automatically resume.\n",
    "\n",
    "The diagram below depicts how Tune launches trainables on the worker nodes as processes in which the the trainables are run. \n",
    "Each trial will have its own instance of a trainable, hence we parallelize trials and its respective configuration across cores on a worker. \n",
    "\n",
    "![](../images/ray_tune_report_launch_trainables.png)\n",
    "\n",
    "Whenever the trainble calls `tune.report`, the driver will pull the metrics via `ray.get`, as shown in the diagram below.\n",
    "\n",
    "![](../images/ray_tune_report_metrics.png)\n",
    "\n",
    "\n",
    "Tune also integrates implementations of many state-of-the-art [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html) and [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html), so it is easy to optimize your HPO process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:3px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Ray Tune and RLlib\n",
    "\n",
    "We used Ray Tune in several of the reinforcement learning lessons, where we actually didn't optimize any hyperparameters; we just used it to drive the RLlib training process. Now we'll see an example of HPO with Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to make sure that Ray is running and we want to initialize Ray in this \"driver\" program explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start Ray in this \"driver\" program (notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:26:49,460\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-17_16-26-47_171239_15391/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-17_16-26-47_171239_15391/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2022-03-17_16-26-47_171239_15391',\n",
       " 'metrics_export_port': 64707,\n",
       " 'gcs_address': '127.0.0.1:64143',\n",
       " 'address': '127.0.0.1:64143',\n",
       " 'node_id': '5c55c9776d3465f37a4be525460f004226a5dd2dd3632f29fdbd9a06'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our [RLlib Simple Multi-Armed Bandit](../ray-rllib/multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb) lesson, we used Tune to train RLlib, but not train hyperparameters. \n",
    "\n",
    "> **NOTE**: If you have not gone through the RLlib tutorial yet, you may skip the rest of this tutorital, for it takes about 5-6 minutes. The message to take away here is that Tune provides many search algorithms and schedulers that can be specified as arguments. The next three tutoirals shows you how.\n",
    "\n",
    "Now we'll use Tune to train hyperparameters. We'll use the same experimental environment we used in our [RLlib `CartPole` lesson](../ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb). \n",
    "\n",
    "> **Aside:** In case you haven't gone through the [Ray RLlib tutorial](../ray-rllib/00-Ray-RLlib-Overview.ipynb), [CartPole](https://gym.openai.com/envs/CartPole-v1/) is an [OpenAI Gym](https://gym.openai.com/envs/) environment that simulates a cart that moves left or right while attempting to balance a vertical pole. A _policy_ is trained to optimize changing the velocity up and down to control right and left movement with the goal of keeping the pole balanced for as long as possible.\n",
    "\n",
    "The most important hyperparameters for `CartPole` are for the neural network used to learn how to balance the pole. A simple network suffices. We'll use a fully-connected network with two hidden layers, but use Tune to find an optimal choice for the sizes of the layers. To keep the computation tractable for our purposes, we'll just pick sizes from the list of 20 and 40. \n",
    "\n",
    "It's probably that some specific number between these 10 and 100 is optimal for one layer and a different number is optimal for the other layer. However, the computation required for trying all possible combinations is $O(n^2)$. You should consider trying more numbers if you don't mind waiting!\n",
    "\n",
    "However, the two we've chosen are good enough as we'll see. We'll consider what \"good enough\" means when we look at the results.\n",
    "\n",
    "The next cell runs Tune for this purpose. The comments explain what each argument does. We'll do four tries, one for each combination of the two possible values for the two hidden layers.\n",
    "\n",
    "> **Note:** `tune.run` will handle Ray initialization for us, if it isn't already initialized. To force Tune to throw an error instead, pass the argument `ray_auto_init=False`.\n",
    "\n",
    "The next cell will take 5-6 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-17 16:30:15 (running for 00:03:20.01)<br>Memory usage on this node: 17.3/32.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/12.7 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/jules/ray_results/PPO<br>Number of trials: 4/4 (4 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:30:15,790\tINFO tune.py:639 -- Total run time: 200.78 seconds (199.99 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",                                    # Use proximal policy optimization to train \n",
    "    stop={\"episode_reward_mean\": 400},        # Stopping criteria, when average reward over the episodes\n",
    "                                              # of training equals 400 out of a maximum possible 500 score.\n",
    "    config={\n",
    "        \"env\": \"CartPole-v1\",                 # Tune can associate this string with the environment.\n",
    "        \"num_gpus\": 0,                        # If you have GPUs, go for it!\n",
    "        \"num_workers\": 6,                     # Number of Ray workers to use; Use one LESS than \n",
    "                                              # the number of cores you want to use (or omit this argument)!\n",
    "        \"model\": {                            # The NN model we'll optimize.\n",
    "            'fcnet_hiddens': [                # \"Fully-connected network with N hidden layers\".\n",
    "                tune.grid_search([20, 40]),   # Try these four values for layer one.\n",
    "                tune.grid_search([20, 40])    # Try these four values for layer two.\n",
    "            ]\n",
    "        },\n",
    "    },\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "First, how long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 196.33 seconds,    3.27 minutes\n"
     ]
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one performed best based on our stopping criteria, with mode modes of \"max\" or \"min?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': 'CartPole-v1',\n",
       " 'num_gpus': 0,\n",
       " 'num_workers': 6,\n",
       " 'model': {'fcnet_hiddens': [40, 20]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_config(metric=\"episode_reward_mean\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': 'CartPole-v1',\n",
       " 'num_gpus': 0,\n",
       " 'num_workers': 6,\n",
       " 'model': {'fcnet_hiddens': [40, 40]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_config(metric=\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the smallest combination is good enough, even the best according to this metric! \n",
    "\n",
    "Let's look at the data for all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>timesteps_this_iter</th>\n",
       "      <th>agent_timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/default_policy/learner_stats/vf_loss</th>\n",
       "      <th>info/learner/default_policy/learner_stats/vf_explained_var</th>\n",
       "      <th>info/learner/default_policy/learner_stats/kl</th>\n",
       "      <th>info/learner/default_policy/learner_stats/entropy</th>\n",
       "      <th>info/learner/default_policy/learner_stats/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/model</th>\n",
       "      <th>config/num_gpus</th>\n",
       "      <th>config/num_workers</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>405.42</td>\n",
       "      <td>405.42</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>87912</td>\n",
       "      <td>7992</td>\n",
       "      <td>87912</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>3235.0166</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.546697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>{'fcnet_hiddens': [20, 20]}</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/Users/jules/ray_results/PPO/PPO_CartPole-v1_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>401.36</td>\n",
       "      <td>401.36</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>79920</td>\n",
       "      <td>7992</td>\n",
       "      <td>79920</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>3109.3748</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.512160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>{'fcnet_hiddens': [40, 20]}</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/Users/jules/ray_results/PPO/PPO_CartPole-v1_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>401.39</td>\n",
       "      <td>401.39</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>87912</td>\n",
       "      <td>7992</td>\n",
       "      <td>87912</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1644.8188</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.007992</td>\n",
       "      <td>0.551561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>{'fcnet_hiddens': [20, 40]}</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/Users/jules/ray_results/PPO/PPO_CartPole-v1_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>407.76</td>\n",
       "      <td>407.76</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>87912</td>\n",
       "      <td>7992</td>\n",
       "      <td>87912</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.4314</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>{'fcnet_hiddens': [40, 40]}</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>/Users/jules/ray_results/PPO/PPO_CartPole-v1_b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0               500.0               129.0               405.42   \n",
       "1               500.0               132.0               401.36   \n",
       "2               500.0                55.0               401.39   \n",
       "3               500.0                70.0               407.76   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0            405.42                  19                    6            87912   \n",
       "1            401.36                  20                    6            79920   \n",
       "2            401.39                  19                    6            87912   \n",
       "3            407.76                  17                    6            87912   \n",
       "\n",
       "   timesteps_this_iter  agent_timesteps_total  done  ...  \\\n",
       "0                 7992                  87912  True  ...   \n",
       "1                 7992                  79920  True  ...   \n",
       "2                 7992                  87912  True  ...   \n",
       "3                 7992                  87912  True  ...   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/vf_loss  \\\n",
       "0                                          3235.0166   \n",
       "1                                          3109.3748   \n",
       "2                                          1644.8188   \n",
       "3                                          1797.4314   \n",
       "\n",
       "   info/learner/default_policy/learner_stats/vf_explained_var  \\\n",
       "0                                           0.000062            \n",
       "1                                           0.000017            \n",
       "2                                           0.005182            \n",
       "3                                           0.000356            \n",
       "\n",
       "  info/learner/default_policy/learner_stats/kl  \\\n",
       "0                                     0.002596   \n",
       "1                                     0.005365   \n",
       "2                                     0.007992   \n",
       "3                                     0.002107   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/entropy  \\\n",
       "0                                          0.546697   \n",
       "1                                          0.512160   \n",
       "2                                          0.551561   \n",
       "3                                          0.496262   \n",
       "\n",
       "  info/learner/default_policy/learner_stats/entropy_coeff   config/env  \\\n",
       "0                                                0.0       CartPole-v1   \n",
       "1                                                0.0       CartPole-v1   \n",
       "2                                                0.0       CartPole-v1   \n",
       "3                                                0.0       CartPole-v1   \n",
       "\n",
       "                  config/model  config/num_gpus  config/num_workers  \\\n",
       "0  {'fcnet_hiddens': [20, 20]}                0                   6   \n",
       "1  {'fcnet_hiddens': [40, 20]}                0                   6   \n",
       "2  {'fcnet_hiddens': [20, 40]}                0                   6   \n",
       "3  {'fcnet_hiddens': [40, 40]}                0                   6   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/jules/ray_results/PPO/PPO_CartPole-v1_b...  \n",
       "1  /Users/jules/ray_results/PPO/PPO_CartPole-v1_b...  \n",
       "2  /Users/jules/ray_results/PPO/PPO_CartPole-v1_b...  \n",
       "3  /Users/jules/ray_results/PPO/PPO_CartPole-v1_b...  \n",
       "\n",
       "[4 rows x 59 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, all four combinations actually work about equally well, as far as the `episode_reward_mean` mean is concerned. So what's actually best? What's \"good enough\" in this case?\n",
    "\n",
    "It's useful to consider the `training_iteration` (roughly proportional to `episodes_total`) and `timesteps_total`. Let's project out the most interesting data so we can see it more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>config/model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>401.36</td>\n",
       "      <td>10</td>\n",
       "      <td>79920</td>\n",
       "      <td>{'fcnet_hiddens': [40, 20]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>405.42</td>\n",
       "      <td>11</td>\n",
       "      <td>87912</td>\n",
       "      <td>{'fcnet_hiddens': [20, 20]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>401.39</td>\n",
       "      <td>11</td>\n",
       "      <td>87912</td>\n",
       "      <td>{'fcnet_hiddens': [20, 40]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>407.76</td>\n",
       "      <td>11</td>\n",
       "      <td>87912</td>\n",
       "      <td>{'fcnet_hiddens': [40, 40]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_mean  training_iteration  timesteps_total  \\\n",
       "1               401.36                  10            79920   \n",
       "0               405.42                  11            87912   \n",
       "2               401.39                  11            87912   \n",
       "3               407.76                  11            87912   \n",
       "\n",
       "                  config/model  \n",
       "1  {'fcnet_hiddens': [40, 20]}  \n",
       "0  {'fcnet_hiddens': [20, 20]}  \n",
       "2  {'fcnet_hiddens': [20, 40]}  \n",
       "3  {'fcnet_hiddens': [40, 40]}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['episode_reward_mean', 'training_iteration', 'timesteps_total', 'config/model']].sort_values('timesteps_total', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this table that the `[20,20]` hyperparameter set took the *most* training iterations, which is understandable as it is the least powerful network configuration. The corresponding number of timesteps was the longest. In contrast, `[40,20]` and `[40,40]` are the fastest to train with almost the same `episode_reward_mean` value.\n",
    "\n",
    "Since all four combinations perform equally well, perhaps it's best to choose the largest network as it trains the fastest. If we need to train the neural network frequently, then fast training times might be most important. This also suggests that we should be sure the trial sizes we used are really best. In a real-world application, you would want to spend more time on HPO, trying a larger set of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Results with TensorBoard \n",
    "\n",
    "Here is the directory where the training data was written for the \"best\" hyperparameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jules/ray_results/PPO/PPO_CartPole-v1_bb7ed_00003_3_fcnet_hiddens_0=40,fcnet_hiddens_1=40_2022-03-17_16-28-36'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.get_best_logdir(metric=\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a separate directory for each trial run.\n",
    "\n",
    "The easiest way to inspect all the training data, including graphs, is to use [TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown in the output.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a screen shot of TensorBoard after the previous Tune run, showing how _scalars_ like the `episdoe_reward_mean` evolved. Some data shown is for runs with trial values of 60 and 80:\n",
    "\n",
    "![TensorBoard](../images/tune/TensorBoard-CartPole-HPO-scalars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the hyperparameter data, similar to what we saw above by looking at the `analysis.dataframe()`:\n",
    "\n",
    "![TensorBoard](../images/tune/TensorBoard-CartPole-HPO-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Useful Information \n",
    "\n",
    "The `ray.tune.trial.Trial` object ([documentation](https://docs.ray.io/en/latest/tune/api_docs/internals.html?highlight=Trial#trial-docstring)) records lots of information about particular trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PPO_CartPole-v1_bb7ed_00003, ray.tune.trial.Trial)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "trial, type(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_mean': 400}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.stopping_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': {'max': 500.0,\n",
       "  'min': 61.0,\n",
       "  'avg': 394.72727272727275,\n",
       "  'last': 500.0,\n",
       "  'last-5-avg': 500.0,\n",
       "  'last-10-avg': 428.1},\n",
       " 'episode_reward_min': {'max': 70.0,\n",
       "  'min': 8.0,\n",
       "  'avg': 23.272727272727273,\n",
       "  'last': 70.0,\n",
       "  'last-5-avg': 37.6,\n",
       "  'last-10-avg': 24.8},\n",
       " 'episode_reward_mean': {'max': 407.76,\n",
       "  'min': 22.153203342618383,\n",
       "  'avg': 215.14293953238783,\n",
       "  'last': 407.76,\n",
       "  'last-5-avg': 342.004,\n",
       "  'last-10-avg': 234.44191315136476},\n",
       " 'episode_len_mean': {'max': 407.76,\n",
       "  'min': 22.153203342618383,\n",
       "  'avg': 215.14293953238783,\n",
       "  'last': 407.76,\n",
       "  'last-5-avg': 342.004,\n",
       "  'last-10-avg': 234.44191315136476},\n",
       " 'episodes_this_iter': {'max': 359,\n",
       "  'min': 15,\n",
       "  'avg': 77.09090909090908,\n",
       "  'last': 17,\n",
       "  'last-5-avg': 18.8,\n",
       "  'last-10-avg': 48.9},\n",
       " 'num_healthy_workers': {'max': 6,\n",
       "  'min': 6,\n",
       "  'avg': 6.0,\n",
       "  'last': 6,\n",
       "  'last-5-avg': 6.0,\n",
       "  'last-10-avg': 6.0},\n",
       " 'timesteps_total': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'timesteps_this_iter': {'max': 7992,\n",
       "  'min': 7992,\n",
       "  'avg': 7992.0,\n",
       "  'last': 7992,\n",
       "  'last-5-avg': 7992.0,\n",
       "  'last-10-avg': 7992.0},\n",
       " 'agent_timesteps_total': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'done': {'max': True,\n",
       "  'min': False,\n",
       "  'avg': 0.09090909090909091,\n",
       "  'last': True,\n",
       "  'last-5-avg': 0.2,\n",
       "  'last-10-avg': 0.1},\n",
       " 'episodes_total': {'max': 848,\n",
       "  'min': 359,\n",
       "  'avg': 708.7272727272726,\n",
       "  'last': 848,\n",
       "  'last-5-avg': 813.0,\n",
       "  'last-10-avg': 743.7},\n",
       " 'training_iteration': {'max': 11,\n",
       "  'min': 1,\n",
       "  'avg': 6.0,\n",
       "  'last': 11,\n",
       "  'last-5-avg': 9.0,\n",
       "  'last-10-avg': 6.5},\n",
       " 'time_this_iter_s': {'max': 3.4978299140930176,\n",
       "  'min': 3.1673600673675537,\n",
       "  'avg': 3.275379029187289,\n",
       "  'last': 3.312864065170288,\n",
       "  'last-5-avg': 3.252665948867798,\n",
       "  'last-10-avg': 3.253133940696716},\n",
       " 'time_total_s': {'max': 36.02916932106018,\n",
       "  'min': 3.4978299140930176,\n",
       "  'avg': 19.73438852483576,\n",
       "  'last': 36.02916932106018,\n",
       "  'last-5-avg': 29.46787209510803,\n",
       "  'last-10-avg': 21.358044385910034},\n",
       " 'time_since_restore': {'max': 36.02916932106018,\n",
       "  'min': 3.4978299140930176,\n",
       "  'avg': 19.73438852483576,\n",
       "  'last': 36.02916932106018,\n",
       "  'last-5-avg': 29.46787209510803,\n",
       "  'last-10-avg': 21.358044385910034},\n",
       " 'timesteps_since_restore': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'iterations_since_restore': {'max': 11,\n",
       "  'min': 1,\n",
       "  'avg': 6.0,\n",
       "  'last': 11,\n",
       "  'last-5-avg': 9.0,\n",
       "  'last-10-avg': 6.5},\n",
       " 'sampler_perf/mean_raw_obs_processing_ms': {'max': 0.10710910437233617,\n",
       "  'min': 0.0928757959251784,\n",
       "  'avg': 0.09700232170443049,\n",
       "  'last': 0.0928757959251784,\n",
       "  'last-5-avg': 0.09386788863695325,\n",
       "  'last-10-avg': 0.09599164343763991},\n",
       " 'sampler_perf/mean_inference_ms': {'max': 0.6329657271702972,\n",
       "  'min': 0.601721754106611,\n",
       "  'avg': 0.61420130440633,\n",
       "  'last': 0.6094126234919908,\n",
       "  'last-5-avg': 0.6112793619235516,\n",
       "  'last-10-avg': 0.6123248621299334},\n",
       " 'sampler_perf/mean_action_processing_ms': {'max': 0.07001821282241519,\n",
       "  'min': 0.06778255114337664,\n",
       "  'avg': 0.06905671204643177,\n",
       "  'last': 0.06893265236490795,\n",
       "  'last-5-avg': 0.06896619551238117,\n",
       "  'last-10-avg': 0.06896056196883342},\n",
       " 'sampler_perf/mean_env_wait_ms': {'max': 0.06738279589554026,\n",
       "  'min': 0.0653856924061201,\n",
       "  'avg': 0.06668496829354423,\n",
       "  'last': 0.06667774024210282,\n",
       "  'last-5-avg': 0.06669588305604869,\n",
       "  'last-10-avg': 0.06661518553334461},\n",
       " 'sampler_perf/mean_env_render_ms': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0},\n",
       " 'timers/sample_time_ms': {'max': 3284.082,\n",
       "  'min': 1229.785,\n",
       "  'avg': 2757.8076363636364,\n",
       "  'last': 3284.082,\n",
       "  'last-5-avg': 3091.7358000000004,\n",
       "  'last-10-avg': 2910.6099},\n",
       " 'timers/sample_throughput': {'max': 6498.698,\n",
       "  'min': 2433.557,\n",
       "  'avg': 3108.3213636363635,\n",
       "  'last': 2433.557,\n",
       "  'last-5-avg': 2587.5362,\n",
       "  'last-10-avg': 2769.2837000000004},\n",
       " 'timers/load_time_ms': {'max': 0.914,\n",
       "  'min': 0.768,\n",
       "  'avg': 0.8349999999999999,\n",
       "  'last': 0.777,\n",
       "  'last-5-avg': 0.8058,\n",
       "  'last-10-avg': 0.8417},\n",
       " 'timers/load_throughput': {'max': 10403748.469,\n",
       "  'min': 8739864.83,\n",
       "  'avg': 9602062.463272726,\n",
       "  'last': 10284686.15,\n",
       "  'last-5-avg': 9926407.117,\n",
       "  'last-10-avg': 9521893.8627},\n",
       " 'timers/learn_time_ms': {'max': 2279.517,\n",
       "  'min': 2110.631,\n",
       "  'avg': 2151.2712727272724,\n",
       "  'last': 2110.631,\n",
       "  'last-5-avg': 2122.0636,\n",
       "  'last-10-avg': 2138.4467000000004},\n",
       " 'timers/learn_throughput': {'max': 3786.545,\n",
       "  'min': 3506.006,\n",
       "  'avg': 3716.6418181818176,\n",
       "  'last': 3786.545,\n",
       "  'last-5-avg': 3766.1822,\n",
       "  'last-10-avg': 3737.7054},\n",
       " 'timers/update_time_ms': {'max': 2.898,\n",
       "  'min': 2.771,\n",
       "  'avg': 2.8350909090909093,\n",
       "  'last': 2.771,\n",
       "  'last-5-avg': 2.8131999999999997,\n",
       "  'last-10-avg': 2.8288},\n",
       " 'info/num_steps_sampled': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'info/num_agent_steps_sampled': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'info/num_steps_trained': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'info/num_steps_trained_this_iter': {'max': 7992,\n",
       "  'min': 7992,\n",
       "  'avg': 7992.0,\n",
       "  'last': 7992,\n",
       "  'last-5-avg': 7992.0,\n",
       "  'last-10-avg': 7992.0},\n",
       " 'info/num_agent_steps_trained': {'max': 87912,\n",
       "  'min': 7992,\n",
       "  'avg': 47952.0,\n",
       "  'last': 87912,\n",
       "  'last-5-avg': 71928.0,\n",
       "  'last-10-avg': 51948.0},\n",
       " 'perf/cpu_util_percent': {'max': 40.25000000000001,\n",
       "  'min': 30.440000000000005,\n",
       "  'avg': 36.79090909090909,\n",
       "  'last': 40.25000000000001,\n",
       "  'last-5-avg': 37.194,\n",
       "  'last-10-avg': 36.502},\n",
       " 'perf/ram_util_percent': {'max': 54.54,\n",
       "  'min': 53.9,\n",
       "  'avg': 54.05909090909089,\n",
       "  'last': 53.925,\n",
       "  'last-5-avg': 53.99499999999999,\n",
       "  'last-10-avg': 54.01099999999999},\n",
       " 'info/learner/default_policy/learner_stats/cur_kl_coeff': {'max': 0.30000001192092896,\n",
       "  'min': 0.01875000074505806,\n",
       "  'avg': 0.16988636993549083,\n",
       "  'last': 0.01875000074505806,\n",
       "  'last-5-avg': 0.0637500025331974,\n",
       "  'last-10-avg': 0.16687500663101673},\n",
       " 'info/learner/default_policy/learner_stats/cur_lr': {'max': 4.999999873689376e-05,\n",
       "  'min': 4.999999873689376e-05,\n",
       "  'avg': 4.999999873689376e-05,\n",
       "  'last': 4.999999873689376e-05,\n",
       "  'last-5-avg': 4.999999873689376e-05,\n",
       "  'last-10-avg': 4.999999873689376e-05},\n",
       " 'info/learner/default_policy/learner_stats/total_loss': {'max': 2533.4404,\n",
       "  'min': 212.90225,\n",
       "  'avg': 1765.9712801846592,\n",
       "  'last': 1797.4242,\n",
       "  'last-5-avg': 2124.2416748046876,\n",
       "  'last-10-avg': 1921.2781829833984},\n",
       " 'info/learner/default_policy/learner_stats/policy_loss': {'max': -0.0071964045,\n",
       "  'min': -0.034465723,\n",
       "  'avg': -0.01503528565676375,\n",
       "  'last': -0.0071964045,\n",
       "  'last-5-avg': -0.00886505814269185,\n",
       "  'last-10-avg': -0.013092241948470473},\n",
       " 'info/learner/default_policy/learner_stats/vf_loss': {'max': 2533.449,\n",
       "  'min': 212.93239,\n",
       "  'avg': 1765.9848091819067,\n",
       "  'last': 1797.4314,\n",
       "  'last-5-avg': 2124.250244140625,\n",
       "  'last-10-avg': 1921.2900512695312},\n",
       " 'info/learner/default_policy/learner_stats/vf_explained_var': {'max': 0.037650675,\n",
       "  'min': 0.00035597963,\n",
       "  'avg': 0.010901687177300284,\n",
       "  'last': 0.00035597963,\n",
       "  'last-5-avg': 0.011459510534768924,\n",
       "  'last-10-avg': 0.010636414235341363},\n",
       " 'info/learner/default_policy/learner_stats/kl': {'max': 0.021703217,\n",
       "  'min': 0.0021071592,\n",
       "  'avg': 0.00729063017801805,\n",
       "  'last': 0.0021071592,\n",
       "  'last-5-avg': 0.003988826274871826,\n",
       "  'last-10-avg': 0.00584937147796154},\n",
       " 'info/learner/default_policy/learner_stats/entropy': {'max': 0.67187303,\n",
       "  'min': 0.49626222,\n",
       "  'avg': 0.56354790113189,\n",
       "  'last': 0.49626222,\n",
       "  'last-5-avg': 0.5170289218425751,\n",
       "  'last-10-avg': 0.5527153879404068},\n",
       " 'info/learner/default_policy/learner_stats/entropy_coeff': {'max': 0.0,\n",
       "  'min': 0.0,\n",
       "  'avg': 0.0,\n",
       "  'last': 0.0,\n",
       "  'last-5-avg': 0.0,\n",
       "  'last-10-avg': 0.0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.metric_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_time': 1647559615.462753, 'timestamp': 1647559811.790977}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 196.33 seconds,    3.27 minutes\n"
     ]
    }
   ],
   "source": [
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_insufficient_resources_manager': <ray.tune.insufficient_resources_manager.InsufficientResourcesManager at 0x7ff20a7d8d30>,\n",
       " '_max_pending_trials': 16,\n",
       " '_metric': None,\n",
       " '_total_time': 139.02484822273254,\n",
       " '_iteration': 15406,\n",
       " '_has_errored': False,\n",
       " '_fail_fast': False,\n",
       " '_server_port': None,\n",
       " '_cached_trial_decisions': {},\n",
       " '_queued_trial_decisions': {},\n",
       " '_updated_queue': True,\n",
       " '_result_wait_time': 1,\n",
       " '_should_stop_experiment': False,\n",
       " '_local_checkpoint_dir': '/Users/jules/ray_results/PPO',\n",
       " '_remote_checkpoint_dir': None,\n",
       " '_stopper': <ray.tune.stopper.NoopStopper at 0x7ff1f91c45e0>,\n",
       " '_resumed': False,\n",
       " '_start_time': 1647559615.462753,\n",
       " '_last_checkpoint_time': -inf,\n",
       " '_session_str': '2022-03-17_16-26-55',\n",
       " 'checkpoint_file': '/Users/jules/ray_results/PPO/experiment_state-2022-03-17_16-26-55.json',\n",
       " '_checkpoint_period': 'auto',\n",
       " 'launch_web_server': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.runner_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used grid search here, which is a naïve approach. In subsequent lessons, we'll explore how to optimize the search process using some of Tune's built-in algorithms for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try More Neural Network Sizes\n",
    "\n",
    "Repeat the experiment above using the sizes `[20, 40, 60, 80, 100]` or some subset of these numbers, depending on how long you are willing to wait. (It takes about 25 minutes on a recent-vintage laptop.) What combination appears to be best, given the considerations we discussed above?\n",
    "\n",
    "> **Note:** The exercise solution for this tutorial can be found [here](solutions/01-Understanding-Hyperparameter-Tuning-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
