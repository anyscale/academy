{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to Reinforcement Learning\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "_Reinforcement Learning_ is the category of machine learning that focuses on training one or more _agents_ to achieve maximal _rewards_ while operating in an environment. This lesson discusses the core concepts of RL, while subsequent lessons explore RLlib in depth. We'll use two examples with exercises to give you a taste of RL. If you already understand RL concepts, you can either skim this lesson or skip to the [next lesson](02-Introduction-to-RLlib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Reinforcement Learning?\n",
    "\n",
    "Let's explore the basic concepts of RL, specifically the _Markov Decision Process_ abstraction, and to show its use in Python.\n",
    "\n",
    "Consider the following image:\n",
    "\n",
    "![RL Concepts](../images/rllib/RL-concepts.png)\n",
    "\n",
    "In RL, one or more **agents** interact with an **environment** to maximize a **reward**. The agents make **observations** about the **state** of the environment and take **actions** that are believed will maximize the long-term reward. However, at any particular moment, the agents can only observe the immediate reward. So, the training process usually involves lots and lot of replay of the game, the robot simulator traversing a virtual space, etc., so the agents can learn from repeated trials what decisions/actions work best to maximize the long-term, cumulative reward.\n",
    "\n",
    "The trial and error search and delayed reward are the distinguishing characterists of RL vs. other ML methods ([Sutton 2018](06-RL-References.ipynb#Books)).\n",
    "\n",
    "The way to formalize trial and error is the **exploitation vs. exploration tradeoff**. When an agent finds what appears to be a \"rewarding\" sequence of actions, the agent may naturally want to continue to **exploit** these actions. However, even better actions may exist. An agent won't know whether alternatives are better or not unless some percentage of actions taken **explore** the alternatives. So, all RL algorithms include a strategy for exploitation and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Applications\n",
    "\n",
    "RL has many potential applications. RL became \"famous\" due to these successes, including achieving expert game play, training robots, autonomous vehicles, and other simulated agents:\n",
    "\n",
    "![AlphaGo](../images/rllib/alpha-go.jpg)\n",
    "\n",
    "The famous Alpha Go Game\n",
    "\n",
    "![Game](../images/rllib/breakout.png)\n",
    "\n",
    "Playing Atari with Deep Reinforcement Learning \n",
    "\n",
    "![Stacking Legos with Sawyer](../images/rllib/stacking-legos-with-sawyer.gif)\n",
    "\n",
    "Learning to stack legos using RL\n",
    "\n",
    "![Walking Man](../images/rllib/walking-man.gif)\n",
    "\n",
    "The famous walking man scenario\n",
    "\n",
    "![Autonomous Vehicle](../images/rllib/daimler-autonomous-car.jpg)\n",
    "\n",
    "The autonous vehicles increasingly becoming prevalent\n",
    "\n",
    "![\"Cassie\": Two-legged Robot](../images/rllib/cassie-crouched.png)\n",
    "\n",
    "The cartonish two-legged robot, Cassie\n",
    "\n",
    "\n",
    "Credits and References:\n",
    "* [AlphaGo](https://www.youtube.com/watch?v=l7ngy56GY6k)\n",
    "* [Breakout](https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756) ([paper](https://arxiv.org/abs/1312.5602))\n",
    "* [Stacking Legos with Sawyer](https://robohub.org/soft-actor-critic-deep-reinforcement-learning-with-real-world-robots/)\n",
    "* [Walking Man](https://openai.com/blog/openai-baselines-ppo/)\n",
    "* [Autonomous Vehicle](https://www.daimler.com/innovation/case/autonomous/intelligent-drive-2.html)\n",
    "* [\"Cassie\": Two-legged Robot](https://mime.oregonstate.edu/research/drl/robots/cassie/) (Uses Ray!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, other industry applications have emerged, include the following:\n",
    "\n",
    "* **Process optimization:** industrial processes (factories, pipelines) and other business processes, routing problems, cluster optimization.\n",
    "* **Ad serving and recommendations:** Some of the traditional methods, including _collaborative filtering_, are hard to scale for very large data sets. RL systems are being developed to do an effective job more efficiently than traditional methods.\n",
    "* **Finance:** Markets are time-oriented _environments_ where automated trading systems are the _agents_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "At its core, Reinforcement learning builds on the concepts of [Markov Decision Process (MDP)](https://en.wikipedia.org/wiki/Markov_decision_process), where the current state, the possible actions that can be taken, and overall goal are the building blocks.\n",
    "\n",
    "An MDP model's sequential interactions with an external environment consists of the following:\n",
    "\n",
    "- a **state space** where the current state of the system is sometimes called the **context**.\n",
    "- a set of **actions** that can be taken at a particular state $s$ (or sometimes the same set for all states).\n",
    "- a **transition function** that describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and an action $a$ was taken. The next state is selected stochastically based on these probabilities.\n",
    "- a **reward function**, which determines the reward received at time $t$ following action $a$, based on the decision of **policy** $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "The goal of MDP is to develop a **policy** $\\pi$ that specifies what action $a$ should be chosen for a given state $s$ so that the cumulative reward is maximized. When it is possible for the policy \"trainer\" to fully observe all the possible states, actions, and rewards, it can define a deterministic policy, fixing a single action choice for each state. In this scenario, the transition probabilities reduce to the probability of transitioning to state $s'$ given the current state is $s$, independent of actions, because the state now leads to a deterministic action choice. Various algorithms can be used to compute this policy. \n",
    "\n",
    "Put another way, if the policy isn't deterministic, then the transition probability to state $s'$ at a time $t+1$ when action $a$ is taken for state $s$ at time $t$, is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "P_a(s',s) = P(s_{t+1} = s'|s_t=s,a)\n",
    "\\end{equation}\n",
    "\n",
    "When the policy is deterministic, this transition probability reduces to the following, independent of $a$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(s',s) = P(s_{t+1} = s'|s_t=s)\n",
    "\\end{equation}\n",
    "\n",
    "To be clear, a deterministic policy means that one and only one action will always be selected for a given state $s$, but the next state $s'$ will still be selected stochastically.\n",
    "\n",
    "In the general case of RL, it isn't possible to fully know all this information, some of which might be hidden and evolving, so it isn't possible to specify a fully-deterministic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "Often this cumulative reward is computed using the **discounted sum** over all rewards observed:\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t(\\pi),\n",
    "\\end{equation}\n",
    "\n",
    "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$), $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$), and $\\gamma$ is the **discount factor**. The value of $\\gamma$ is between 0 and 1, meaning it has the effect of \"discounting\" earlier rewards vs. more recent rewards. \n",
    "\n",
    "The [Wikipedia page on MDP](https://en.wikipedia.org/wiki/Markov_decision_process) provides more details. Note what we said in the third bullet, that the new state only depends on the previous state and the action taken. The assumption is that we can simplify our effort by ignoring all the previous states except the last one and still achieve good results. This is known as the [Markov property](https://en.wikipedia.org/wiki/Markov_property). This assumption often works well and it greatly reduces the resources required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## The Elements of RL\n",
    "\n",
    "Here are the elements of RL that expand on MDP concepts (see [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for more details):\n",
    "\n",
    "#### Policies\n",
    "\n",
    "Unlike MDP, the **transition function** probabilities are often not known in advance, but must be learned. Learning is done through repeated \"play,\" where the agent interacts with the environment.\n",
    "\n",
    "This makes the **policy** $\\pi$ harder to determine. Because the fully state space usually can't be fully known, the choice of an action $a$ for given state $s$ almostly always remains a stochastic choice, never deterministic, unlike MDP.\n",
    "\n",
    "#### Reward Signal\n",
    "\n",
    "The idea of a **reward signal** encapsulates the desired goal for the system and provides feedback for updating the policy based on how well particular events or actions contribute rewards towards the goal.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The **value function** encapsulates the maximum cumulative reward likely to be achieved starting from a given state for an **episode**. This is harder to determine than the simple reward returned after taking an action. In fact, much of the research in RL over the decades has focused on finding better and more efficient implementations of value functions. To illustrate the challenge, repeatedly taking one sequence of actions may yield low rewards for a while, but eventually provide large rewards. Conversely, always choosing a different sequence of actions may yield a good reward at each step, but be suboptimal for the cumulative reward.\n",
    "\n",
    "#### Episode\n",
    "\n",
    "A sequence of steps by the agent starting in an initial state. At each step, the agent observes the current state, chooses the next action, and receives the new reward. Episodes are used for both training policies and replaying with an existing policy (called _rollout_).\n",
    "\n",
    "#### Model\n",
    "\n",
    "An optional feature, some RL algorithms develop or use a **model** of the environment to anticipate the resulting states and rewards for future actions. Hence, they are useful for _planning_ scenarios. Methods for solving RL problems that use models are called _model-based methods_, while methods that learn by trial and error are called _model-free methods_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Reinforcement Learning Example\n",
    "\n",
    "Let's finish this introduction and let's learn about the popular \"hello world\" [1] example environment for RL, balancing a pole vertically on a moving cart, called `CartPole`. Then we'll see how to use RLlib to train a policy using a popular RL algorithm, _Proximal Policy Optimization_, again using `CartPole`.\n",
    "\n",
    "[1] In books and tutorials on programming languages, it is a tradition that the very first program shown prints the message \"Hello World!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### CartPole and OpenAI\n",
    "\n",
    "The popular [OpenAI \"gym\" environment](https://gym.openai.com/) provides MDP interfaces to a variety of simulated environments. Perhaps the most popular for learning RL is `CartPole`, a simple environment that simulates the physics of balancing a pole on a moving cart. The `CartPole` problem is described at https://gym.openai.com/envs/CartPole-v1. Here is an image from that website, where the pole is currently falling to the right, which means the cart will need to move to the right to restore balance:\n",
    "\n",
    "![Cart Pole](../images/rllib/Cart-Pole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "This example fits into the MDP framework as follows:\n",
    "- The **state** consists of the position and velocity of the cart (moving in one dimension from left to right) as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
    "- The **actions** are to decrease or increase the cart's velocity by one unit. A negative velocity means it is moving to the left.\n",
    "- The **transition function** is deterministic and is determined by simulating physical laws. Specifically, for a given **state**, what should we choose as the next velocity value? In the RL context, the correct velocity value to choose has to be learned. Hence, we learn a _policy_ that approximates the optimal transition function that could be calculated from the laws of physics.\n",
    "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
    "- The **discount factor** in this case can be taken to be 1, meaning we treat the rewards at all time steps equally and don't discount any of them.\n",
    "\n",
    "More information about the `gym` Python module is available at https://gym.openai.com/. The list of all the available Gym environments is in [this wiki page](https://github.com/openai/gym/wiki/Table-of-environments). We'll use a few more of them and even create our own in subsequent lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Kwo5ZfNlhn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpofaxQNlhp"
   },
   "source": [
    "The code below illustrates how to create and manipulate MDPs in Python. An MDP can be created by calling `gym.make`. Gym environments are identified by names like `CartPole-v1`. A **catalog of built-in environments** can be found at https://gym.openai.com/envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "6DZ68SG9Nlhp",
    "outputId": "293be60b-8107-42f2-c54a-58f3eaf295f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<CartPoleEnv<CartPole-v1>>>\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(\"Created env:\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xn5PqgDzNlhr"
   },
   "source": [
    "Reset the state of the MDP by calling `env.reset()`. This call returns the initial state of the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "zRA58dOFNlhs",
    "outputId": "7aba4eac-fb0f-4654-eb49-0fbd6d664f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [ 0.04093885 -0.03678606  0.02658814  0.01931461]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(\"The starting state is:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state is the position of the cart, its velocity, the angle of the pole, and the angular velocity of the pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MuXXesWNlhu"
   },
   "source": [
    "The `env.step` method takes an action. In the case of the `CartPole` environment, the appropriate actions are 0 or 1, for pushing the cart to the left or right, respectively. `env.step()` returns a tuple of four things:\n",
    "1. the new state of the environment\n",
    "2. a reward\n",
    "3. a boolean indicating whether the simulation has finished\n",
    "4. a dictionary of miscellaneous extra information\n",
    "\n",
    "Let's show what happens if we take one step with an action of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "TufVaMz_Nlhu",
    "outputId": "920b9758-7d85-49e8-f8ef-4586b6947dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04020313 -0.23227903  0.02697443  0.3202664 ] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "state, reward, done, info = env.step(action)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBIoIuWYNlhw"
   },
   "source": [
    "A **rollout** is a simulation of a policy in an environment. It is used both during training and when running simulations with a trained policy. \n",
    "\n",
    "The code below performs a rollout in a given environment. It takes **random actions** until the simulation has finished and returns the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [],
   "source": [
    "def random_rollout(env):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished.\n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1).\n",
    "        action = np.random.choice([0, 1])\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward.\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the following cell a few times. How much do the answers change? Note that the maximum possible reward for `CartPole-v1` is 500. You'll probably get numbers well under 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "48.0\n"
     ]
    }
   ],
   "source": [
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3FVvEJRNlhy"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Choosing actions at random in `random_rollout` is not a very effective policy, as the previous results showed. Finish implementing the `rollout_policy` function below, which takes an environment *and* a policy. Recall that the *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, like we just did (with poor results), the action should be chosen **with the policy** (as a function of the state).\n",
    "\n",
    "> **Note:** Exercise solutions for this tutorial can be found [here](solutions/Ray-RLlib-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "9PgmkROqNlhy",
    "outputId": "91445278-aeb7-4e86-e1b7-c92e5ba830a2"
   },
   "outputs": [],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # EXERCISE: Fill out this function by copying the appropriate part of 'random_rollout'\n",
    "    # and modifying it to choose the action using the policy.\n",
    "    raise NotImplementedError\n",
    "\n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to `CartPole` in lesson [01: Application Cart Pole](explore-rllib/01-Application-Cart-Pole.ipynb) in the `explore-rllib` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "### RLlib Reinforcement Learning Example: Cart Pole with Proximal Policy Optimization\n",
    "\n",
    "This section demonstrates how to use the _proximal policy optimization_ (PPO) algorithm implemented by [RLlib](http://rllib.io). PPO is a popular way to develop a policy. RLlib also uses [Ray Tune](http://tune.io), the Ray Hyperparameter Tuning framework, which is covered in the [Ray Tune Tutorial](../ray-tune/00-Ray-Tune-Overview.ipynb).\n",
    "\n",
    "We'll provide relatively little explanation of **RLlib** concepts for now, but explore them in greater depth in subsequent lessons. For more on RLlib, see the documentation at http://rllib.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "PPO is described in detail in [this paper](https://arxiv.org/abs/1707.06347). It is a variant of _Trust Region Policy Optimization_ (TRPO) described in [this earlier paper](https://arxiv.org/abs/1502.05477). [This OpenAI post](https://openai.com/blog/openai-baselines-ppo/) provides a more accessible introduction to PPO.\n",
    "\n",
    "PPO works in two phases. In the first phase, a large number of rollouts are performed in parallel. The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. In the second phase, we use SGD (_stochastic gradient descent_) to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![PPO](../images/rllib/ppo.png)\n",
    "\n",
    "> **NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary. Hence, for normal usage, one or more GPUs is recommended.\n",
    "\n",
    "(The original version of this example can be found [here](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwPnR2ibNlh2"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Ray. If you are running these tutorials on your laptop, then a single-node Ray cluster will be started by the next cell. If you are running in the Anyscale platform, it will connect to the running Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:09:28,771\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-03-17_16-09-26_550801_99855/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-03-17_16-09-26_550801_99855/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-03-17_16-09-26_550801_99855', 'metrics_export_port': 62444, 'gcs_address': '127.0.0.1:53561', 'address': '127.0.0.1:53561', 'node_id': 'b948df77a0e95112f1432667f1fe3d29c78870984c4409b6b96232ab'}\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** Having trouble starting Ray? See the [Troubleshooting](../reference/Troubleshooting-Tips-Tricks.ipynb) tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prints the URL for the Ray Dashboard. **This is only correct if you are running this tutorial on a laptop.** Click the link to open the dashboard.\n",
    "\n",
    "If you are running on the Anyscale platform, use the URL provided by your instructor to open the Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9yhpJZVNlh5"
   },
   "source": [
    "Instantiate a PPOTrainer object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used. In a cluster, these actors will be spread over the available nodes.\n",
    "- `num_sgd_iter` is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO, for each _minibatch_ (\"chunk\") of training data. Using minibatches is more efficient than training with one record at a time.\n",
    "- `sgd_minibatch_size` is the SGD minibatch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers. Here, we have two hidden layers of size 100, each.\n",
    "- `num_cpus_per_worker` when set to 0 prevents Ray from pinning a CPU core to each worker, which means we could run out of workers in a constrained environment like a laptop or a cloud VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 1\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:09:49,745\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config, 'CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty1a6AWVNlh7"
   },
   "source": [
    "Now let's train the policy on the `CartPole-v1` environment for `N` steps. The JSON object returned by each call to `agent.train()` contains a lot of information we'll inspect below. For now, we'll extract information we'll graph, such as `episode_reward_mean`. The _mean_ values are more useful for determining successful training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward:   8.0000/ 22.4157/ 92.0000\n",
      "  1: Min/Mean/Max reward:   9.0000/ 37.3868/123.0000\n",
      "  2: Min/Mean/Max reward:   9.0000/ 54.8000/138.0000\n",
      "  3: Min/Mean/Max reward:  10.0000/ 84.6300/379.0000\n",
      "  4: Min/Mean/Max reward:  10.0000/117.7400/397.0000\n",
      "  5: Min/Mean/Max reward:  10.0000/149.7000/500.0000\n",
      "  6: Min/Mean/Max reward:  10.0000/178.4400/500.0000\n",
      "  7: Min/Mean/Max reward:  12.0000/209.8400/500.0000\n",
      "  8: Min/Mean/Max reward:  12.0000/244.6100/500.0000\n",
      "  9: Min/Mean/Max reward:  12.0000/270.0500/500.0000\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min':  result['episode_reward_min'],  \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max':  result['episode_reward_max'],  \n",
    "               'episode_len_mean':    result['episode_len_mean']} \n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the episode data to a Pandas `DataFrame` for easy manipulation. The results indicate how much reward the policy is receiving (`episode_reward_*`) and how many time steps of the environment the policy ran (`episode_len_mean`). The maximum possible reward for this problem is `500`. The reward mean and trajectory length are very close because the agent receives a reward of one for every time step that it survives. However, this is specific to this environment and not true in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.415730</td>\n",
       "      <td>92.0</td>\n",
       "      <td>22.415730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.386792</td>\n",
       "      <td>123.0</td>\n",
       "      <td>37.386792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>54.800000</td>\n",
       "      <td>138.0</td>\n",
       "      <td>54.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>84.630000</td>\n",
       "      <td>379.0</td>\n",
       "      <td>84.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>117.740000</td>\n",
       "      <td>397.0</td>\n",
       "      <td>117.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>149.700000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>149.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>178.440000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>178.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>209.840000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>209.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>244.610000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>244.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>270.050000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>270.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  episode_reward_min  episode_reward_mean  episode_reward_max  \\\n",
       "0  0                 8.0            22.415730                92.0   \n",
       "1  1                 9.0            37.386792               123.0   \n",
       "2  2                 9.0            54.800000               138.0   \n",
       "3  3                10.0            84.630000               379.0   \n",
       "4  4                10.0           117.740000               397.0   \n",
       "5  5                10.0           149.700000               500.0   \n",
       "6  6                10.0           178.440000               500.0   \n",
       "7  7                12.0           209.840000               500.0   \n",
       "8  8                12.0           244.610000               500.0   \n",
       "9  9                12.0           270.050000               500.0   \n",
       "\n",
       "   episode_len_mean  \n",
       "0         22.415730  \n",
       "1         37.386792  \n",
       "2         54.800000  \n",
       "3         84.630000  \n",
       "4        117.740000  \n",
       "5        149.700000  \n",
       "6        178.440000  \n",
       "7        209.840000  \n",
       "8        244.610000  \n",
       "9        270.050000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'episode_reward_min',\n",
       " 'episode_reward_mean',\n",
       " 'episode_reward_max',\n",
       " 'episode_len_mean']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data. Since the length and reward means are equal, we'll only plot one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is quickly able to hit the maximum value of 500, but the mean is what's most valuable. After 10 steps, we're more than half way there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI, here are two views of the whole value for one result. First, a \"pretty print\" output.\n",
    "\n",
    "> **Tip:** The output will be long. When this happens for a cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3o0wjdZ3Nlh7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 40000\n",
      "custom_metrics: {}\n",
      "date: 2022-03-17_16-11-03\n",
      "done: false\n",
      "episode_len_mean: 270.05\n",
      "episode_media: {}\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 270.05\n",
      "episode_reward_min: 12.0\n",
      "episodes_this_iter: 12\n",
      "episodes_total: 441\n",
      "experiment_id: 73459c6b55f748bdae8d42fe303b6f9a\n",
      "hostname: Juless-MacBook-Pro-16-inch-2019\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5147084593772888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004637665580958128\n",
      "        model: {}\n",
      "        policy_loss: -0.00826053787022829\n",
      "        total_loss: 832.4736328125\n",
      "        vf_explained_var: 0.16725607216358185\n",
      "        vf_loss: 832.4817504882812\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "  num_steps_trained_this_iter: 4000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 0.0\n",
      "  ram_util_percent: 51.42\n",
      "pid: 99855\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.04614653261891873\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.04571587329741563\n",
      "  mean_inference_ms: 0.42432151722661793\n",
      "  mean_raw_obs_processing_ms: 0.06788292425999021\n",
      "time_since_restore: 36.21382212638855\n",
      "time_this_iter_s: 3.654078960418701\n",
      "time_total_s: 36.21382212638855\n",
      "timers:\n",
      "  learn_throughput: 3203.836\n",
      "  learn_time_ms: 1248.504\n",
      "  load_throughput: 16727034.895\n",
      "  load_time_ms: 0.239\n",
      "  sample_throughput: 553.77\n",
      "  sample_time_ms: 7223.22\n",
      "  update_time_ms: 1.596\n",
      "timestamp: 1647558663\n",
      "timesteps_since_restore: 40000\n",
      "timesteps_this_iter: 4000\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pretty_print(results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn about more of these values as continue the tutorial.\n",
    "\n",
    "The whole, long JSON blob, which includes the historical stats about episode rewards and lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 500.0,\n",
       " 'episode_reward_min': 12.0,\n",
       " 'episode_reward_mean': 270.05,\n",
       " 'episode_len_mean': 270.05,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 12,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [34.0,\n",
       "   241.0,\n",
       "   191.0,\n",
       "   171.0,\n",
       "   83.0,\n",
       "   237.0,\n",
       "   151.0,\n",
       "   124.0,\n",
       "   18.0,\n",
       "   206.0,\n",
       "   66.0,\n",
       "   140.0,\n",
       "   32.0,\n",
       "   132.0,\n",
       "   186.0,\n",
       "   144.0,\n",
       "   73.0,\n",
       "   148.0,\n",
       "   106.0,\n",
       "   182.0,\n",
       "   159.0,\n",
       "   75.0,\n",
       "   191.0,\n",
       "   186.0,\n",
       "   275.0,\n",
       "   184.0,\n",
       "   236.0,\n",
       "   252.0,\n",
       "   317.0,\n",
       "   152.0,\n",
       "   174.0,\n",
       "   397.0,\n",
       "   103.0,\n",
       "   262.0,\n",
       "   156.0,\n",
       "   328.0,\n",
       "   261.0,\n",
       "   189.0,\n",
       "   243.0,\n",
       "   141.0,\n",
       "   12.0,\n",
       "   176.0,\n",
       "   330.0,\n",
       "   250.0,\n",
       "   323.0,\n",
       "   214.0,\n",
       "   270.0,\n",
       "   272.0,\n",
       "   289.0,\n",
       "   286.0,\n",
       "   243.0,\n",
       "   295.0,\n",
       "   261.0,\n",
       "   383.0,\n",
       "   500.0,\n",
       "   163.0,\n",
       "   215.0,\n",
       "   140.0,\n",
       "   342.0,\n",
       "   165.0,\n",
       "   295.0,\n",
       "   439.0,\n",
       "   300.0,\n",
       "   398.0,\n",
       "   475.0,\n",
       "   329.0,\n",
       "   330.0,\n",
       "   88.0,\n",
       "   367.0,\n",
       "   242.0,\n",
       "   500.0,\n",
       "   476.0,\n",
       "   381.0,\n",
       "   401.0,\n",
       "   388.0,\n",
       "   500.0,\n",
       "   182.0,\n",
       "   370.0,\n",
       "   500.0,\n",
       "   500.0,\n",
       "   424.0,\n",
       "   456.0,\n",
       "   500.0,\n",
       "   456.0,\n",
       "   406.0,\n",
       "   500.0,\n",
       "   500.0,\n",
       "   355.0,\n",
       "   500.0,\n",
       "   500.0,\n",
       "   90.0,\n",
       "   255.0,\n",
       "   341.0,\n",
       "   261.0,\n",
       "   375.0,\n",
       "   337.0,\n",
       "   500.0,\n",
       "   190.0,\n",
       "   173.0,\n",
       "   350.0],\n",
       "  'episode_lengths': [34,\n",
       "   241,\n",
       "   191,\n",
       "   171,\n",
       "   83,\n",
       "   237,\n",
       "   151,\n",
       "   124,\n",
       "   18,\n",
       "   206,\n",
       "   66,\n",
       "   140,\n",
       "   32,\n",
       "   132,\n",
       "   186,\n",
       "   144,\n",
       "   73,\n",
       "   148,\n",
       "   106,\n",
       "   182,\n",
       "   159,\n",
       "   75,\n",
       "   191,\n",
       "   186,\n",
       "   275,\n",
       "   184,\n",
       "   236,\n",
       "   252,\n",
       "   317,\n",
       "   152,\n",
       "   174,\n",
       "   397,\n",
       "   103,\n",
       "   262,\n",
       "   156,\n",
       "   328,\n",
       "   261,\n",
       "   189,\n",
       "   243,\n",
       "   141,\n",
       "   12,\n",
       "   176,\n",
       "   330,\n",
       "   250,\n",
       "   323,\n",
       "   214,\n",
       "   270,\n",
       "   272,\n",
       "   289,\n",
       "   286,\n",
       "   243,\n",
       "   295,\n",
       "   261,\n",
       "   383,\n",
       "   500,\n",
       "   163,\n",
       "   215,\n",
       "   140,\n",
       "   342,\n",
       "   165,\n",
       "   295,\n",
       "   439,\n",
       "   300,\n",
       "   398,\n",
       "   475,\n",
       "   329,\n",
       "   330,\n",
       "   88,\n",
       "   367,\n",
       "   242,\n",
       "   500,\n",
       "   476,\n",
       "   381,\n",
       "   401,\n",
       "   388,\n",
       "   500,\n",
       "   182,\n",
       "   370,\n",
       "   500,\n",
       "   500,\n",
       "   424,\n",
       "   456,\n",
       "   500,\n",
       "   456,\n",
       "   406,\n",
       "   500,\n",
       "   500,\n",
       "   355,\n",
       "   500,\n",
       "   500,\n",
       "   90,\n",
       "   255,\n",
       "   341,\n",
       "   261,\n",
       "   375,\n",
       "   337,\n",
       "   500,\n",
       "   190,\n",
       "   173,\n",
       "   350]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.06788292425999021,\n",
       "  'mean_inference_ms': 0.42432151722661793,\n",
       "  'mean_action_processing_ms': 0.04614653261891873,\n",
       "  'mean_env_wait_ms': 0.04571587329741563,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 1,\n",
       " 'timesteps_total': 40000,\n",
       " 'timesteps_this_iter': 4000,\n",
       " 'agent_timesteps_total': 40000,\n",
       " 'timers': {'sample_time_ms': 7223.22,\n",
       "  'sample_throughput': 553.77,\n",
       "  'load_time_ms': 0.239,\n",
       "  'load_throughput': 16727034.895,\n",
       "  'learn_time_ms': 1248.504,\n",
       "  'learn_throughput': 3203.836,\n",
       "  'update_time_ms': 1.596},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.03750000149011612,\n",
       "     'cur_lr': 4.999999873689376e-05,\n",
       "     'total_loss': 832.47363,\n",
       "     'policy_loss': -0.008260538,\n",
       "     'vf_loss': 832.48175,\n",
       "     'vf_explained_var': 0.16725607,\n",
       "     'kl': 0.0046376656,\n",
       "     'entropy': 0.51470846,\n",
       "     'entropy_coeff': 0.0,\n",
       "     'model': {}},\n",
       "    'custom_metrics': {}}},\n",
       "  'num_steps_sampled': 40000,\n",
       "  'num_agent_steps_sampled': 40000,\n",
       "  'num_steps_trained': 40000,\n",
       "  'num_steps_trained_this_iter': 4000,\n",
       "  'num_agent_steps_trained': 40000},\n",
       " 'done': False,\n",
       " 'episodes_total': 441,\n",
       " 'training_iteration': 10,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '73459c6b55f748bdae8d42fe303b6f9a',\n",
       " 'date': '2022-03-17_16-11-03',\n",
       " 'timestamp': 1647558663,\n",
       " 'time_this_iter_s': 3.654078960418701,\n",
       " 'time_total_s': 36.21382212638855,\n",
       " 'pid': 99855,\n",
       " 'hostname': 'Juless-MacBook-Pro-16-inch-2019',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'num_workers': 1,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [100, 100],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'CartPole-v1',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'num_workers': 1,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 200,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'gamma': 0.99,\n",
       "   'lr': 5e-05,\n",
       "   'train_batch_size': 4000,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [100, 100],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': False,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'CartPole-v1',\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_config': {},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'tf',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'metrics_episode_collection_timeout_s': 180,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_reporting': None,\n",
       "   'min_train_timesteps_per_reporting': None,\n",
       "   'min_sample_timesteps_per_reporting': 0,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 0,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), action_space=Discrete(2), config={})},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': 0,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'lambda': 1.0,\n",
       "   'kl_coeff': 0.2,\n",
       "   'sgd_minibatch_size': 128,\n",
       "   'shuffle_sequences': True,\n",
       "   'num_sgd_iter': 30,\n",
       "   'lr_schedule': None,\n",
       "   'vf_loss_coeff': 1.0,\n",
       "   'entropy_coeff': 0.0,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'clip_param': 0.3,\n",
       "   'vf_clip_param': 10.0,\n",
       "   'grad_clip': None,\n",
       "   'kl_target': 0.01,\n",
       "   'vf_share_layers': -1},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': None,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 0,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32), action_space=Discrete(2), config={})},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr_schedule': None,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_share_layers': -1},\n",
       " 'time_since_restore': 36.21382212638855,\n",
       " 'timesteps_since_restore': 40000,\n",
       " 'iterations_since_restore': 10,\n",
       " 'perf': {'cpu_util_percent': 0.0, 'ram_util_percent': 51.42}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the `episode_reward` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='episode'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode_rewards = results[-1]['hist_stats']['episode_reward']\n",
    "df_episode_rewards = pd.DataFrame(data={'episode':range(len(episode_rewards)), 'reward':episode_rewards})\n",
    "\n",
    "df_episode_rewards.plot(x=\"episode\", y=\"reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a well-trained model, most runs do very well while occasional runs do poorly. Try plotting other results episodes by changing the array index in `results[-1]` to another number between `0` and `9`. (The length of `results` is `10`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPdkWLrENlh9"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "The current network and training configuration are too large and heavy-duty for a simple problem like `CartPole`. Modify the configuration to use a smaller network (the `config['model']['fcnet_hiddens']` setting) and to speed up the optimization of the surrogate objective. (Fewer SGD iterations and a larger batch size should help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lp6tqkNNlh9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:12:27,542\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n"
     ]
    }
   ],
   "source": [
    "# Make edits here:\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['num_workers'] = 3\n",
    "config['num_sgd_iter'] = 30\n",
    "config['sgd_minibatch_size'] = 128\n",
    "config['model']['fcnet_hiddens'] = [100, 100]\n",
    "config['num_cpus_per_worker'] = 0\n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64FmVP7kNlh_"
   },
   "source": [
    "Train the agent and try to get a reward of 500. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
    "\n",
    "This should take around `N` = 20 or 30 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB7sdKUzNliA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 86.0\n",
      "Max reward: 114.0\n",
      "Max reward: 255.0\n",
      "Max reward: 373.0\n",
      "Max reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}   \n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    \n",
    "    print(f'Max reward: {episode[\"episode_reward_max\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW6bN9CYNliB"
   },
   "source": [
    "# Using Checkpoints\n",
    "\n",
    "You checkpoint the current state of a trainer to save what it has learned. Checkpoints are used for subsequent _rollouts_ and also to continue training later from a known-good state.  Calling `agent.save()` creates the checkpoint and returns the path to the checkpoint file, which can be used later to restore the current state to a new trainer. Here we'll load the trained policy into the same process, but often it would be loaded in a new process, for example on a production cluster for serving that is separate from the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uf808LMNliC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jules/ray_results/PPOTrainer_CartPole-v1_2022-03-17_16-12-27gdrnqw6v/checkpoint_000005/checkpoint-5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05icI8bfNliD"
   },
   "source": [
    "Now load the checkpoint in a new trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qq2_AYVNliE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:13:24,362\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1333.\n",
      "2022-03-17 16:13:30,386\tINFO trainable.py:495 -- Restored on 127.0.0.1 from checkpoint: /Users/jules/ray_results/PPOTrainer_CartPole-v1_2022-03-17_16-12-27gdrnqw6v/checkpoint_000005/checkpoint-5\n",
      "2022-03-17 16:13:30,387\tINFO trainable.py:503 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': 39990, '_time_total': 20.854461193084717, '_episodes_total': 686}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "test_agent = PPOTrainer(trained_config, \"CartPole-v1\")\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "Use the previously-trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action. This is an example of _rollout_, which we'll study in a subsequent lesson.\n",
    "\n",
    "Verify that the cumulative reward received roughly matches up with the reward printed above. It will be at or near 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9asL5Z5lNliH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_single_action(state)  # key line; get the next action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [02: Introduction to RLlib](02-Introduction-to-RLlib.ipynb) steps back to introduce to RLlib, its goals and the capabilities it provides."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
