{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Market Bandit Example\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about multi-armed bandits and methods for optimizing rewards, let's look at real-world applications, starting with a stock market example.\n",
    "\n",
    "How well could you invest in the public markets, if you could only observe one macroeconomic signal *inflation* and could only update your investments once each year?\n",
    "\n",
    "To explore this, first we'll load a dataset derived from this [NYU Stern table](http://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/histretSP.html) that shows returns for nearly a century of market data, including dividends and adjustments for inflation. The `market.tsv` file in this folder contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some properties we'll need:\n",
    "DEFAULT_MAX_INFLATION = 100.0\n",
    "DEFAULT_TICKERS = [\"sp500\", \"t.bill\", \"t.bond\", \"corp\"]\n",
    "DEFAULT_DATA_FILE = os.path.abspath(os.path.curdir) + \"/market.tsv\"  # full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_market_data (file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return pd.read_table(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and examine the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_market_data(DEFAULT_DATA_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the data spans 92 years, from 1928 to 2019. \n",
    "\n",
    "The columns represent:\n",
    "  * `year`: the year\n",
    "  * `inflation`: the inflation rate at the time\n",
    "  * `sp500`: [S&P500](https://en.wikipedia.org/wiki/S%26P_500_Index) (composite stock index)\n",
    "  * `t.bill`: [Treasury Bills](https://www.investopedia.com/terms/t/treasurybill.asp) (short-term gov bonds)\n",
    "  * `t.bond`: [Treasury Bonds](https://www.investopedia.com/terms/t/treasurybond.asp) (long-term gov bonds)\n",
    "  * `corp`: [Moody's Baa Corporate Bonds](https://en.wikipedia.org/wiki/Moody%27s_Investors_Service#Moody's_credit_ratings) (moderate risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Data\n",
    "\n",
    "Let's also look at descriptions statistics for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the worst case and best case scenarios? In other words, if one could predict the future market performance, what are the possible ranges of total failure vs. total success over the past century? By \"total\", we mean what if you had all your money in a given year invested in the worst performing _sector_ (S&P500, T bills, or other) or you were invested in the best performing sector for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years = len(df)\n",
    "min_list = []\n",
    "max_list = []\n",
    "\n",
    "for i in range(n_years):\n",
    "    row = df.iloc[i, 2:]\n",
    "    min_list.append(min(row))\n",
    "    max_list.append(max(row))\n",
    "    \n",
    "print(\"{:5.2f}% worst case annualized\".format(sum(min_list) / n_years))\n",
    "print(\"{:5.2f}% best case annualized\".format(sum(max_list) / n_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = pd.DataFrame.from_dict({'year': df['year'], 'min':min_list, 'max':max_list})\n",
    "min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the best and worst returns, year over year.\n",
    "Overall this should look like a [*random walk*](https://en.wikipedia.org/wiki/Random_walk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.fill_between(\n",
    "    df[\"year\"],\n",
    "    min_list,\n",
    "    max_list,\n",
    "    color=\"b\",\n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "plt.title(\"Best vs. Worst Market Return\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some years where the performance varies widely, while other years everything returns about the same performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an Environment\n",
    "\n",
    "Now let's define a Gym environment so that we can train a contextual bandit to optimize annual investments over that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.utils import seeding\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the bandit we'll use to represent the market \"environment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketBandit(gym.Env):\n",
    "    \n",
    "    def __init__ (self, config={}):\n",
    "        self.max_inflation = config.get(\"max-inflation\", DEFAULT_MAX_INFLATION)\n",
    "        self.tickers = config.get(\"tickers\", DEFAULT_TICKERS)\n",
    "        self.data_file = config.get(\"data-file\", DEFAULT_DATA_FILE)\n",
    "        print(f\"MarketBandit: max_inflation: {self.max_inflation}, tickers: {self.tickers}, data file: {self.data_file} (config: {config})\")\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(\n",
    "            low  = -self.max_inflation,\n",
    "            high =  self.max_inflation,\n",
    "            shape=(1, )\n",
    "        )\n",
    "        self.df = load_market_data(self.data_file)\n",
    "        self.cur_context = None\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        self.year = self.df[\"year\"].min()\n",
    "        self.cur_context = self.df.loc[self.df[\"year\"] == self.year][\"inflation\"][0]\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return [self.cur_context]\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        if self.done:\n",
    "            reward = 0.\n",
    "            regret = 0.\n",
    "        else:\n",
    "            row = self.df.loc[self.df[\"year\"] == self.year]\n",
    "\n",
    "            # calculate reward\n",
    "            ticker = self.tickers[action]\n",
    "            reward = float(row[ticker])\n",
    "\n",
    "            # calculate regret\n",
    "            max_reward = max(map(lambda t: float(row[t]), self.tickers))\n",
    "            regret = round(max_reward - reward)\n",
    "\n",
    "            # update the context\n",
    "            self.cur_context = float(row[\"inflation\"])\n",
    "\n",
    "            # increment the year\n",
    "            self.year += 1\n",
    "\n",
    "            if self.year >= self.df[\"year\"].max():\n",
    "                self.done = True\n",
    "\n",
    "        context = [self.cur_context]\n",
    "        #context = self.observation_space.sample()\n",
    "\n",
    "        self.info = {\n",
    "            \"regret\": regret,\n",
    "            \"year\": self.year\n",
    "        }\n",
    "         \n",
    "        return [context, reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = MarketBandit()\n",
    "bandit.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    obs = bandit.step(action)\n",
    "    print(action, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this environment in a kind of [*monte carlo simulation*](https://en.wikipedia.org/wiki/Monte_Carlo_method) to measure a baseline for what the rewards would be over a long period if you merely used actions selected at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = 1\n",
    "reward_list = []\n",
    "iterations = 10000 #50000\n",
    "\n",
    "for i in range(iterations):\n",
    "    if done == 1:\n",
    "        bandit.reset()\n",
    "\n",
    "    action = bandit.action_space.sample()\n",
    "    obs = bandit.step(action)\n",
    "    context, reward, done, info = obs\n",
    "    reward_list.append(reward)\n",
    "    #print(action, context, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.DataFrame(reward_list, columns=[\"reward\"])\n",
    "df_mc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of iterations, you'll probably get a value approaching 3.75% as a baseline for random actions. That's more than the -5.64% worst case and must less than 15.18% best case for the reward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc.plot(y=\"reward\", title=\"Reward Over Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../images/rllib/MarketReward-Random.png))\n",
    "\n",
    "Yes, it looks quite random... There is no improvement (i.e., *learning*) happening at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a policy in RLlib\n",
    "\n",
    "Now let's train a policy using our contextual bandit, specifically using _Linear Thompson Sampling_ in RLlib. Hopefully it will do better than the random results we just computed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in the `__init__()` method for `MarketBandit` that we set some parameters from the passed in `config` object. \n",
    "So we need to create a custom config object with our parameters, by building on the default `TS_CONFIG` object for _LinTS_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from ray.rllib.contrib.bandits.agents.lin_ts import TS_CONFIG\n",
    "\n",
    "market_config = copy.deepcopy(TS_CONFIG)\n",
    "\n",
    "market_config[\"env\"] = MarketBandit\n",
    "market_config[\"max-inflation\"] = DEFAULT_MAX_INFLATION;\n",
    "market_config[\"tickers\"] = DEFAULT_TICKERS;\n",
    "market_config[\"data-file\"] = DEFAULT_DATA_FILE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a custom trainer, which builds on the `LinTSTrainer` with \"updates\". \n",
    "This will be the first argument that we'll pass to `ray.tune.run()` later. \n",
    "\n",
    "Note: if all we needed was the default `LinTSTrainer` trainer, as is and with no customized config settings, we could instead just pass the string `\"contrib/LinTS\"` to `ray.tune.run()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents.lin_ts import LinTSTrainer\n",
    "\n",
    "MarketLinTSTrainer = LinTSTrainer.with_updates(\n",
    "    name=\"MarketLinTSTrainer\",\n",
    "    default_config=market_config,      # Will be merged with Trainer.COMMON_CONFIG (rllib/agent/trainer.py)\n",
    "    #default_policy=[somePolicyClass]  # If we had a policy...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 100\n",
    "}\n",
    "\n",
    "analysis = ray.tune.run(\n",
    "    MarketLinTSTrainer,\n",
    "    config=market_config,\n",
    "    stop=stop,\n",
    "    num_samples=3,    \n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the results\n",
    "\n",
    "Let's analyze the rewards and cumulative regrets from these trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame()\n",
    "\n",
    "for key, df_trial in analysis.trial_dataframes.items():\n",
    "    df_ts = df_ts.append(df_trial, ignore_index=True)\n",
    "    \n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"episode_reward_mean\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.plot(y=[\"mean\", \"max\"], secondary_y=True, title=\"Rewards vs. Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rewards bounce around at first, then appear to stabilize after 5000 steps, with slow improvement afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.plot(y=\"mean\", yerr=\"std\", title=\"Regrets vs. Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Trained Policy\n",
    "\n",
    "Overall, how well did the trained policy perform? The results should be better than random, but less than the best case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:5.2f}% optimized return annualized\".format(max(rewards[\"mean\"]) / n_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a number between near 6%. That's better than the random action baseline of 3.75%, but no where near the best case scenario of 15.18% return. Hence, our regrets continue to grow over time...\n",
    "\n",
    "Note that investing solely in the S&P stock index which would have produced better than 8% return over that period -- that is, if one could wait 92 years. However, investing one's entire portfolio into stocks can become quite a risky policy in the short-term, so we were exploring how to balance a portfolio given only limited information.\n",
    "\n",
    "In any case, the contextual bandit performed well considering that it could only use *inflation* for the context of its decisions, and could only take actions once each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Try using a `LinUCBTrainer`-based trainer.\n",
    "\n",
    "How does the annualized return compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Inflation rates tend to get reported months after they've occurred. To be more accurate with using this dataset, offset the *inflation* observation one step (1 year) ahead.\n",
    "\n",
    "How does the annualized return compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extra - Restoring from a Checkpoint\n",
    "\n",
    "In the previous lesson, [05 Thompson Sampling](05-Thompson-Sampling.ipynb), we showed how to restore a trainer from a checkpoint, but almost \"in passing\". Let's use this feature again, this time with our custom trainer class `MarketLinTSTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = analysis.trials[0]\n",
    "path = trial.checkpoint.value\n",
    "print(f'checkpoint_path: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MarketLinTSTrainer(market_config)  # create instance and then restore from checkpoint\n",
    "trainer.restore(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the model, to review the distribution of arm weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "means = [model.arms[i].theta.numpy() for i in range(3)]\n",
    "covs = [model.arms[i].covariance.numpy() for i in range(3)]\n",
    "means, covs, model.arms[0].theta.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final note: using checkpoints will change how the training performs in this notebook, if you rerun it. So be sure to start from scratch when doing experiments here, if that's what you intend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
