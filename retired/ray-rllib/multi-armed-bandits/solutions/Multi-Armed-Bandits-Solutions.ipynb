{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Multi-Armed Bandits - Exercise Solutions\n",
    "\n",
    "Â© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with 3 arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import ray\n",
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit2 (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        self.current_context = random.choice([-1.,1.])\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit2(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains just as easily as the original implementation that didn't switch contexts between steps. Is this surprising? Probably not, because the relationship between the reward and the context remains linear, so what LinUCB learns for one context is correct for the second context, too. Also, _Tune_ runs many episodes, so it studies both contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Also remove the change made for exercise 1, the line `self.current_context = random.choice([-1.,1.])` in the `step` method.\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBanditNonlinear (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {   # Changed here:\n",
    "            -1.: [-10, 10, 0],\n",
    "            1.: [0, 10, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBanditNonlinear(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ran the maximum of 20,000 steps and the best it does (for different runs) is well below 10.0. the `episode_reward_mean` is chaotic:\n",
    "\n",
    "![Nonlinear model with LinUCB](../../../images/rllib/TensorBoard2.png).\n",
    "\n",
    "Because LinUCB expcts a linear relationship between the context and each reward, it's not surprising that it fails to converge to the desired reward mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 3\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](../02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}\n",
    "\n",
    "analysis = ray.tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the training only takes 200 steps and converge to the desired reward mean of `10.0`.\n",
    "\n",
    "Now let's try the nonlinear bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "analysis = ray.tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "                        progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                        verbose=1)\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run with Thompson sampling yields similar results with the reward mean between 4.5 and 5.0, with somewhat chaotic results over the 20000 steps, if you look at the `episode_reward_mean` graph in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Linear Upper Confidence Bound - Exercise 1\n",
    "\n",
    "Change the `training_iterations` from 20 to 40. Does the characteristic behavior of cumulative regret change at higher steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import ParametricItemRecoEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCB_CONFIG[\"env\"] = ParametricItemRecoEnv\n",
    "\n",
    "# Actual training_iterations will be 40 * timesteps_per_iteration (100 by default) = 4,000\n",
    "training_iterations = 40\n",
    "\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\n",
    "    \"contrib/LinUCB\",\n",
    "    config=UCB_CONFIG,\n",
    "    stop={\"training_iteration\": training_iterations},\n",
    "    num_samples=5,\n",
    "    checkpoint_at_end=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "\n",
    "for key, df in analysis.trial_dataframes.items():\n",
    "    frame = frame.append(df, ignore_index=True)\n",
    "\n",
    "df = frame.groupby(\"info/num_steps_trained\")[\n",
    "    \"info/learner/default_policy/cumulative_regret\"].aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"mean\", yerr=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinUCB-cumulative-regret2.png))\n",
    "\n",
    "The slope appears to stop flattening, suggesting that the previous number of steps, 2000, was sufficient to get the optimal behavior. Beyond that, regret continues to accumulate, but it's linear in the number of steps, neither getting better or worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Linear Thompson Sampling - Exercise 1\n",
    "\n",
    "Experiment with different $\\delta$ values, for example 0.7 and 0.9. What do the cumulative regret and weights graphs look like? \n",
    "\n",
    "You can set the $\\delta$ value like this:\n",
    "\n",
    "```python\n",
    "TS_CONFIG[\"delta\"] = 0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents import LinTSTrainer\n",
    "from ray.rllib.contrib.bandits.agents.lin_ts import TS_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import WheelBanditEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_CONFIG[\"env\"] = WheelBanditEnv\n",
    "\n",
    "training_iterations = 20\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ts (delta):\n",
    "    TS_CONFIG[\"delta\"] = delta\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    analysis = ray.tune.run(\n",
    "        LinTSTrainer,\n",
    "        config=TS_CONFIG,\n",
    "        stop={\"training_iteration\": training_iterations},\n",
    "        num_samples=2,\n",
    "        checkpoint_at_end=True,\n",
    "        verbose=1)\n",
    "\n",
    "    print(\"The trials took\", time.time() - start_time, \"seconds\\n\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for key, df_trial in analysis.trial_dataframes.items():\n",
    "        df = df.append(df_trial, ignore_index=True)\n",
    "\n",
    "    return df, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df (df, analysis):\n",
    "    ts_regrets = df \\\n",
    "        .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "        .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "    \n",
    "    trial = analysis.trials[0]\n",
    "    trainer = LinTSTrainer(config=TS_CONFIG)\n",
    "    trainer.restore(trial.checkpoint.value)\n",
    "    \n",
    "    model = trainer.get_policy().model\n",
    "    means = [model.arms[i].theta.numpy() for i in range(5)]\n",
    "    covs = [model.arms[i].covariance.numpy() for i in range(5)]\n",
    "\n",
    "    return ts_regrets, model, means, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.7\n",
    "ts_df7, analysis7 = run_ts(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_regrets7, model7, means7, covs7 = process_df(ts_df7, analysis7)\n",
    "ts_regrets7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_regrets7.plot(y=\"mean\", yerr=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Cumulative-Regret-07.png))\n",
    "\n",
    "The cumulative regret values are much higher than for $\\delta = 0.5$ in the lesson, and the standard deviation may diverge. We mentioned in the lesson that the problem becomes harder for higher $\\delta$, which fits this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors  = [\"blue\", \"black\", \"green\", \"red\", \"yellow\"]\n",
    "\n",
    "for i in range(0, 5):\n",
    "    x, y = np.random.multivariate_normal(means7[i] / 30, covs7[i], 5000).T\n",
    "    plt.scatter(x, y, color=colors[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-07.png))\n",
    "\n",
    "Compare to the separation of the clusters compared to $\\delta = 0.5$:\n",
    "\n",
    "![image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-05.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.9\n",
    "ts_df9, analysis9 = run_ts(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_regrets9, model9, means9, covs9 = process_df(ts_df9, analysis9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_regrets9.plot(y=\"mean\", yerr=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Cumulative-Regret-09.png))\n",
    "\n",
    "Qualitatively the same as for $\\delta = 0.7$, but the size of the cumulative regret values are even higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors  = [\"blue\", \"black\", \"green\", \"red\", \"yellow\"]\n",
    "\n",
    "for i in range(0, 5):\n",
    "    x, y = np.random.multivariate_normal(means9[i] / 30, covs9[i], 5000).T\n",
    "    plt.scatter(x, y, color=colors[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-09.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Market Example - Exercise 1\n",
    "\n",
    "Try using a `LinUCBTrainer`-based trainer. How does the annualized return compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some properties we'll need:\n",
    "DEFAULT_MAX_INFLATION = 100.0\n",
    "DEFAULT_TICKERS = [\"sp500\", \"t.bill\", \"t.bond\", \"corp\"]\n",
    "DEFAULT_DATA_FILE = os.path.abspath(os.path.curdir) + \"/../market.tsv\"  # full path\n",
    "\n",
    "def load_market_data (file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return pd.read_table(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_market_data(DEFAULT_DATA_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.agents.lin_ucb import LinUCBTrainer\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketBandit (gym.Env):\n",
    "\n",
    "    def __init__ (self, config={}):\n",
    "        self.max_inflation = config.get('max-inflation', DEFAULT_MAX_INFLATION)\n",
    "        self.tickers = config.get('tickers', DEFAULT_TICKERS)\n",
    "        self.data_file = config.get('data-file', DEFAULT_DATA_FILE)\n",
    "        print(f\"MarketBandit: max_inflation: {self.max_inflation}, tickers: {self.tickers}, data file: {self.data_file} (config: {config})\")\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(\n",
    "            low  = -self.max_inflation,\n",
    "            high =  self.max_inflation,\n",
    "            shape=(1, )\n",
    "        )\n",
    "        self.df = load_market_data(self.data_file)\n",
    "        self.cur_context = None\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        self.year = self.df[\"year\"].min()\n",
    "        self.cur_context = self.df.loc[self.df[\"year\"] == self.year][\"inflation\"][0]\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return [self.cur_context]\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        if self.done:\n",
    "            reward = 0.\n",
    "            regret = 0.\n",
    "        else:\n",
    "            row = self.df.loc[self.df[\"year\"] == self.year]\n",
    "\n",
    "            # calculate reward\n",
    "            ticker = self.tickers[action]\n",
    "            reward = float(row[ticker])\n",
    "\n",
    "            # calculate regret\n",
    "            max_reward = max(map(lambda t: float(row[t]), self.tickers))\n",
    "            regret = round(max_reward - reward)\n",
    "\n",
    "            # update the context\n",
    "            self.cur_context = float(row[\"inflation\"])\n",
    "\n",
    "            # increment the year\n",
    "            self.year += 1\n",
    "\n",
    "            if self.year >= self.df[\"year\"].max():\n",
    "                self.done = True\n",
    "\n",
    "        context = [self.cur_context]\n",
    "        #context = self.observation_space.sample()\n",
    "\n",
    "        self.info = {\n",
    "            \"regret\": regret,\n",
    "            \"year\": self.year\n",
    "        }\n",
    "\n",
    "        return [context, reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "market_config = copy.deepcopy(UCB_CONFIG)\n",
    "\n",
    "market_config[\"env\"] = MarketBandit\n",
    "market_config[\"max-inflation\"] = DEFAULT_MAX_INFLATION;\n",
    "market_config[\"tickers\"] = DEFAULT_TICKERS;\n",
    "market_config[\"data-file\"] = DEFAULT_DATA_FILE;\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MarketLinUCBTrainer = LinUCBTrainer.with_updates(\n",
    "    name=\"MarketLinUCBTrainer\",\n",
    "    default_config=market_config,      # Will be merged with Trainer.COMMON_CONFIG (rllib/agent/trainer.py)\n",
    "    #default_policy=[somePolicyClass]  # If we had a policy...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\n",
    "    MarketLinUCBTrainer,\n",
    "    config=market_config,\n",
    "    stop=stop,\n",
    "    num_samples=3,    \n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame()\n",
    "\n",
    "for key, df_trial in analysis.trial_dataframes.items():\n",
    "    df_ts = df_ts.append(df_trial, ignore_index=True)\n",
    "    \n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"episode_reward_mean\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for _LinTS_ were ~340 for reward mean. So, training with _LinUCB_ isn't as successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.plot(y=\"mean\", yerr=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/Market-Bandit-Rewards-vs-Steps-LinUCB.png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets.plot(y=\"mean\", yerr=\"std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/Market-Bandit-Cumulative-Regret-LinUCB.png))\n",
    "\n",
    "What's the annualized return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:5.2f}% optimized return annualized\".format(max(rewards[\"mean\"]) / n_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is likely the same as the completely random choices investigated in the lesson or worse!!\n",
    "\n",
    "The market that we're modeling doesn't exhibit a linear relationship between the context, inflation in our case, and the rewards. Hence, it's not too surprising that a linear algorithm would fail to model the behavior perfectly. What's interesting here is that Thompson Sampling did a noticeably better job than Upper Confidence Bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
