{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Introduction to Bandits\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Multi-Armed Bandits?\n",
    "\n",
    "A **multi-armed bandit (MAB)** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit)) is one way to implement **online machine learning**, based on probability theory. Online machine learning is the idea of updating models as data arrives, rather than training \"offline\" using batch processes and historical data. Hence, a MAB provides a means to inform sequential decision-making about a process over time in the face of uncertainty. An analogous real-world situation is the way an investment fund manager adjusts a stock portfolio in response to evolving market conditions. \n",
    "\n",
    "Multi-armed bandits are named after slot machines in casinos (a.k.a., _one-armed bandits_) where pulling an arm probabilistically produces a cash payout as a reward (or fails to do so). In a typical MAB implementation, a bandit will have N “arms,” one for each of N possible actions to take at a given time step.\n",
    "\n",
    "For simplicity, we'll often just say \"bandits\" to mean MABs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandits vs. General Reinforcement Learning\n",
    "\n",
    "Bandits, in their simplest form, have two restrictions compared to general reinforcement learning:\n",
    "\n",
    "1. They do not attempt to learn the best actions to take for more than one state, which is either stationary or slowly evolving. \n",
    "2. The action taken only affects the reward received, but not the next state of the environment. Recall from our [01 Introduction to Reinforcement Learning](../01-Introduction-to-Reinforcement-Learning.ipynb) lesson that RL is based on _Markov Decision Processes_, where the next state of the system is determined in part by the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Bandits\n",
    "\n",
    "A **contextual bandit** ([Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit), [Ray documentation](https://docs.ray.io/en/latest/rllib-algorithms.html#contextual-bandits-contrib-bandits)) relaxes the first restriction by adding access to a state of the environment, which is called the _context_, that influences the decisions that are made. At each time step, the algorithm observes this context, which can change significantly from one step to the next, and decides to choose one action among the available alternatives, then observes the outcome/reward of that decision. The environment can change randomly between steps.\n",
    "\n",
    "Returning to the inspiration of slot machines, a machine might change the images or colors it shows on the display to indicate a new context. Internally, this might involve a change in the probability of a payout. This change could happen between each pull of the arm.\n",
    "\n",
    "So, contextual bandits are a step closer to full RL, but the second difference remains, that the state is not influenced by the action taken.\n",
    "\n",
    "[Sutton 2018](../06-RL-References.ipynb#Books) uses the term _situation_ as a generic term for the environment's state and contextual bandits are called _associative search_ bandits, because they search for the best actions, but associated to a particular state (situation) value. Hence, non-contextual bandits are called _non-associative_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward vs. Regret\n",
    "\n",
    "While it's common in reinforcement learning to maximize **rewards**, we can also model the opposite of reward, called **regret**. Essentially, regret is the difference between the reward actually achieved and the best possible reward, if the algorithm could choose the best decision at every step.\n",
    "\n",
    "The point of using a bandit is to try to _maximize average reward_. If the algorithm had perfect visibility into the dynamics of the environment, it would always choose the optimal arm to maximize the cumulative reward! However, that's not an interesting real-world problem to solve. Instead, we focus on the case where the algorithm only has limited visibility, so it cannot predict rewards. This means that the **exploitation vs. exploration tradeoff** is an essential aspect of bandits that is shared with general reinforcement learning systems. Pulling one arm repeatedly may provide an excellent reward, but other arms my provide even better rewards, especially as the context evolves.\n",
    "\n",
    "Even while the reward can't be predicted, some of the exploration-exploitation strategies can place theoretical upper bounds on the regret. The use of machine learning to leverage the context and then minimize the regret in turn can provide good solutions. We'll explore this idea in the lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating these concepts to the investment fund example, market conditions define the context. Imagine having one arm of the contextual bandit for each of the stocks that could be selected as an investment. For example, when a particular arm gets “pulled” $10,000 gets invested into that stock for that week. Subsequent performance of the selected stocks defines the _reward_. The difference between this reward and the subsequent performance of the best-performing stock in the market defines the _regret_.\n",
    "\n",
    "Of course, this example grossly oversimplified how investment fund management actually works, but it provides a good idea of how contextual bandits can be used for online learning problems. Indeed, sophisticated bandit models are used in finance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology Summary\n",
    "\n",
    "Let's summarize terms we've defined already and review RL terms we already know:\n",
    "\n",
    "* **State:** The state of the environment is its “context”.\n",
    "* **Action:** The decision taken, i.e., which arm is pulled.\n",
    "* **Agent:** Same as before; the agent decides what action to take, based on a policy for the environment.\n",
    "* **Reward:** The observed reward received by taking the last action and the cumulative rewards for all actions.\n",
    "* **Regret:** The difference between the reward that could be achieved by always selecting the optimal arm and the reward actually achieved by the algorithm.\n",
    "* **Exploration/exploitation tradeoff:** Balancing when to exploit the best rewarding arm known vs. trying different choices to learn more about the possibility of even better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "Bandits have been applied to a wide variety of applications. Another practical use of bandits is testing alternative designs (like websites), often with greater flexibility and less cost than conventional A/B testing. Many example uses are described in [this paper](https://arxiv.org/abs/1904.10040):\n",
    "\n",
    "1. Determining the correct doses for individuals of widely-used medications, where the \"context\" is the individual's health data.\n",
    "2. Modeling brain and behaviorial disorders, accounting for sequential patterns.\n",
    "3. Portfolio management, balancing risk vs. reward and accounting for correlated behaviors between \"instruments\" (e.g., particular stocks).\n",
    "4. Dynamic pricing of goods and services based on demand and feature sets.\n",
    "5. Recommendation systems when the user and item data sets are too large for traditional collaborative filtering approaches and also to better account for users' evolving preferences.\n",
    "6. Information retrieval that accounts for iterative retrieval and likely exploration paths using the results.\n",
    "7. Dialog systems, where the sequential, online aspect of MABs is a better fit for dialogs than offline supervised learning, and also enables the dialog systems to continue learning as they interact with users.\n",
    "8. Anomaly detection, where the exploitation, exploration tradeoff informs how suspected anomalies are investigated, including which ones to pursue or ignore, based on relative likely significance.\n",
    "9. Frequency hopping, \"spreading,\" and power control in telecommunications applications modeled and optimized to improve the end user experience."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
