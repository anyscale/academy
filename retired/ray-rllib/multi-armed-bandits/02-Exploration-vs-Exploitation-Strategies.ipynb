{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Exploration-Exploitation Strategies\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What strategy should we follow for selecting actions that balance the exploration-exploitation tradeoff, yielding the maximum average reward over time? This is the core challenge of RL/bandit algorithms.\n",
    "\n",
    "This lesson has two goals, to give you an intuitive sense of what makes a good algorithm and to introduce several popular examples. \n",
    "\n",
    "> **Tip:** For the first time through this material, you may wish to focus on the first goal, developing an intuitive sense of the requirements for a good algorithm. Come back later to explore the details of the algorithms discussed.\n",
    "\n",
    "So, at least, read through the first sections, stopping at _UCB in More Detail_ under _Upper Confidence Bound_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes a Good Exploration-Exploitation Algorithm?\n",
    "\n",
    "Let's first assume we are considered only stationary bandits. The ideal algorithm achieves these properties:\n",
    "\n",
    "1. It explores all the actions reasonably aggressively.\n",
    "2. When exploring, it picks the action most likely to produce an optimal reward, rather than making random choices.\n",
    "3. It converges quickly to the action that optimizes the mean reward.\n",
    "4. It stops exploration once the optimal action is known and just exploits!\n",
    "\n",
    "For non-stationary and context bandits, the optimal action will likely change over time, so some exploration may always be needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Algorithms\n",
    "\n",
    "With these properties in mind, let's briefly discuss four algorithms. We'll use two of them in examples over several subsequent lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-Greedy\n",
    "\n",
    "One possible strategy is quite simple, called $\\epsilon$-Greedy, where $\\epsilon$ is small number that determines how frequently exploration is done. The best-known action is exploited most of the time (\"greedily\"), governed by probability $1 - \\epsilon$ (i.e., in percentage terms $100*(1 - \\epsilon)$%). With probability $\\epsilon$, an action is picked at  random in the hopes of finding a new action that provides even better rewards.\n",
    "\n",
    "Typical values of $\\epsilon$ are between 0.01 and 0.1. A larger value, like 0.1, explores more aggressively and finds the optimal policy more quickly, but afterwards the aggressive exploration strategy becomes a liability, as it only selects the optimal action ~90% of the time, continuing excessive exploration that is now counterproductive. In contrast, smaller values, like 0.01, are slower to find the optimal policy, but once found continue to select it ~99% of the time, so over time the mean reward is _higher_ for _smaller_ $\\epsilon$ values, as the optimal action is selected more often.\n",
    "\n",
    "How does $\\epsilon$-Greedy stack up against our desired properties?\n",
    "\n",
    "1. The higher the $\\epsilon$ value, the more quickly the action space is explored.\n",
    "2. It randomly picks the next action, so there is no \"intelligence\" involved in optimizing the choice.\n",
    "3. The higher the $\\epsilon$ value, the more quickly the optimal action is found.\n",
    "4. Just as this algorithm makes no attempt to optimize the choice of action during exploration, it makes no attempt to throttle back exploration when the optimal value is found. \n",
    "\n",
    "To address point 4, you could adopt an enhancement that decays the $\\epsilon$ value over time, rather than keeping it fixed.\n",
    "\n",
    "See [Wikipedia - MAB Approximate Solutions](https://en.wikipedia.org/wiki/Multi-armed_bandit) and [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence Bound\n",
    "\n",
    "A limitation about $\\epsilon$-greedy is that exploration is done indiscriminately. Is it possible to make a more informed choice about which alternative actions are more likely to yield a good result, so we preferentially pick one of them? That's what the Upper Confidence Bound (UCB) algorithm attempts to do. It weights some choices over others. \n",
    "\n",
    "It's worth looking at the formula that governs the choice for the next action at time $t$:\n",
    "\n",
    "$$A_t \\doteq \\frac{argmax}{a}\\bigg[ Q_t(a) + c\\sqrt{ \\dfrac{\\ln(t)}{N_t(a)} }\\bigg]$$\n",
    "\n",
    "It's not essential to fully understand all the details, but here is the gist of it; the best action to take at time $t$, $A_t$, is decided by picking the best known action for returning the highest value (the $Q_t(a)$ term in the brackets [...] computes this), but with a correction that encourages exploration, especially for smaller $t$, but penalizing particular actions $a$ if we've already picked them a lot previously (the second term starting with a constant $c$ that governs the \"strength\" of this correction).\n",
    "\n",
    "UCB is one of the best performing algorithms [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition). How does it stack up against our desired properties?\n",
    "\n",
    "1. Exploration is reasonably quick, governed by the $c$ hyperparameter for the \"correction term\".\n",
    "2. It attempts to pick a good action when exploring, rather than randomly.\n",
    "3. Finding the optimal action occurs efficiently, governed by the constant $c$.\n",
    "4. The $ln(t)$ factor in the correction term grows more slowly over time relative to the counts $N_t(a)$, so exploration occurs less frequently at longer time scales.\n",
    "\n",
    "Because UCB is based on prior measured results, it is an example of a _Frequentist_ approach that is _model free_, meaning we just measure outcomes, we don't build a model to explain the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UCB in More Detail\n",
    "\n",
    "Let's explain the equation in more detail. If you are just interested in developing an intuition about strategies, this is a good place to stop and go to the next lesson, [Simple Multi-Armed-Bandit](03-Simple-Multi-Armed-Bandit.ipynb).\n",
    "\n",
    "* $A_t$ is the action we want to select at time $t$, the action that is most likely to produce the best reward or most likely to be worth exploring.\n",
    "* For all the actions we can choose from, we pick the action $a$ that maximizes the formula in the brackets [...].\n",
    "* $Q_t(a)$ is any equation we're using to measure the \"value\" received at time $t$ for action $a$. This is the greedy choice, i.e., the equation that tells us which action $a$ we currently know will give us the highest value. If we never wanted to explore, the second term in the brackets wouldn't exist. $Q_t(a)$ alone would always tell us to pick the best action we already know about. (The use of $Q$ comes from an early RL algorithm called _Q learning_ that models the _value_ returned from actions over time.)\n",
    "* The second term in the brackets is the correction that UCB gives us. As time $t$ increases, the natural log of $t$ also increases, but slower and slower for larger $t$. This is good because we hope we will find the optimal action at some earlier time $t$, so exploration at large $t$ is less useful (as long as the bandit is stationary or slowly changing). However, the denominator, $N_t(a)$ is the number of times we've selected $a$ already. The more times we've already tried $a$, the less \"interesting\" it is to try again, so this term penalizes choosing $a$. Finally, $c$ is a constant, a \"knob\" or _hyperparameter_ that determines how much we weight exploration vs. exploitation.\n",
    "\n",
    "\n",
    "When we use UCB in subsequent lessons, we'll use a simple _linear_ equation for $Q_t(a)$, i.e., something of the form $z = ax + by + c$.\n",
    "\n",
    "See [Wikipedia - MAB Approximate solutions for contextual bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit), [these references](../06-RL-References.ipynb#Upper-Confidence-Bound), and the [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson Sampling\n",
    "\n",
    "Thompson sampling, developed in the 30s, is similar to UCB in that it picks the action that is believed to have the highest potential of maximum reward. It is a _Bayesian, model-based_ approach, where the model is the posterior distribution and may incorporate prior belief about the environment.\n",
    "\n",
    "The agent samples weights for each action, using their posterior distributions, and chooses the action that produces the highest reward. Calculating the exact posterior is intractable in most cases, so they are usually approximated. Hence, the algorithm models beliefs about the problem. Then, during each iteration, the agent initializes with a random belief acts acts optimally based on it.\n",
    "\n",
    "One trade-off is that Thompson Sampling requires an accurate model of the past policy and may suffer from large variance when the past policy differs significantly from a policy being evaluated. You may observe this if you rerun experiments in subsequent lessons that use Thompson Sampling. The graphs of rewards and especially the ranges from high to low, may change significantly from run to run.\n",
    "\n",
    "Relatively speaking, the Thompson Sampling exploration strategies are newer than UCB and tend to perform better (as we'll see in subsequent lessons), although the math for their theoretical performance is less rigorous than for UCB.\n",
    "\n",
    "For more information, see [Wikipedia](https://en.wikipedia.org/wiki/Thompson_sampling), [A Tutorial on Thompson Sampling](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf), [RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints), and other references in [RL References](../References-Reinforcement-Learning.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bandit Algorithms\n",
    "\n",
    "Focusing explicitly on rewards isn't the only approach. What if we use a more general measure, a _preference_, for selecting an action $a$ at time $t$? We'll use $H_t(a)$ to represent this preference at time $t$ for action $a$. We need to model this so we have a probability of selecting an action $a$. Using the _soft-max distribution_ works, also known as the Gibbs or Boltzmann distribution:\n",
    "\n",
    "$Pr\\{A_t = a\\} \\doteq \\frac{e^{H_t(a)}}{\\sum^{k}_{b=1}e^{H_t(b)}} \\doteq \\pi_t(a)$\n",
    "\n",
    "$\\pi_t(a)$ is defined to encapsulate this formula for the probability of taking action $a$ at time $t$.\n",
    "\n",
    "The term _gradient_ is used for this algorithm because the training update formula for $H_t(a)$ is very similar to the _stochastic gradient descent_ formula used in other ML problems. \n",
    "\n",
    "After an action $A_t$ is selected at a time $t$ and reward $R_t$ is received, the action preferences are updated as follows:\n",
    "\n",
    "$ H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha(R_t - \\overset{\\_}{R_t})(1 - \\pi_t(A_t))$,  and\n",
    "\n",
    "$ H_{t+1}(a) \\doteq H_t(a) - \\alpha(R_t - \\overset{\\_}{R_t})(\\pi_t(a))$,  for all $a \\ne A_t$\n",
    "\n",
    "where $H_0(a)$ values are initialized to zero, $\\alpha > 0$ is a step size parameter and $\\overset{\\_}{R_t}$ is the average of all the rewards up through and including time $t$. Note that if $R_t - \\overset{\\_}{R_t}$ is positive, meaning the current reward is larger than the average, the preference $H(A_t)$ increases. Otherwise, it decreases. \n",
    "\n",
    "Note the plus vs. minus signs in the two equations before the $\\alpha$ term. If our preference for $A_t$ increases, our preferences for the other actions should decrease.\n",
    "\n",
    "How does Thompson Sampling satisfy our desired properties?\n",
    "\n",
    "1. As shown, this algorithm doesn't have tuning parameters to control the rate of exploration or convergence to the optimal solution. However, the convergence is reasonably quick if the variance in reward values is relatively high, so that the difference $R_t - \\overset{\\_}{R_t}$ is also relatively large for low $t$ values.\n",
    "2. It attempts to pick a good action when exploring, rather than randomly.\n",
    "3. See 1.\n",
    "4. As $\\overset{\\_}{R_t}$ converges to a maximum, the difference $R_t - \\overset{\\_}{R_t}$ and hence all the preference values $H_t(a)$ will become relatively stationary, with the optimal action having the highest $H$. Since the $H_t(a)$ values govern the probability of being selected, based on the _soft-max distribution_, if the optimal action has a significantly higher $H_t(a)$ than the other actions, it will be chosen most frequently. If the differences between $H_t(a)$ values are not large, then several will be chosen frequently, but that also means their rewards are relatively close. Hence, in either case, the average reward over time will still be close to optimal.\n",
    "\n",
    "There are many more details about Thompson Sampling, but we won't discuss them further here. See [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for the details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
