{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Extra Application Example - MountainCar-v0\n",
    "\n",
    "© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy with the `MountainCar-v0` environment, ([gym.openai.com/envs/MountainCar-v0/](gym.openai.com/envs/MountainCar-v0/)). The idea is that a cart starts at an arbitrar point on a hill. Without any \"pushes\", it will rock back and forth between the two sides of the valley below, never rising above the starting point. However, there are three actions, accelerate to the left (by some unit), accelerate to the right, or apply no acceleration. Timing accelerations in the appropriate directions at the appropriate steps is the key to getting to the top of the hill.\n",
    "\n",
    "The primary idea demonstrated in this lesson is how to start from a previous checkpoint. A checkpoint is provided in the `mountain-car-checkpoint` directory, captured after 200 training episodes. Still, the with the provided checkpoint and addition training of 50 episodes, the cart is unable to reach the top.\n",
    "\n",
    "Hence, you should consider this lesson a big exercise to try when you aren't pressed for time (like in a class setting). Modifications you can try are discussed below.\n",
    "\n",
    "> **Note:** This rollout can only show the rollout visualization popup windows when running on a local laptop.\n",
    "\n",
    "Like `CartPole`, _MountainCar_ is one of OpenAI Gym's [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "* [\"Efficient memory-based learning for robot control\"](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf), [Andrew William Moore](https://www.cl.cam.ac.uk/~awm22/), University of Cambridge (1990)\n",
    "* [\"Solving Mountain Car with Q-Learning\"](https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2), [Tim Sullivan](https://twitter.com/ts_1829)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Ray and the PPO support, then start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard is useful for monitoring Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy with the `MountainCar-v0` environment.\n",
    "\n",
    "By default, training runs for `20` iterations. Increase the `N_ITER` setting _considerably_ if you want to train long enough to see good results. Consider saving high checkpoints and using them in the `agent.restore()` cell below. Note the directory, which is different from the directory used to save the *checkpoints* after each iteration, `tmp/ppo/mountain-car`.\n",
    "\n",
    "For `MountainCar`, the environment has these parameters and behaviors (from this [source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py)):\n",
    "\n",
    "```\n",
    "Observation\n",
    "    Type: Box(2)\n",
    "    Num    Observation               Min            Max\n",
    "    0      Car Position              -1.2           0.6\n",
    "    1      Car Velocity              -0.07          0.07\n",
    "Actions:\n",
    "    Type: Discrete(3)\n",
    "    Num    Action\n",
    "    0      Accelerate to the Left\n",
    "    1      Don't accelerate\n",
    "    2      Accelerate to the Right\n",
    "    Note: This does not affect the amount of velocity affected by the\n",
    "    gravitational pull acting on the car.\n",
    "Reward:\n",
    "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "     on top of the mountain.\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "Starting State:\n",
    "     The position of the car is assigned a uniform random value in\n",
    "     [-0.6 , -0.4].\n",
    "     The starting velocity of the car is always assigned to 0.\n",
    " Episode Termination:\n",
    "     The car position is more than 0.5\n",
    "     Episode length is greater than 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up previous stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/mountain-car\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the default configuration for PPO applied to this environment. There are no configuration parameters that are passed to _MountainCar_ itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell copies the default configuration and makes a few modifications, like a larger training batch size. Other changes you might consider are the following:\n",
    "\n",
    "* Tweak the `model` parameters for the neural net.\n",
    "* Try other `train_batch_size` values (default: `4000`).\n",
    "* SGD parameters: `num_sgd_iter` and `sgd_minibatch_size`.\n",
    "\n",
    "To speed up training:\n",
    "\n",
    "* Increase the `num_workers` to fully utilize your available machine or cluster, \n",
    "* Use GPUs if you have them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_ENV = \"MountainCar-v0\"\n",
    "N_ITER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"            # the default, at this time\n",
    "config[\"num_workers\"] = 4               # default = 2\n",
    "config[\"train_batch_size\"] = 10000      # default = 4000\n",
    "config[\"sgd_minibatch_size\"] = 256      # default = 128\n",
    "config[\"evaluation_num_episodes\"] = 50  # default = 10\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.restore(\"mountain-car-checkpoint/checkpoint-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Training gives up on an episode after 200 steps. The reward is `-1*N` when the cart doesn't reach the top of the hill. The reward is zero if it does reach the top. Hence, there are no incremental rewards here; it's success or failure.\n",
    "\n",
    "Let's print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "\n",
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "This visualizes the \"car\" agent operating within the simulation: rocking back and forth to gain momentum to overcome the mountain, using the last checkpoint. Edit the number in the checkpoint path if necessary! Also change the configuration to match the changes above.\n",
    "\n",
    "> **Note:** This rollout can only show the visualization popup windows when running on a local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rllib rollout \\\n",
    "    tmp/ppo/mountain-car/checkpoint_200/checkpoint-200 \\\n",
    "    --config '{\"env\": \"MountainCar-v0\", \"num_workers\":4, \"train_batch_size\":10000, \"sgd_minibatch_size\":256, \"evaluation_num_episodes\":50}' --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "The rollout uses the second saved checkpoint, evaluated through `2000` steps.\n",
    "Modify the path to view other checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (\"Homework\")\n",
    "\n",
    "In addition to _Mountain Car_ and _Cart Pole_, there are other so-called [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples you can try."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
