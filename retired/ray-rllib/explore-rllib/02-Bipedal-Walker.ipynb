{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Explore RLlib - Sample Application: BipedalWalker-v3 (Optional)\n",
    "\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This example uses a harder problem, the _Bipedal Walker_, a two-legged \"robot\" in two dimensions (see [here](https://gym.openai.com/envs/BipedalWalker-v2/) and [here](https://github.com/openai/gym/wiki/BipedalWalker-v2); we'll actually use version 3, not 2). \n",
    "\n",
    "![Bipedal Walker](../../images/rllib/Bipedal-Walker.png)\n",
    "\n",
    "([source](https://gym.openai.com/envs/BipedalWalker-v2/))\n",
    "\n",
    "Reward is given for moving forward, a total of 300+ points up to the far end. If the robot falls, it gets -100. Applying motor torque costs a small amount of points, so a more optimal agent that minimizes torque application will get a better score. The state consists of the hull angle speed, angular velocity, horizontal speed, vertical speed, position of joints, joints angular speed, legs contact with ground, and 10 LIDAR rangefinder measurements. There are no coordinates in the state vector.\n",
    "\n",
    "This notebook requires more computation than the other lessons to achieve a well-trained policy. However, to make it faster, we provide a checkpoint from previous training episodes, which will accelerate your efforts somewhat. Even starting with the provided checkpoint, you'll see good results. However, consider iterating on the neural network structure and run more training iterations. How well can you train the walker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, import Ray and the PPO module in RLlib, then start Ray. \n",
    "\n",
    "> **NOTE:** There are lots of warnings from TF code for transitioning between V1 and V2. Please ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *checkpoints* will get saved after each iteration into directories under `tmp/ppo/bipedal-walker`, i.e., relative to this directory. \n",
    "The default directories for checkpoints are `$HOME/ray_results/<algo_env>/.../checkpoint_N`.\n",
    "\n",
    "> **Note:** If you prefer to use a different directory root, change it in the next cell _and_ in the `rllib rollout` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/bipedal-walker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up output of previous lessons (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard is useful for monitoring Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train a policy for the [Bipedal Walker](https://gym.openai.com/envs/BipedalWalker-v2/) environment.\n",
    "\n",
    "> **Note:** If you change the values shown for `config['model']['fcnet_hiddens']`, make the same change in the `rllib rollout` command below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_ENV = \"BipedalWalker-v3\"                 # Specifies the OpenAI Gym environment\n",
    "N_ITER = 20                                     # Number of training runs. We'll only do 20 because this is compute intensive.\n",
    "                                                # If you have a powerful machine or cluster or more time, try a bigger number like 50 or 100!\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
    "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
    "config[\"framework\"] = \"tf\"                      # TensorFlow\n",
    "\n",
    "# Other settings we might adjust:\n",
    "config[\"num_workers\"] = 4                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "config[\"num_sgd_iter\"] = 50                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "config[\"sgd_minibatch_size\"] = 250              # The amount of data records per minibatch\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [512, 512]   # Larger network than we used for CartPole.\n",
    "config[\"num_cpus_per_worker\"] = 0               # This avoids running out of resources in the notebook environment when this cell is re-executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall you can see what configuration settings are defined for PPO. Note in particular the parameters for the deep learning `model`. As you try to make the performance better and better, what else might you modify here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You can safely ignore the following warnings if you see them:\n",
    "> ```\n",
    "> WARNING:tensorflow:From .../python3.X/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables \n",
    "> (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version...\n",
    "> ```\n",
    "> Also, there may be warnings about `box bound precision...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore from a previously-captured checkpoint, after training for 100 iterations.\n",
    "\n",
    "> **WARNING:** If you change the configuration parameters above and you get an exception on the next line, it probably means the checkpoint is incompatible with the change. Just skip loading the checkpoint, but consider training for 100-200 iterations instead of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.restore(\"bipedal-walker-checkpoint/checkpoint-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for an additional `N_ITER` iterations. \n",
    "\n",
    "> **Note:** Depending on the machine or cluster you are running on, this can take a long time. If you are on a powerful laptop or running in a cluster, or you don't mind waiting, try using a larger value for `N_ITER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "The episode rewards should increase after multiple iterations. Try tweaking the config parameters. Smaller values for the `num_sgd_iter`, `sgd_minibatch_size`, or the `model`'s `fcnet_hiddens` will train faster, but take longer to improve the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results training starting from the iteration-100 checkpoint and training for an additional `N_ITER` iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Compare with these images after 50 and 100 iterations. Note the sign of the `reward` in all graphs!\n",
    "\n",
    "After 100 iterations, starting from a checkpoint at 50 (so 50 _new_ iterations):\n",
    "\n",
    "![image](../../images/rllib/Bipedal-Walker-Rewards-100.png)\n",
    "\n",
    "After the first 50 iterations:\n",
    "\n",
    "![image](../../images/rllib/Bipedal-Walker-Rewards-50.png)\n",
    "\n",
    "By 100 iterations, the reward has mostly leveled off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Let's print out the policy and model to see the results of training in detailâ¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "\n",
    "Next we'll use the [RLlib rollout CLI](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies), to evaluate the trained policy.\n",
    "\n",
    "We'll use the last saved checkpoint you created for the rollout, `checkpoint_120` (or a different number if you changed the number of steps, etc. See the output from the training above), evaluated through `2000` steps.\n",
    "\n",
    "> **Notes:** \n",
    ">\n",
    "> 1. If you changed `checkpoint_root` value above, then change it here, too. Note that bugs in variable substitution in Jupyter notebooks, we can't use variables in the next cell, unfortunately.\n",
    "> 2. If you changed the model parameters, specifically the `fcnet_hiddens` array in the `config` object above, make the same change here.\n",
    "\n",
    "You may need to make one more modification, depending on how you are running this tutorial:\n",
    "\n",
    "1. Running on your laptop? - Remove the line `--no-render`. \n",
    "2. Running on the Anyscale Service? The popup windows that would normally be created by the rollout can't be viewed in this case. Hence, the `--no-render` flag suppresses them. The code cell afterwards provides a sample video. You can try adding `--video-dir tmp/ppo/cart`, which will generate MP4 videos, then download them to view them. Or copy the `Video` cell below and use it to view the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib rollout tmp/ppo/bipedal-walker/checkpoint_120/checkpoint-120 \\\n",
    "    --config \"{\\\"env\\\": \\\"BipedalWalker-v3\\\", \\\"model\\\": {\\\"fcnet_hiddens\\\": [512, 512]}}\" \\\n",
    "    --run PPO \\\n",
    "    --no-render \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample episode video after training 100 times.\n",
    "\n",
    "> **Note:** This video was created by running the previous `rllib rollout` command with the additional argument `--video-dir tmp/ppo/bipedal-walker` (then the video was copied to the location below). It creates one video per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "sample_video =\"../../images/rllib/Bipedal-Walker-Example-100.mp4\"\n",
    "Video(sample_video, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "Finally, use [TensorBoard](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (\"Homework\")\n",
    "\n",
    "Try a long training run while you do other work. Increase `N_ITER` above to some large number. When it finishes, change the `rllib rollout` command to use the last checkpoint. How well does it run? \n",
    "\n",
    "Redo the experiment a few times. You might increase `N_ITER`. For each run, load the last checkpoint that was saved in the previous run. How well can you train the walker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (\"Homework\")\n",
    "\n",
    "In addition to _Cart Pole_, _Bipedal Walker_, and _Mountain Car_ (see the `extras` folder), there are other so-called [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples you can try. Make a copy of this notebook and edit as required."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
