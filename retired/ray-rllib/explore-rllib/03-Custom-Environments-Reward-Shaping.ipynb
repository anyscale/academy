{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Explore RLlib - Custom Environments and Reward Shaping\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This lesson demonstrates how to adapt your own problem to use [Ray RLlib](http://rllib.io).\n",
    "\n",
    "We cover two important concepts: \n",
    "\n",
    "1. How to create your own _Markov Decision Process_ abstraction.\n",
    "2. How to shape the reward of your environment so make your agent more effective. \n",
    "\n",
    "For a more detailed discussion of how to build a custom environment for training a policy with RLlib using OpenAI [Gym](https://gym.openai.com/), see the [Recsys](../recsys/00-Recsys-Overview.ipynb) (recommender system) lessons and the blog post [\"Anatomy of a custom environment for RLlib\"](https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5). Full source code for that post is available at <https://github.com/DerwenAI/gym_example>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import gym\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do when formulating an RL problem is to specify the dimensions of your observation space and action space. Abstractions for these are provided in Gym. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Different Actions to Their Corresponding Space\n",
    "\n",
    "Let's familiarize ourselves with different Gym spaces. For example:\n",
    "\n",
    "    discrete = spaces.Discrete(10)\n",
    "    print(\"Random sample of this space: \", [discrete.sample() for i in range(4)])\n",
    "\n",
    "Use `help(gym.spaces)` or `help([specific space])` (i.e., `help(gym.spaces.Discrete)`) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(gym.spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following example values in `action_space_examples` that the correspond to the declares spaces in `action_space_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "action_space_map = {\n",
    "    \"discrete_10\": spaces.Discrete(10),\n",
    "    \"box_1\": spaces.Box(0, 1, shape=(1,), dtype=np.float64),  # the dtype can be omitted.\n",
    "    \"box_3x1\": spaces.Box(-2, 2, shape=(3, 1), dtype=np.float64),\n",
    "    \"multi_discrete\": spaces.MultiDiscrete([ 5, 2, 2, 4 ])\n",
    "}\n",
    "\n",
    "action_space_examples = {\n",
    "    \"discrete_10\": 1,\n",
    "    \"box_1\": np.array([0.89089584]),\n",
    "    \"box_3x1\": np.array([[-1.2657754], [-1.6528835], [ 0.5982418]]),\n",
    "    \"multi_discrete\": np.array([0, 0, 0, 2]),\n",
    "}\n",
    "\n",
    "for space_id, state in action_space_examples.items():\n",
    "    assert action_space_map[space_id].contains(state), (f'Looks like {space_id} to {state} is matched incorrectly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a space with 10 discrete values, 0 through 9, from which we sample and then update a counts map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {key:0 for key in range(10)}\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    key = spaces.Discrete(10).sample()\n",
    "    counts[key] = counts[key] + 1\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have more than one dimension of discrete (or continuous) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = spaces.MultiDiscrete([ 5, 2, 2, 4 ])\n",
    "[md.sample() for _ in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the values for each dimension in the discrete space are inclusive, but zero-offset. For example, in the samples shown, the first integer returned in the array is 0-4, inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = spaces.Box(-2, 2, shape=(3,2), dtype=np.float64)\n",
    "[box.sample() for _ in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: A Custom Environment with Rewards\n",
    "\n",
    "Now we'll create an `n-Chain` environment, which represents moves along a linear chain of states, with two actions:\n",
    "\n",
    "* (0) **forward**: move along the chain but returns no reward\n",
    "* (1) **backward**: returns to the beginning and has a small reward\n",
    "\n",
    "The end of the chain, however, provides a large reward, and by moving **forward** at the end of the chain, this large reward can be repeated.\n",
    "\n",
    "#### Step 1: Implement `ChainEnv._setup_spaces`\n",
    "\n",
    "Use a `spaces.Discrete` action space and observation space. Implement `ChainEnv._setup_spaces` in `ChainEnv` so that `self.action_space` and `self.obseration_space` are proper gym spaces.\n",
    "  \n",
    "1. The observation space is an integer in the range `[0 to n-1]`.\n",
    "2. The action space is an integer in `[0, 1]`.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "self.action_space = spaces.Discrete(2)\n",
    "self.observation_space = ...\n",
    "```\n",
    "\n",
    "You should see a message indicating tests passing when done correctly!\n",
    "\n",
    "#### Step 2: Implement a reward function.\n",
    "\n",
    "When `env.step` is called, it returns a tuple of `(state, reward, done, info)`. Right now, the reward is always 0. Modify `step()` so that the following rewards are returned for the given actions: \n",
    "\n",
    "1. `action == 1` will return `self.small_reward`.\n",
    "2. `action == 0` will return 0 if `self.state < self.n - 1`.\n",
    "3. `action == 0` will return `self.large_reward` if `self.state == self.n - 1`.\n",
    "\n",
    "You should see a message indicating tests passing when done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_exercises import test_chain_env_spaces, test_chain_env_reward, test_chain_env_behavior\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, env_config = None):\n",
    "        env_config = env_config or {}\n",
    "        self.n = env_config.get(\"n\", 20)\n",
    "        self.small_reward = env_config.get(\"small\", 2)  # payout for 'backwards' action\n",
    "        self.large_reward = env_config.get(\"large\", 10)  # payout at end of chain for 'forwards' action\n",
    "        self.state = 0  # Start at beginning of the chain\n",
    "        self._horizon = self.n\n",
    "        self._counter = 0  # For terminating the episode\n",
    "        self._setup_spaces()\n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        ##############\n",
    "        # TODO: Implement this so that it passes tests\n",
    "        self.action_space = None\n",
    "        self.observation_space = None\n",
    "        ##############\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        if action == 1:  # 'backwards': go back to the beginning, get small reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = -1\n",
    "            ##############\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = -1\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain, collect large reward\n",
    "            ##############\n",
    "            # TODO 2: Implement this so that it passes tests\n",
    "            reward = -1\n",
    "            ##############\n",
    "        self._counter += 1\n",
    "        done = self._counter >= self._horizon\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self._counter = 0\n",
    "        return self.state\n",
    "    \n",
    "# Tests here:\n",
    "test_chain_env_spaces(ChainEnv)\n",
    "test_chain_env_reward(ChainEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Policy on the Environment \n",
    "\n",
    "Now we'll train a policy on the environment and evaluate the policy. You'll see that despite an extremely high reward, the policy has barely explored the state space. \n",
    "\n",
    "In order to proceed, we'll import an implementation of the previous exercise, but you should actually comment-out the next cell once you complete the previous exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chain_env import ChainEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(chainEnvClass, config = trainer_config, iterations=20):\n",
    "    trainer = PPOTrainer(config, chainEnvClass)\n",
    "    print(f'Training iterations: ', end='')\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print('.', end='')\n",
    "        trainer.train()\n",
    "        \n",
    "    print('')\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = do_training(ChainEnv, config=trainer_config, iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChainEnv({})\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(f'Cumulative reward you received is: {cumulative_reward}. Congratulations!')\n",
    "print(f'Max state you visited is: {max_state}. This is out of {env.n} states.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only visited a small number of states, maybe only 1 or 2 (max == 0 or 1?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping the Reward to Encourage Desired Behavior\n",
    "\n",
    "We see that despite an extremely high reward, the policy has barely explored the state space. This is often the situation - where the reward designed to encourage a particular solution is suboptimal, and the behavior created is unintended.\n",
    "\n",
    "### Exercise 2: Improve the Policy\n",
    "\n",
    "Modify `ShapedChainEnvVisited.step()` in the next cell to return rewards that encourage the policy to traverse the chain (not just stick to 0). Do not change the behavior of the environment. That is, the action -> state behavior should be the same. You can change the reward to be whatever you wish. We'll test it in the next section.\n",
    "\n",
    "This implementation also adds a constructor argument `done_percentage`, which specifies what percentage of states, between `0.0` and `1.0` must be visited before `done` is reached. Play with this number when you modify the rewards to gain a sense of how long it takes to explore the action space. Note that there is a \"safety\"; it stops after `10*env.n` iterations, even if the percentage of visited states isn't reached. As the code exists in the following cell, it will always hit this safety!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapedChainEnvVisited(ChainEnv):\n",
    "\n",
    "    def __init__(self, env_config = None):\n",
    "        super().__init__(env_config)\n",
    "        self.visited = set()\n",
    "        self.done_percentage = 0.5\n",
    "        self.done_n = self.done_percentage * self.n\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        self.visited.add(self.state)\n",
    "        if action == 1:  # 'backwards': go back to the beginning\n",
    "            reward = self.small_reward\n",
    "            self.state = 0\n",
    "        elif self.state < self.n - 1:  # 'forwards': go up along the chain\n",
    "            reward = 0\n",
    "            self.state += 1\n",
    "        else:  # 'forwards': stay at the end of the chain\n",
    "            reward = self.large_reward\n",
    "        self._counter += 1\n",
    "        done = len(self.visited) >= self.done_n\n",
    "        if not done and self._counter > (self.n*10):\n",
    "            done = True\n",
    "            visited_per = (len(self.visited)*100.0)/self.n\n",
    "            print(f'Stopping after {self.n*10} iterations. Visited {visited_per:6.2f}% of the states.')\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "test_chain_env_behavior(ShapedChainEnvVisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `ShapedChainEnv` by Running the Cell(s) Below\n",
    "\n",
    "This trains PPO on the new env and counts the number of states seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = do_training(ShapedChainEnvVisited, config=trainer_config, iterations=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long it takes to get to 50% (the value hard-coded for `done_percentage`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShapedChainEnvVisited({})\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "max_state = -1\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    max_state = max(max_state, state)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(f'Cumulative reward you received is: {cumulative_reward}!')\n",
    "print(f'Max state you visited is: {max_state}. (There are {env.n} states.)')\n",
    "\n",
    "desired = env.done_percentage\n",
    "actual = (max_state+1)/env.n  # add one because of zero indexing\n",
    "\n",
    "print(f\"This policy traversed {actual*100:4.1f}% of the available states.\")\n",
    "assert actual > desired, f\"{actual*100:4.1f}% is less than the desired percentage of {desired*100:4.1f}%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
