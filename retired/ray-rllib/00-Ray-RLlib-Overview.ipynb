{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray RLlib - Overview\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "## Watch Ray Summit 2021 on demand!\n",
    "\n",
    "Ray Summit 2021 [virtual conference](https://www.anyscale.com/ray-summit-2021) was on June 22-24, 2021. We had an amazing lineup of luminar keynote speakers and breakout sessions on the Ray ecosystem, third-party Ray libraries, and applications of Ray in the real world.\n",
    "\n",
    "\n",
    "For information about other online events, see [anyscale.com/events](https://anyscale.com/events).\n",
    "\n",
    "## About This Tutorial\n",
    "\n",
    "This tutorial, part of [Anyscale Academy](https://anyscale.com/academy), introduces the broad topic of _reinforcement learning_ (RL) and [RLlib](https://ray.readthedocs.io/en/latest/rllib.html), Ray's comprehensive RL library.\n",
    "\n",
    "![Ray RLlib](../images/RLlib.png)\n",
    "\n",
    "The lessons in this tutorial use different _environments_ from [OpenAI Gym](https://gym.openai.com/) to illustrate how to train _policies_.\n",
    "\n",
    "See the instructions in the [README](../README.md) for setting up your environment to use this tutorial.\n",
    "\n",
    "Go [here](../Overview.ipynb) for an overview of all tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Sections\n",
    "\n",
    "Because of the breadth of RL this tutorial is divided into several sections. See below for a recommended _learning plan_.\n",
    "\n",
    "### Introduction to Reinforcement Learning and RLlib\n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [Ray RLlib Overview](00-Ray-RLlib-Overview.ipynb) | Overview of this tutorial, including all the sections. (This file.) |\n",
    "| 01 | [Introduction to Reinforcement Learning](01-Introduction-to-Reinforcement-Learning.ipynb) | A quick introduction to the concepts of reinforcement learning. You can skim or skip this lesson if you already understand RL concepts. |\n",
    "| 02 | [Introduction to RLlib](02-Introduction-to-RLlib.ipynb) | An overview of RLlib, its goals and the capabilities it provides. |\n",
    "| 03 | [Using Ray Tune with RLlib](03-Understanding-Hyperparameter-Tuning.ipynb) | An overview of Ray Tune and how to use with RLlib, its goals and the capabilities it provides. |\n",
    "|    | [RL References](References-Reinforcement-Learning.ipynb) | References on reinforcement learning. |\n",
    "\n",
    "Exercise solutions for this introduction can be found [here](solutions/Ray-RLlib-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits\n",
    "\n",
    "_Multi-Armed Bandits_ (MABs) are a special kind of RL problem that have broad and growing applications. They are also an excellent platform for investigating the important _exploitation vs. exploration tradeoff_ at the heart of RL. The term _multi-armed bandit_ is inspired by the slot machines in casinos, so called _one-armed bandits_, but where a machine might have more than one arm. \n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [Multi-Armed-Bandits Overview](multi-armed-bandits/00-Multi-Armed-Bandits-Overview.ipynb) | Overview of this set of lessons. |\n",
    "| 01 | [Introduction to Multi-Armed Bandits](multi-armed-bandits/01-Introduction-to-Multi-Armed-Bandits.ipynb) | A quick introduction to the concepts of multi-armed bandits (MABs) and how they fit in the spectrum of RL problems. |\n",
    "| 02 | [Exploration vs. Exploitation Strategies](multi-armed-bandits/02-Exploration-vs-Exploitation-Strategies.ipynb) | A deeper look at algorithms that balance exploration vs. exploitation, the key challenge for efficient solutions. Much of this material is technical and can be skipped in a first reading, but skim the first part of this lesson at least. |\n",
    "| 03 | [Simple Multi-Armed Bandit](multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb) | A simple example of a multi-armed bandit to illustrate the core ideas. |\n",
    "| 04 | [Linear Upper Confidence Bound](multi-armed-bandits/04-Linear-Upper-Confidence-Bound.ipynb) | One popular algorithm for exploration vs. exploitation is _Upper Confidence Bound_. This lesson shows how to use a linear version in RLlib. |\n",
    "| 05 | [Linear Thompson Sampling](multi-armed-bandits/05-Linear-Thompson-Sampling.ipynb) | Another popular algorithm for exploration vs. exploitation is _Thompson Sampling_. This lesson shows how to use a linear version in RLlib. |\n",
    "| 06 | [Market Example](multi-armed-bandits/06-Market-Example.ipynb) | A simplified real-world example of MABs, finding the optimal stock and bond investment strategy. |\n",
    "\n",
    "Exercise solutions for the bandits section of the tutorial can be found [here](multi-armed-bandits/solutions/Multi-Armed-Bandits-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Reinforcement Learning and RLlib\n",
    "\n",
    "This section dives into more details about RL and using RLlib. It is best studied after going through the MAB material.\n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [Explore RLlib Overview](explore-rllib/00-Explore-RLlib-Overview.ipynb) | Overview of this set of lessons. |\n",
    "| 01 | [Application - Cart Pole](explore-rllib/01-Application-Cart-Pole.ipynb) | The best starting place for learning how to use RL, in this case to train a moving car to balance a vertical pole. Based on the `CartPole-v1` environment from OpenAI Gym, combined with RLlib. |\n",
    "| 02 | [Application: Bipedal Walker](explore-rllib/02-Bipedal-Walker.ipynb) | Train a two-legged robot simulator. This is an optional lesson, due to the longer compute times required, but fun to try. |\n",
    "| 03 | [Custom Environments and Reward Shaping](explore-rllib/03-Custom-Environments-Reward-Shaping.ipynb) | How to customize environments and rewards for your applications. |\n",
    "\n",
    "Some additional examples you might explore can be found in the `extras` folder:\n",
    "\n",
    "| Lesson | Description |\n",
    "| :----- | :---------- |\n",
    "| [Extra: Application - Mountain Car](explore-rllib/extras/Extra-Application-Mountain-Car.ipynb) | Based on the `MountainCar-v0` environment from OpenAI Gym. |\n",
    "| [Extra: Application - Taxi](explore-rllib/extras/Extra-Application-Taxi.ipynb) | Based on the `Taxi-v3` environment from OpenAI Gym. |\n",
    "| [Extra: Application - Frozen Lake](explore-rllib/extras/Extra-Application-Frozen-Lake.ipynb) | Based on the `FrozenLake-v0` environment from OpenAI Gym. |\n",
    "\n",
    "In addition, exercise solutions for this \"exploration\" section of the tutorial can be found [here](explore-rllib/solutions/Ray-RLlib-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RecSys: Recommender System\n",
    "\n",
    "This section applies RL to the problem of building a recommender system, a state-of-the-art technique that addresses many of the limitations of older approaches.\n",
    "\n",
    "|    | Lesson | Description |\n",
    "| :- | :----- | :---------- |\n",
    "| 00 | [RecSys: Recommender System Overview](recsys/00-RecSys-Overview.ipynb) | Overview of this set of lessons. |\n",
    "| 01 | [Recsys: Recommender System](recsys/01-Recsys.ipynb) | An example that builds a recommender system using reinforcement learning. |\n",
    "\n",
    "The [Custom Environments and Reward Shaping](explore-rllib/03-Custom-Environments-Reward-Shaping.ipynb) lesson from _Explore RLlib_ might be useful background for this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For earlier versions of some of these tutorials, see [`rllib_exercises`](https://github.com/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb) in the original [github.com/ray-project/tutorial](https://github.com/ray-project/tutorial) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Plan\n",
    "\n",
    "We recommend the following _learning plan_ for working through the lessons:\n",
    "\n",
    "Start with the introduction material for RL and RLlib:\n",
    "\n",
    "* [Ray RLlib Overview](00-Ray-RLlib-Overview.ipynb) - This file\n",
    "* [Introduction to Reinforcement Learning](01-Introduction-to-Reinforcement-Learning.ipynb) \n",
    "* [Introduction to RLlib](02-Introduction-to-RLlib.ipynb)\n",
    "\n",
    "Then study several of the lessons for multi-armed bandits, starting with these lessons:\n",
    "\n",
    "* [Multi-Armed-Bandits Overview](multi-armed-bandits/00-Multi-Armed-Bandits-Overview.ipynb)\n",
    "* [Introduction to Multi-Armed Bandits](multi-armed-bandits/01-Introduction-to-Multi-Armed-Bandits.ipynb)\n",
    "* [Exploration vs. Exploitation Strategies](multi-armed-bandits/02-Exploration-vs-Exploitation-Strategies.ipynb): Skim at least the first part of this lesson. \n",
    "* [Simple Multi-Armed Bandit](multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb)\n",
    "\n",
    "As time permits, study one or both of the following lessons:\n",
    "\n",
    "* [Linear Upper Confidence Bound](multi-armed-bandits/04-Linear-Upper-Confidence-Bound.ipynb)\n",
    "* [Linear Thompson Sampling](multi-armed-bandits/05-Linear-Thompson-Sampling.ipynb)\n",
    "\n",
    "Then finish with this more complete example:\n",
    "\n",
    "* [Market Example](multi-armed-bandits/06-Market-Example.ipynb)\n",
    "\n",
    "Next, return to the \"exploration\" lessons under `explore-rllib` and work through as many of the following lessons as time permits:\n",
    "\n",
    "* [Application: Cart Pole](explore-rllib/01-Application-Cart-Pole.ipynb): Further exploration of the popular `CartPole` example.\n",
    "* [Application: Bipedal Walker](explore-rllib/02-Bipedal-Walker.ipynb): A nontrivial, but simplified robot simulator.\n",
    "* [Custom Environments and Reward Shaping](explore-rllib/03-Custom-Environments-Reward-Shaping.ipynb): More about creating custom environments for your problem. Also, finetuning the rewards to ensure sufficient exploration.\n",
    "\n",
    "Other examples that use different OpenAI Gym environments are provided for your use in the `extras` directory:\n",
    "\n",
    "* [Extra: Application - Mountain Car](explore-rllib/extras/Extra-Application-Mountain-Car.ipynb)\n",
    "* [Extra: Application - Taxi](explore-rllib/extras/Extra-Application-Taxi.ipynb)\n",
    "* [Extra: Application - Frozen Lake](explore-rllib/extras/Extra-Application-Frozen-Lake.ipynb)\n",
    "\n",
    "Finally, the [references](References-Reinforcement-Learning.ipynb) collect useful books, papers, blog posts, and other available tutorial materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Help\n",
    "\n",
    "* The [#tutorial channel](https://ray-distributed.slack.com/archives/C011ML23W5B) on the [Ray Slack](https://ray-distributed.slack.com). [Click here](https://forms.gle/9TSdDYUgxYs8SA9e8) to join.\n",
    "\n",
    "Find an issue? Please report it!\n",
    "\n",
    "* [GitHub issues](https://github.com/anyscale/academy/issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Us Feedback!\n",
    "\n",
    "Let us know what you like and don't like about this RL and RLlib tutorial.\n",
    "\n",
    "* [Survey](https://forms.gle/D2Lo4K5tkcqsWeKU8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
