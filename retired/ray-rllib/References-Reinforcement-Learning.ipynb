{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning References\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a deep topic and a focus of intense research. We can only scratch the surface here, but the following references may be useful. See also links in the [Introduction to Reinforcement Learning](01-Introduction-to-Reinforcement-Learning.ipynb) lesson and other lessons.\n",
    "\n",
    "## Books\n",
    "\n",
    "Several books are available on RL:\n",
    "\n",
    "* [*Reinforcement Learning: An Introduction*](https://mitpress.mit.edu/books/reinforcement-learning-second-edition), by Richard S. Sutton and Andrew G. Barto, MIT Press, 2018. This is the definitive textbook. Deep, but highly recommended. See this independent [repo of Python code](https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks).\n",
    "* [*Practical Reinforcement Learning*](https://www.endtoend.ai/practical-rl/), by Seungjae Ryan Lee.\n",
    "* [*Hands-On Reinforcement Learning with Python*](https://learning.oreilly.com/library/view/hands-on-reinforcement-learning/9781788836524/), by Sudharsan Ravichandiran, Packt, 2018.\n",
    "* [*Hands-On Reinforcement Learning for Games*](https://www.packtpub.com/game-development/hands-on-game-ai-with-python), by Micheal Lanham, Packt, 2020.\n",
    "* [*Grokking Deep Reinforcement Learning*](https://www.manning.com/books/grokking-deep-reinforcement-learning), by Miguel Morales, Manning 2020 (preview). Deep RL means using deep learning as part of the training system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blogs\n",
    "\n",
    "Several blog posts and series provide concise introductions to RL:\n",
    "\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70).\n",
    "* [Anatomy of a custom environment for RLlib](https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5).\n",
    "* [A Reinforcement Learning Cheat Sheet](https://towardsdatascience.com/reinforcement-learning-cheat-sheet-2f9453df7651).\n",
    "* [Reinforcement Learning Explained](https://www.oreilly.com/radar/reinforcement-learning-explained/), Junling Hu, 2016. A gentle introduction to the ideas of RL.\n",
    "* [A Beginner's Guide to Deep Reinforcement Learning](https://pathmind.com/wiki/deep-reinforcement-learning), Pathmind, 2019. From Pathmind, which uses RLlib for its products and services. Lots of good references at the end of this post.\n",
    "* [An Outsider's Tour of Reinforcement Learning](http://www.argmin.net/2018/06/25/outsider-rl/), Ben Recht, 2018. A series of posts on technical aspects of RL.\n",
    "* [Exploration Strategies in Deep Reinforcement Learning](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html), Lilian Weng, Jun 7, 2020.\n",
    "* [The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm](https://costa.sh/blog-the-32-implementation-details-of-ppo.html), Costa Huang, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Tutorials and Academic Courses on RL\n",
    "\n",
    "* [OSU: Distributed AI with Ray](http://web.engr.oregonstate.edu/~afern/distributed-AI-labs/osu-distributed-ai.html)\n",
    "* [University College London COMPM050/COMPGI13](https://www.davidsilver.uk/teaching/)\n",
    "* [UC Berkeley CS 285](http://rail.eecs.berkeley.edu/deeprlcourse/)\n",
    "* [CS 294 Deep Reinforcement Learning, Spring 2017](http://rll.berkeley.edu/deeprlcourse/)\n",
    "* [A Tutorial on Reinforcement Learning I - YouTube](https://www.youtube.com/watch?v=fIKkhoI1kF4)\n",
    "* [A Tutorial on Reinforcement Learning II - YouTube](https://www.youtube.com/watch?v=8hK0NnG_DhY)\n",
    "* [ICML 2017 Tutorial](https://sites.google.com/view/icml17deeprl)\n",
    "* [Ray Summit 2021 RLlib Tutorial](https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers\n",
    "\n",
    "Here is a small sample of papers on various RL topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Reinforcement Learning\n",
    "\n",
    "#### PPO and TRPO\n",
    "* John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, \"Proximal Policy Optimization Algorithms\", July 2017, [arxiv](https://arxiv.org/abs/1707.06347). The paper that introduced PPO (Proximal Policy Optimization).  It is a variant of _Trust Region Policy Optimization_ (TRPO) described in the next paper.\n",
    "* John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel, \"Trust Region Policy Optimization\", February 2015, [arxiv](https://arxiv.org/abs/1502.05477). \n",
    "* OpenAI, \"Proximal Policy Optimization\", [blog post](https://openai.com/blog/openai-baselines-ppo/). An accessible introduction to PPO.\n",
    "\n",
    "#### DQN and Variants\n",
    "\n",
    "* Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et. al, \"Playing Atari with Deep Reinforcement Learning\", December 2013, [arxiv](https://arxiv.org/abs/1312.5602) (original paper describing DQN).\n",
    "* Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. \"Human-level control through deep reinforcement learning\", Nature 518, 529–533 (2015). [Nature](https://doi.org/10.1038/nature14236).\n",
    "* Dan Horgan, John Quan, David Budden, et al., \"Distributed Prioritized Experience Replay\", March 2018, [arxiv](https://arxiv.org/abs/1803.00933).\n",
    "* Matteo Hessel, Joseph Modayil, Hado van Hasselt, et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning, October 2017, [arxiv](https://arxiv.org/abs/1710.02298).\n",
    "* Dan Horgan, John Quan, David Budden, et al., \"Distributed Prioritized Experience Replay\", March 2018, [arxiv](https://arxiv.org/abs/1803.00933).\n",
    "* Hado van Hasselt, Arthur Guez, and David Silver, \"Deep Reinforcement Learning with Double Q-learning\", December 2015, [arxiv](https://arxiv.org/abs/1509.06461).\n",
    "* Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver, \"Prioritized Experience Replay\", November 2015, [arxiv](https://arxiv.org/abs/1511.05952).\n",
    "* Ziyu Wang, Tom Schaul, Matteo Hessel, et al., \"Dueling Network Architectures for Deep Reinforcement Learning\", November 2015, [arxiv](https://arxiv.org/abs/1511.06581)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommender Systems Using RL\n",
    "\n",
    "* Ken Goldberg, Theresa Roeder, Dhruv Gupta, Chris Perkins, \"Eigentaste: A Constant Time Collaborative Filtering Algorithm\", *Information Retrieval*, 4(2), 133-151 (July 2001) [pdf](https://goldberg.berkeley.edu/pubs/eigentaste.pdf).\n",
    "* Julian McAuley and Jure Leskovec, \"From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews\", [arxiv](https://arxiv.org/abs/1303.4402) (March 18, 2013).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits\n",
    "\n",
    "* Djallel Bouneffouf, Irina Rish, \"A Survey on Practical Applications of Multi-Armed and Contextual Bandits\", [arxiv](https://arxiv.org/abs/1904.10040).\n",
    "\n",
    "### Upper Confidence Bound\n",
    "\n",
    "* Lihong Li, Wei Chu, John Langford, Robert E. Schapire, \"A contextual-bandit approach to personalized news article recommendation\", Proceedings of the 19th International Conference on World Wide Web (WWW 2010), [arxiv](https://arxiv.org/abs/1003.0146).\n",
    "* Wei Chu, Lihong Li, Lev Reyzin, Robert E. Schapire (), \"Contextual bandits with linear payoff functions\" (PDF), Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS 2011), [arxiv](https://arxiv.org/abs/1003.0146).\n",
    "* T.L. Lai, Herbert Robbins, “Asymptotically efficient adaptive allocation rules”, Advances in Applied Mathematics, Volume 6, Issue 1 (1985), pp 4-22, [link](https://doi.org/10.1016/0196-8858(85)90002-8).\n",
    "* M N Katehakis and H Robbins, “Sequential choice from several populations”, Proc Natl Acad Sci U S A. 1995 Sep 12; 92(19): 8584–8585, [link](https://doi.org/10.1073/pnas.92.19.8584).\n",
    "* Warrick Masson, Pravesh Ranchod, George Konidaris, \"Reinforcement Learning with Parameterized Actions\" [arxiv](https://arxiv.org/abs/1509.01644).\n",
    "\n",
    "### Thompson Sampling\n",
    "\n",
    "* William R. Thompson, \"On the likelihood that one unknown probability exceeds another in view of the evidence of two samples\". Biometrika, 25(3–4):285–294, 1933, [pdf](https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf).\n",
    "* Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband and Zheng Wen, \"A Tutorial on Thompson Sampling\", Foundations and Trends in Machine Learning: Vol. 11: No. 1, pp 1-96, 2018, [pdf](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf).\n",
    "* Carlos Riquelme, George Tucker, Jasper Snoek, “Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling”, ICLR 2018, [arxiv](https://arxiv.org/abs/1802.09127). This paper introduces the _WheelBandit_ used in our Thompson Sampling MAB lesson and also discusses exploration algorithms not discussed in this tutorial.\n",
    "* Shipra Agrawal, Navin Goyal, \"Analysis of Thompson Sampling for the Multi-armed Bandit Problem\", JMLR: Workshop and Conference Proceedings vol 23 (2012) 39.1–39.26, [pdf](http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf).\n",
    "* Shipra Agrawal, Navin Goyal, \"Thompson Sampling for Contextual Bandits with Linear Payoffs\", Proceedings of the 30th International Conference on Ma- chine Learning, Atlanta, Georgia, USA, 2013, [pdf](http://proceedings.mlr.press/v28/agrawal13.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RISELab\n",
    "\n",
    "The RISE Lab and U.C. Berkeley has many useful tutorials, videos, etc.:\n",
    "\n",
    "* [RISE Lab YouTube channel](https://www.youtube.com/channel/UCP2-wiA964pif0secCpPbfw/videos)\n",
    "* [RISE Camp 2019](https://risecamp.berkeley.edu/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
