{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - Linear Thompson Sampling\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson uses a second exploration strategy we discussed briefly in lesson [02 Exploration vs. Exploitation Strategies](02-Exploration-vs-Exploitation-Strategies.ipynb), _Thompson Sampling_, with a linear variant, [LinTS](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wheel Bandit\n",
    "\n",
    "We'll use it on the `Wheel Bandit` problem ([RLlib discrete.py source code](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/discrete.py)), which is an artificial problem designed to force exploration. It is described in the paper [Deep Bayesian Bandits Showdown](https://arxiv.org/abs/1802.09127) (see _The Wheel Bandit_ section). The paper uses it to  model 2D contexts, but it can be generalized to more than two dimensions.\n",
    "\n",
    "You can visualize this problem as a wheel (circle) with four other regions around it. An exploration parameter delta $\\delta$ defines a threshold, such that if the norm of the context vector is less than or equal to delta (inside the “wheel”) then the leader action is taken (conventionally numbered `1`). Otherwise, the other four actions are explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From figure 3 in [Deep Bayesian Bandits Showdown](https://arxiv.org/abs/1802.09127), the Wheel Bandit can be visualized this way:\n",
    "\n",
    "![Wheel Bandit](../../images/rllib/Wheel-Bandit.png)\n",
    "\n",
    "The radius of the entire colored circle is 1.0, while the radius of the blue \"core\" is $\\delta$.\n",
    "\n",
    "Contexts are sampled randomly within the unit circle (radius 1.0). The optimal action for the blue, red, green, black, or yellow region is the action 1, 2, 3, 4, or 5, respectively. In other words, if the context is in the blue region, radius < $\\delta$, action 1 is optimal, if it is in the upper-right-hand quadrant with radius between $\\delta$ and 1.0, then action 2 is optimal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\delta$ controls how aggressively we explore. The reward $r$ for each action and context combination are based on a normal distribution as follows:\n",
    "\n",
    "Action 1 offers the reward, $r \\sim \\mathcal{N}({\\mu_1,\\sigma^2})$, independent of context.\n",
    "\n",
    "Actions 2-5 offer the reward, $r \\sim \\mathcal{N}({\\mu_2,\\sigma^2})$ where $\\mu_2 < \\mu_1$, _when they are suboptimal choices_. When they are optimal, the reward is $r \\sim \\mathcal{N}({\\mu_3,\\sigma^2})$ where $\\mu_3 \\gg \\mu_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to $\\delta$, the parameters $\\mu_1$, $\\mu_2$ $\\mu_3$, and $\\sigma$ are configurable. The default values for these parameters in the paper and in the [RLlib implementation](https://github.com/ray-project/ray/blob/master/rllib/contrib/bandits/envs/discrete.py) are as follows:\n",
    "\n",
    "```python\n",
    "DEFAULT_CONFIG_WHEEL = {\n",
    "    \"delta\": 0.5,\n",
    "    \"mu_1\": 1.2,\n",
    "    \"mu_2\": 1.0,\n",
    "    \"mu_3\": 50.0,\n",
    "    \"std\": 0.01  # sigma\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the probability of a context randomly falling in the high-reward region (not blue) is 1 − $\\delta^2$. Therefore, the difficulty of the problem increases with $\\delta$, and algorithms used with this bandit are more likely to get stuck repeatedly selecting action 1 for large $\\delta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Wheel Bandit with Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the import in the next cell of `LinTSTrainer` and how it is used below when setting up the _Tune_ job. For the `LinUCB` example in the [previous lesson](04-Linear-Upper-Confidence-bound.ipynb), we didn't import the corresponding `LinUCBTrainer`, but passed a \"magic\" string to Tune, `contrib/LinUCB`, which RLlib already knows how to associate with the corresponding `LinUCBTrainer` implementation. Passing the class explicitly, as we do here, is an alternative. The [RLlib environments documentation](https://docs.ray.io/en/latest/rllib-env.html) discusses these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray.rllib.contrib.bandits.agents import LinTSTrainer\n",
    "from ray.rllib.contrib.bandits.agents.lin_ts import TS_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import WheelBanditEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh_util import plot_cumulative_regret, plot_wheel_bandit_model_weights\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "import bokeh\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wbe = WheelBanditEnv()\n",
    "wbe.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effective number of `training_iterations` will be `20 * timesteps_per_iteration == 2,000` where the timesteps per iteration is `100` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_CONFIG[\"env\"] = WheelBanditEnv\n",
    "\n",
    "training_iterations = 20\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's in the standard config object for _LinTS_ anyway??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TS_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Ray..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\n",
    "    LinTSTrainer,\n",
    "    config=TS_CONFIG,\n",
    "    stop={\"training_iteration\": training_iterations},\n",
    "    num_samples=2,\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze cumulative regrets of the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for key, df_trial in analysis.trial_dataframes.items():\n",
    "    df = df.append(df_trial, ignore_index=True)\n",
    "\n",
    "regrets = df \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, here is an [image](../../images/rllib/LinTS-Cumulative-Regret-05.png) from a previous run. How similar is your graph? We have observed a great deal of variability from one run to the next, more than we have seen with _LinUCB_. This suggests that extra caution is required when using _LinTS_ to ensure that good results are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can restore a trainer from a checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = analysis.trials[0]\n",
    "trainer = LinTSTrainer(config=TS_CONFIG)\n",
    "trainer.restore(trial.checkpoint.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model to plot arm weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.get_policy().model\n",
    "means = [model.arms[i].theta.numpy() for i in range(5)]\n",
    "covs = [model.arms[i].covariance.numpy() for i in range(5)]\n",
    "model, means, covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot weight distributions for different arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wheel_bandit_model_weights(means, covs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an [image](../../images/rllib/LinTS-Weight-Distribution-of-Arms-05.png) from a previous run. How similar is your graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Experiment with different $\\delta$ values, for example 0.7 and 0.9. What do the cumulative regret and weights graphs look like? \n",
    "\n",
    "You can set the $\\delta$ value like this:\n",
    "\n",
    "```python\n",
    "TS_CONFIG[\"delta\"] = 0.7\n",
    "```\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
