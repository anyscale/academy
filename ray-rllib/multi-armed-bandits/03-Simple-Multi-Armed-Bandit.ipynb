{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - A Simple Bandit Example\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with three arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random, time\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bandit as a subclass of an OpenAI Gym environment. We set the action space to have three discrete variables, one action for each arm, and an observation space (the context) in the range -1.0 to 1.0, inclusive. (See the [configuring environments](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments) documentation for more details about creating custom environments.)\n",
    "\n",
    "There are two contexts defined. Note that we'll randomly pick one of them to use when `reset` is called, but it stays fixed (static) throughout the episode (the set of steps between calls to `reset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {        # 2 contexts: -1 and 1\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the definition of `self.rewards_for_context`. For context `-1.`, choosing the **third** arm (index 2 in the array) maximizes the reward, yielding `10.0` for each pull. Similarly, for context `1.`, choosing the **first** arm (index 0 in the array) maximizes the reward. It is never advantageous to choose the second arm.\n",
    "\n",
    "We'll see if our training results agree ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try repeating the next two code cells enough times to see the `current_context` set to `1.0` and `-1.0`, which is initialized randomly in `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBandit()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bandit.current_context` and the observation of the current environment will remain fixed through the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `current_context`. If it's `1.0`, does the `0` (first) action yield the highest reward and lowest regret? If it's `-1.0`, does the `2` (third) action yield the highest reward and lowest regret? The `1` (second) action always returns `0` reward, so it's never optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LinUCB\n",
    "\n",
    "For this simple example, we can easily determine the best actions to take. Let's see how well our system does. We'll train with [LinUCB](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb), a linear version of _Upper Confidence Bound_, for the exploration-exploitation strategy. _LinUCB_ assumes a linear dependency between the expected reward of an action and its context. Recall that a linear function is of the form $z = ax + by + c$, for example, where $x$, $y$, and $z$ are variables and $a$, $b$, and $c$ are constants. _LinUCB_ models the representation space using a set of linear predictors. Hence, the $Q_t(a)$ _value_ function discussed for UCB in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb) is assumed to be a linear function here.\n",
    "\n",
    "Look again at how we defined `rewards_for_context`. Is it linear as expected for _LinUCB_?\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Yes, for each arm, the reward is linear in the context. For example, the first arm has a reward of `-10` for context `-1.0` and `10` for context `1.0`. Crucially, the _same_ linear function that works for the first arm will work for the other two arms if you multiplied the constants in the linear function by `0` and `-1`, respectively. Hence, we expect _LinUCB_ to work well for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Tune to train the policy for this bandit. But first, we want to make sure that Ray is running and we want to initialize Ray in this \"driver\" program explicitly. \n",
    "\n",
    "> **Tip:** Note that the call to `ray.tune.run` in a few cells below would handle Ray initialization for us, if we omitted the `ray_auto_init=False`, but that can lead to multiple Ray clusters running, so we'll control this more tightly. Hence, we now initialize Ray, then setup our experiment.\n",
    "\n",
    "The following shell command will probably print _INFO: Ray is already running._ If not, follow the instructions it prints to start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize Ray in this \"driver\" program (notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                        progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                        verbose=2,     # Change to 0 or 1 to reduce the output.\n",
    "                        ray_auto_init=False,    # Don't allow Tune to initialize Ray.\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the final data as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to inspect the progression of training is to use TensorBoard.\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have many data sets plotted from previous tutorial lessons. In the _Runs_ on the left, look for one named something like this:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "If you have several of them, you want the one with the latest timestamp. To select just that one, click _toggler all runs_ below the list of runs, then select the one you want. You should see something like [this image](../../images/rllib/TensorBoard1.png).\n",
    "\n",
    "The graph for the metric we were optimizing, the mean reward, is shown with a rectangle surrounding it. It improved steadily during the training runs. For this simple example, the reward mean is easily found in 200 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Optional)\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                        progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                        verbose=2,              # Change to 0 or 1 to reduce the output.\n",
    "                        ray_auto_init=False)    # Don't allow Tune to initialize Ray.\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue exploring usage of _LinUCB_ in the next lesson, [04 Linear Upper Confidence Bound](04-Linear-Upper-Confidence-Bound.ipynb) and _LinTS_ in the following lesson, [05 Thompson Sampling](05-Linear-Thompson-Sampling.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
