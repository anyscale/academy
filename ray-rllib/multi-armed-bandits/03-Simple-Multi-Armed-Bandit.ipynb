{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - A Simple Bandit Example\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with three arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from ray import tune\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bandit as a subclass of an OpenAI Gym environment. We set the action space to have three discrete variables, one action for each arm, and an observation space (the context) in the range -1.0 to 1.0, inclusive. \n",
    "\n",
    "There are two contexts defined. Note that we'll randomly pick one of them to use when `reset` is called, but it stays fixed (static) throughout the episode (the set of steps between calls to `reset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the definition of `self.rewards_for_context`. For context `-1.`, choosing the **third** arm (index 2 in the array) maximizes the reward, yielding `10.0` for each pull. Similarly, for context `1.`, choosing the **first** arm (index 0 in the array) maximizes the reward. It is never advantageous to choose the second arm.\n",
    "\n",
    "We'll see if our training results agree ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try repeating the next two code cells enough times to see the `current_context` set to `1.0` and `-1.0`, which is done randomly by `reset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [ 1. -1.], bandit = SimpleContextualBandit(action_space=Discrete(3), observation_space=Box(2,), current_context=-1.0, rewards per context={-1.0: [-10, 0, 10], 1.0: [10, 0, -10]})'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBandit()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bandit.current_context` and the observation of the current environment will remain fixed through the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_context = -1.0\n",
      "observation = [ 1. -1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], action = 2, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], action = 2, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [ 1. -1.], action = 0, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [ 1. -1.], action = 2, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], action = 2, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [ 1. -1.], action = 0, reward =  -10, done = True , info = {'regret': 20}\n"
     ]
    }
   ],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `current_context`. If it's `1.0`, does the `0` (first) action yield the highest reward and lowest regret? If it's `-1.0`, does the `2` (third) action yield the highest reward and lowest regret?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LinUCB\n",
    "\n",
    "For this simple example, we can easily determine the best actions to take. Let's see how well our system does. We'll train with [LinUCB](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb), a linear version of _Upper Confidence Bound_, for the exploration-exploitation strategy. _LinUCB_ assumes a linear dependency between the expected reward of an action and its context. Recall that a linear function is of the form $z = ax + by + c$, for example, where $x$, $y$, and $z$ are variables and $a$, $b$, and $c$ are constants. _LinUCB_ models the representation space using a set of linear predictors. Hence, the $Q_t(a)$ function discussed for UCB in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb) is a linear function here.\n",
    "\n",
    "Look again at how we defined `rewards_for_context`. Is it linear as expected for _LinUCB_?\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Yes, for each arm, the reward is linear in the context. For example, the first arm has a reward of `-10` for context `-1.0` and `10` for context `1.0`. Crucially, the _same_ linear function that works for the first arm will work for the other two arms if you multiplied the constants in the linear function by `0` and `-1`, respectively. Hence, we expect _LinUCB_ to work well for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Tune to train the policy for this bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/8 CPUs, 0/0 GPUs, 0.0/4.15 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=10547)\u001b[0m 2020-06-08 17:41:00,908\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=10547)\u001b[0m 2020-06-08 17:41:00,912\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=10547)\u001b[0m 2020-06-08 17:41:00,926\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=10547)\u001b[0m 2020-06-08 17:41:00,926\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for contrib_LinUCB_SimpleContextualBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_17-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 100\n",
      "  experiment_id: 07b5701998cb4ca1868d1530d01db2f9\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.248\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.248\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.0001399517059326172\n",
      "    num_steps_sampled: 100\n",
      "    num_steps_trained: 100\n",
      "    opt_peak_throughput: 4035.313\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1412.366\n",
      "    sample_time_ms: 0.708\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.0001399517059326172\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 4035.313\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf:\n",
      "    cpu_util_percent: 19.7\n",
      "    ram_util_percent: 61.9\n",
      "  pid: 10547\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1412.366\n",
      "  sample_time_ms: 0.708\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025583965943591434\n",
      "    mean_inference_ms: 0.4129338972639331\n",
      "    mean_processing_ms: 0.32106484517012485\n",
      "  time_since_restore: 0.11829900741577148\n",
      "  time_this_iter_s: 0.11829900741577148\n",
      "  time_total_s: 0.11829900741577148\n",
      "  timestamp: 1591663261\n",
      "  timesteps_since_restore: 100\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 100\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n",
      "Result for contrib_LinUCB_SimpleContextualBandit_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-08_17-41-01\n",
      "  done: true\n",
      "  episode_len_mean: 1.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 10.0\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 200\n",
      "  experiment_id: 07b5701998cb4ca1868d1530d01db2f9\n",
      "  experiment_tag: '0'\n",
      "  grad_time_ms: 0.274\n",
      "  hostname: DWAnyscaleMBP.local\n",
      "  info:\n",
      "    grad_time_ms: 0.274\n",
      "    learner:\n",
      "      cumulative_regret: 30.0\n",
      "      update_latency: 0.0001418590545654297\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "    opt_peak_throughput: 3648.173\n",
      "    opt_samples: 1.0\n",
      "    sample_peak_throughput: 1286.636\n",
      "    sample_time_ms: 0.777\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 2\n",
      "  learner:\n",
      "    cumulative_regret: 30.0\n",
      "    update_latency: 0.0001418590545654297\n",
      "  node_ip: 192.168.1.149\n",
      "  num_healthy_workers: 0\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  off_policy_estimator: {}\n",
      "  opt_peak_throughput: 3648.173\n",
      "  opt_samples: 1.0\n",
      "  optimizer_steps_this_iter: 100\n",
      "  perf: {}\n",
      "  pid: 10547\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sample_peak_throughput: 1286.636\n",
      "  sample_time_ms: 0.777\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.025084955775322607\n",
      "    mean_inference_ms: 0.40028818804233224\n",
      "    mean_processing_ms: 0.2965903400781736\n",
      "  time_since_restore: 0.22234797477722168\n",
      "  time_this_iter_s: 0.1040489673614502\n",
      "  time_total_s: 0.22234797477722168\n",
      "  timestamp: 1591663261\n",
      "  timesteps_since_restore: 200\n",
      "  timesteps_this_iter: 100\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 2\n",
      "  trial_id: '00000'\n",
      "  update_time_ms: 0.001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.15 GiB heap, 0.0/1.42 GiB objects<br>Result logdir: /Users/deanwampler/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.222348</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">      10</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 4.8670172691345215 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the final data as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_steps_trained</th>\n",
       "      <th>num_steps_sampled</th>\n",
       "      <th>sample_time_ms</th>\n",
       "      <th>grad_time_ms</th>\n",
       "      <th>update_time_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>info/sample_peak_throughput</th>\n",
       "      <th>info/opt_samples</th>\n",
       "      <th>learner/cumulative_regret</th>\n",
       "      <th>learner/update_latency</th>\n",
       "      <th>perf/cpu_util_percent</th>\n",
       "      <th>perf/ram_util_percent</th>\n",
       "      <th>info/learner/cumulative_regret</th>\n",
       "      <th>info/learner/update_latency</th>\n",
       "      <th>config/env</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>1286.636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>&lt;class '__main__.SimpleContextualBandit'&gt;</td>\n",
       "      <td>/Users/deanwampler/ray_results/contrib/LinUCB/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0                10.0                10.0                 10.0   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_steps_trained  num_steps_sampled  \\\n",
       "0               1.0                 100                200                200   \n",
       "\n",
       "   sample_time_ms  grad_time_ms  update_time_ms  ...  \\\n",
       "0           0.777         0.274           0.001  ...   \n",
       "\n",
       "   info/sample_peak_throughput  info/opt_samples  learner/cumulative_regret  \\\n",
       "0                     1286.636               1.0                       30.0   \n",
       "\n",
       "   learner/update_latency  perf/cpu_util_percent  perf/ram_util_percent  \\\n",
       "0                0.000142                    NaN                    NaN   \n",
       "\n",
       "   info/learner/cumulative_regret  info/learner/update_latency  \\\n",
       "0                            30.0                     0.000142   \n",
       "\n",
       "                                  config/env  \\\n",
       "0  <class '__main__.SimpleContextualBandit'>   \n",
       "\n",
       "                                              logdir  \n",
       "0  /Users/deanwampler/ray_results/contrib/LinUCB/...  \n",
       "\n",
       "[1 rows x 54 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to inspect the progression of training is to use TensorBoard.\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have many data sets plotted from previous tutorial lessons. In the _Runs_ on the left, look for one named something like this:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "If you have several of them, you want the one with the latest timestamp. To select just that one, click _toggler all runs_ below the list of runs, then select the one you want. You should see something like [this image](../../images/rllib/TensorBoard1.png).\n",
    "\n",
    "The graph for the metric we were optimizing, the mean reward, is shown with a rectangle surrounding it. It improved steadily during the training runs. For this simple example, the reward mean is easily found in 200 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue exploring usage of _LinUCB_ in the next lesson, [04 Linear Upper Confidence Bound](04-Linear-Upper-Confidence-Bound.ipynb.ipynb) and _LinTS_ in the following lesson, [05 Thompson Sampling](05-Thompson-Sampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
