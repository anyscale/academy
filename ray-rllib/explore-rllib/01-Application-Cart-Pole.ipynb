{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Explore RLlib - Sample Application: CartPole\n",
    "\n",
    "© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "We were briefly introduced to the `CartPole` example and the OpenAI gym `CartPole-v1` environment ([gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)) in the [reinforcement learning introduction](../01-Introduction-to-Reinforcement-Learning.ipynb). This lesson uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy for `CartPole`.\n",
    "\n",
    "Recall that the `gym` Python module provides MDP interfaces to a variety of simulators, like the simple simulator for the physics of balancing a pole on a cart that is used by the CartPole environment. The `CartPole` problem is described at https://gym.openai.com/envs/CartPole-v1.\n",
    "\n",
    "![Cart Pole](../../images/rllib/Cart-Pole.png)\n",
    "\n",
    "([source](https://gym.openai.com/envs/CartPole-v1/))\n",
    "\n",
    "Even though this is a relatively simple and quick example to run, its results can be understood quite visually. `CartPole` is one of OpenAI Gym's [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "* [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077), AG Barto, RS Sutton, and CW Anderson, *IEEE Transactions on Systems, Man, and Cybernetics* (1983). The same Sutton and Barto who wrote [*Reinforcement Learning: An Introduction*](https://mitpress.mit.edu/books/reinforcement-learning-second-edition).\n",
    "* [\"Cartpole - Introduction to Reinforcement Learning (DQN - Deep Q-Learning)\"](https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288), [Greg Surma](https://twitter.com/GSurma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import Ray and the PPO module in RLlib, then start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model *checkpoints* will get saved after each iteration into directories under `tmp/ppo/cart`, i.e., relative to this directory. \n",
    "The default directories for checkpoints are `$HOME/ray_results/<algo_env>/.../checkpoint_N`.\n",
    "\n",
    "> **Note:** If you prefer to use a different directory root, change it in the next cell _and_ in the `rllib rollout` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/cart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up output of previous lessons (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:35:23,827\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard is useful for monitoring Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy with the [`CartPole-v1` environment](https://gym.openai.com/envs/CartPole-v1/).\n",
    "\n",
    "If you've gone through the _Multi-Armed Bandits_ lessons, you may recall that we used [Ray Tune](http://tune.io), the Ray Hyperparameter Tuning system, to drive training. Here we'll do it ourselves.\n",
    "\n",
    "By default, training runs for `10` iterations. Increase the `N_ITER` setting if you want to train longer and see the resulting rewards improve. However, if the max score of `200` is achieved early, you can use a smaller number of iterations.\n",
    "\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used. In a cluster, these actors will be spread over the available nodes.\n",
    "- `num_sgd_iter` is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO, for each _minibatch_ (\"chunk\") of training data. Using minibatches is more efficient than training with one record at a time.\n",
    "- `sgd_minibatch_size` is the SGD minibatch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers. Here, we have two hidden layers of size 100, each.\n",
    "- `num_cpus_per_worker` when set to 0 prevents Ray from pinning a CPU core to each worker, which means we could run out of workers in a constrained environment like a laptop or a cloud VM.\n",
    "\n",
    "> **Note:** If you change the values shown for `config['model']['fcnet_hiddens']`, make the same change in the `rllib rollout` command below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_ENV = \"CartPole-v1\"                      # Specifies the OpenAI Gym environment for Cart Pole\n",
    "N_ITER = 10                                     # Number of training runs.\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
    "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
    "\n",
    "# Other settings we might adjust:\n",
    "config[\"num_workers\"] = 1                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "config[\"num_sgd_iter\"] = 10                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "config[\"sgd_minibatch_size\"] = 250              # The amount of data records per minibatch\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    #\n",
    "config[\"num_cpus_per_worker\"] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiousity, let's see what configuration settings are defined for PPO. Note in particular the parameters for the deep learning `model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 5e-05,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [100, 50],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': None,\n",
       " 'min_train_timesteps_per_reporting': None,\n",
       " 'min_sample_timesteps_per_reporting': None,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:35:36,457\tINFO trainer.py:2140 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-03-17 16:35:36,461\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-03-17 16:35:36,462\tINFO trainer.py:779 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2022-03-17 16:35:44,506\tWARNING deprecation.py:45 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2022-03-17 16:35:44,967\tWARNING deprecation.py:45 -- DeprecationWarning: `clear_buffer` has been deprecated. Use `Filter.reset_buffer()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward:   9.0000/ 21.2553/ 74.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000001/checkpoint-1\n",
      "  1: Min/Mean/Max reward:  10.0000/ 29.7594/ 87.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000002/checkpoint-2\n",
      "  2: Min/Mean/Max reward:  12.0000/ 40.4000/123.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000003/checkpoint-3\n",
      "  3: Min/Mean/Max reward:  13.0000/ 55.6100/163.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000004/checkpoint-4\n",
      "  4: Min/Mean/Max reward:  13.0000/ 67.5600/169.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000005/checkpoint-5\n",
      "  5: Min/Mean/Max reward:  21.0000/ 86.5800/220.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000006/checkpoint-6\n",
      "  6: Min/Mean/Max reward:  23.0000/115.5200/412.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000007/checkpoint-7\n",
      "  7: Min/Mean/Max reward:  33.0000/139.7600/500.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000008/checkpoint-8\n",
      "  8: Min/Mean/Max reward:  42.0000/166.6600/500.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000009/checkpoint-9\n",
      "  9: Min/Mean/Max reward:  49.0000/195.6300/500.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000010/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "The episode rewards should increase after multiple iterations. Try tweaking the config parameters. Smaller values for the `num_sgd_iter`, `sgd_minibatch_size`, or the `model`'s `fcnet_hiddens` will train faster, but take longer to improve the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.255319</td>\n",
       "      <td>74.0</td>\n",
       "      <td>21.255319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>29.759398</td>\n",
       "      <td>87.0</td>\n",
       "      <td>29.759398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>40.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>55.610000</td>\n",
       "      <td>163.0</td>\n",
       "      <td>55.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>67.560000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>67.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>86.580000</td>\n",
       "      <td>220.0</td>\n",
       "      <td>86.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>115.520000</td>\n",
       "      <td>412.0</td>\n",
       "      <td>115.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>139.760000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>139.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>166.660000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>166.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>49.0</td>\n",
       "      <td>195.630000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>195.630000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  episode_reward_min  episode_reward_mean  episode_reward_max  \\\n",
       "0  0                 9.0            21.255319                74.0   \n",
       "1  1                10.0            29.759398                87.0   \n",
       "2  2                12.0            40.400000               123.0   \n",
       "3  3                13.0            55.610000               163.0   \n",
       "4  4                13.0            67.560000               169.0   \n",
       "5  5                21.0            86.580000               220.0   \n",
       "6  6                23.0           115.520000               412.0   \n",
       "7  7                33.0           139.760000               500.0   \n",
       "8  8                42.0           166.660000               500.0   \n",
       "9  9                49.0           195.630000               500.0   \n",
       "\n",
       "   episode_len_mean  \n",
       "0         21.255319  \n",
       "1         29.759398  \n",
       "2         40.400000  \n",
       "3         55.610000  \n",
       "4         67.560000  \n",
       "5         86.580000  \n",
       "6        115.520000  \n",
       "7        139.760000  \n",
       "8        166.660000  \n",
       "9        195.630000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3FElEQVR4nO3deXhU1f3H8ffJvi9kh0QSBAn7KosRBJFNQK3VuqFSFK1WxVqr2Na1tbVKFdcfgjuiqLgRqogKtIoiECBAIOwBQvaQfU/m/P6YyZCQnSx3MvN9PU+euXPnLifD5MOZc+/9XqW1RgghhP1yMroBQgghOpcEvRBC2DkJeiGEsHMS9EIIYeck6IUQws65tGVhJycn7enp2VltEUIIu1RaWqq11oZ1rNsU9J6enpSUlHRWW4QQwi4ppcqM3L8M3QghhJ2ToBdCCDsnQS+EEHauTWP0QnSUqqoqUlNTKS8vN7opQnQYDw8PIiMjcXV1Nbop9UjQC0Okpqbi6+tLdHQ0SimjmyNEu2mtyc3NJTU1lZiYGKObU48M3QhDlJeXExQUJCEv7IZSiqCgoHP6lqqUSlFK7VFK7VJKbbfM66GU+lYpdcjyGGiZr5RSLymlDiuldiulRra0fQl6YRgJeWFv2vmZnqy1Hq61Hm15vgj4XmvdD/je8hxgJtDP8nMH8H8tbViGboQQ9VSbqvnk4CfkluUa3RSbcuewO3F16tKx9yuBSZbpd4FNwMOW+e9pc435LUqpAKVUhNY6vakNSdALIaxM2sRjmx8j/mg8CvnGVdeCoQvas7pL7ZCMxTKt9bI6zzWwXimlgdctr4XVCe8MIMwy3Qs4WWfdVMs8CXoh2uuxxx5j4sSJXHbZZe3ajo+PD8XFxR3Uqo6jtebvW/5O/NF47h1xL3cMvcP62qZNm1i8eDFr1641sIVNKysrY8aMGWzYsAFnZ+cGr1900UX89NNPzW4jOjqa7du3ExwcXG/+pk2bcHNzw93ZHYBXXnkFLy8v5s+f35YmVtcZkmnMxVrrU0qpUOBbpVRy3Re11tryn8A5kTF6IVrpqaeeanfIdwStNSaTqcO3uXj7Yj45+Am3D7md2wbd1qHb72xvvfUWV199dYOQr66uBmgx5JuzadOmeuvPnz+fl19++Zy31xit9SnLYxbwOTAGyFRKRQBYHrMsi58CouqsHmmZ1yTp0QvDPRmfxL60wg7d5sCefjw+Z1Czy7z//vu89NJLVFZWMnbsWF577TWcnZ3x8fFhwYIFrF+/nvDwcFatWkVISAjz5s1j9uzZXHPNNSxatIg1a9bg4uLCtGnTWLx4MSkpKcyfP5+cnBxCQkJ4++23Oe+88zh27Bg33ngjxcXFXHnllfXa8Nxzz/Hxxx9TUVHBr371K5588slG25qSksL06dMZO3YsCQkJfPXVV3z88ccN1n3uuedwd3fnvvvu4w9/+AOJiYls2LCBDRs28Oabb7Jy5Uruuusutm3bRllZGddccw1PPvkkryW+xnv73qN3bm/euf0dwh4KIyAggPvvvx8vLy8uvvjiZt/LJ554gmPHjnH06FFOnDjBCy+8wJYtW/j666/p1asX8fHxuLq6kpCQwAMPPEBxcTHBwcG88847REREsHz5cpYtW0ZlZSV9+/ZlxYoVeHl5MW/ePPz8/Ni+fTsZGRk8++yzXHPNNQ32v3LlSj744APAHMyPPvoogYGBJCcnc/DgQeu3KJPJxD333MOGDRuIiorC1dWV+fPnW7f58ssvEx8fT1VVFZ988gkeHh4sXboUZ2dn3n//fV5++WUmTJhAdHQ0W7duZcyYMc2+L62hlPIGnLTWRZbpacBTwBrgVuAZy+OXllXWAPcopVYBY4GC5sbnQXr0wkHt37+fjz76iM2bN7Nr1y6cnZ1ZuXIlACUlJYwePZqkpCQuueSSBuGbm5vL559/TlJSErt37+avf/0rAPfeey+33noru3fv5qabbuK+++4DYOHChdx1113s2bOHiIgI63bWr1/PoUOH2Lp1K7t27SIhIYH//e9/Tbb50KFD3H333SQlJXHgwIFG150wYQI//PADANu3b6e4uJiqqip++OEHJk6cCMDTTz/N9u3b2b17N//97395+tunWZq4lModlUyonsDOHTu56qqrWLBgAfHx8SQkJJCRkdHie3rkyBE2bNjAmjVrmDt3LpMnT2bPnj14enryn//8h6qqKu69915Wr15NQkIC8+fP5y9/+QsAV199Ndu2bSMxMZEBAwbw5ptvWrebnp7Ojz/+yNq1a1m0aFGD/VZWVnL06FGio6Ot83bs2MGLL77IwYMH6y372WefkZKSwr59+1ixYgU///xzvdeDg4PZsWMHd911F4sXLyY6Oprf/e53/OEPf2DXrl1MmDABgNGjR1vf5w4QBvyolEoEtgL/0VqvwxzwU5VSh4DLLM8BvgKOAoeB5cDdLe1AevTCcC31vDvD999/T0JCAhdeeCFgHuMNDQ0FwMnJieuuuw6AuXPncvXVV9db19/fHw8PD2677TZmz57N7NmzAfj555/57LPPALj55pt56KGHANi8eTOffvqpdf7DDz8MmIN+/fr1jBgxAoDi4mIOHTpkDeSz9e7dm3HjxjW77i233EJCQgKFhYW4u7szcuRItm/fzg8//MBLL70EwMcff8yyZcuorq6mIKaAVWmrmBk9k4+f+JjrN10PQHJyMjExMfTr18/6PixbtqyRVp0xc+ZMXF1dGTJkCDU1NcyYMQOAIUOGkJKSwoEDB9i7dy9Tp04FoKamxvof3969e/nrX/9Kfn4+xcXFTJ8+3brdq666CicnJwYOHEhmZmaD/ebk5BAQEFBv3pgxYxq9aOnHH3/k2muvxcnJifDwcCZPnlzv9dp/61GjRln/LRsTGhpKcnJyk6+3hdb6KDCskfm5wJRG5mvg923ZhwS9cEhaa2699Vb++c9/trjs2edGu7i4sHXrVr7//ntWr17NK6+8woYNG9q0jdo2PPLII9x5552tarO3t3er1o2JieGdd97hoosuYujQoWzcuJHDhw8zYMAAjh07xuLFi9m2bRv/zf0vj25+lPP1+Tw94Wk+1h/X20dbububD1Y6OTnh6upq/Z2dnJyorq5Ga82gQYMa9KIB5s2bxxdffMGwYcN455132LRpU4Pt1v7eZ/P09GxwkdK5/h61+3J2draO7zemvLyc7nRvDhm6EQ5pypQprF69mqws8/Gt06dPc/z4cQBMJhOrV68G4IMPPmgwPl1cXExBQQGXX345L7zwAomJiYD5zI5Vq1YB5jHj2q/5cXFx9ebXmj59Om+99Zb1DJxTp05Z29OS5tadMGECixcvZuLEiUyYMIGlS5cyYsQIlFIUFhbi7e3NlrwtPL75cSoPVjKb2Q3OD4+NjSUlJYUjR44A8OGHH7aqXc3p378/2dnZ1qCvqqoiKSkJgKKiIiIiIqiqqqr3HrVGYGAgNTU1rboiNS4ujk8//RSTyURmZma9/1Ca4uvrS1FRUb15Bw8eZPDgwW1qp5Ek6IVDGjhwIH//+9+ZNm0aQ4cOZerUqaSnm49neXt7s3XrVgYPHsyGDRt47LHH6q1bVFTE7NmzGTp0KBdffDHPP/88YD6Q9/bbbzN06FBWrFjBiy++CMCLL77Iq6++ypAhQzh16szJEdOmTePGG29k/PjxDBkyhGuuuaZBoDSluXUnTJhAeno648ePJywsDA8PD+t/OsOGDSPmshj+tOlPOGc6M+DgAFwa+WLv4eHBsmXLmDVrFiNHjrQOa7WHm5sbq1ev5uGHH2bYsGEMHz7cejbL3/72N8aOHUtcXByxsbFt3va0adP48ccfW1zu17/+NZGRkQwcOJC5c+cycuRI/P39m11nzpw5fP755wwfPtw6Lr9582brEFS3oLVu9Y+Xl5cWoiPs27fP6CY0ydvb2+gmdJqfTv2kR743Ul8ff70uqigyujkdJiEhQc+dO7dVyxYVmX/vnJwc3adPH52ent6mfe3YsaPZfTX22QZKdBuytqN/ZIxeCAexM2snCzcupLd/b5ZOXYqPm4/RTeowI0eOZPLkydTU1DR6wVRds2fPJj8/n8rKSh599FHCw8PbtK+cnBz+9re/tae5XU7pRg5uNMXb21vLPWNFR9i/fz8DBgwwuhk2Jzc3lylTGpxowffff09QUNA5bzcpJ4nb199OsGcwb894m2DP4JZXasTbb79tHZKqFRcXx6uvvnrObbM3jX22lVKlWutzP9LdThL0whAS9F3nUN4hfvvNb/Fx9eGdGe8Q7t22HqxoG1sMejkYK4QdSylIYcH6Bbg7ubN82nIJeQclQS+EnTpVfIrb19+ORrN8+nKifKNaXknYJTkYK4QdyirNYsH6BZRWl/L29Lfp49/H6CYJA0nQC2FnTpefZsH6BeSW5bJ82nL69+hvdJOEwWToRohWeuyxx/juu+/avR0fn847rbGwspA7v72TU8WneGXKKwwNGdoh2920aZO1pk973H777ezbt69N63zxxRc89dRTjb62Zs0annnmmUZfq9Vc25csWUJpaan1+WWXXUZeXl6b2tcdSNAL0Uq2Xo++tKqUu767i8P5h1kyeQkXhl94zvuoqalpTxOb9MYbbzBw4MA2rfPss89y990NCzRWV1dzxRVXNFrRsrXODvqbb76Z11577Zy3Z6tk6EYY7+tFkLGnY7cZPgRmNt/Ts6d69FdcfQXp49PZk7WHKaVTuLjXxW2qRw/mOyxdd911fPvttzz00EOdUo9+0qRJLF68mNGjR+Pj48PChQtZu3Ytnp6efPnll4SFhdXb7sGDB3F3d7fe9WnevHl4eHiwc+dO4uLiGDp0KNu3b+eVV17hyJEj3HTTTZSUlHDllVeyZMkSay2g4uJirrnmGvbu3cuoUaOsteXT0tKYPHkywcHBbNy4kSuuuIIJEyZYyyfbC+nRC4dkT/XotyVs4xuPb9iWsY35PeeTujEVaH09+t27d1v3ERQUxI4dOzqlHv3ZSkpKGDduHImJiUycOJHly5c3WGbz5s2MHDmy3rzU1FR++ukna42hWgsXLmThwoXs2bOHyMjIeq/t3LmTJUuWsG/fPo4ePcrmzZu577776NmzJxs3bmTjxo2AuUBaRUUFubn2dWN06dEL47XQ8+4M9lKPfvjI4ZhmmaAfTKmZwu8n/56XfvdSq+vRp6ens2/fPoYONY/l1/7enVGP/mxubm7W927UqFF8++23DZZJT08nJCSk3rxrr7220TIHP//8M1988QUAN954Iw8++KD1tTFjxljDf/jw4aSkpDT5LSU0NJS0tLR2XYlsayTohUPSdlCPftEjizgx8ATxR+N56MKHuHngzUDr69EHBgYyb968euV9O7Me/dnqLtNU/XdPT08KCgqafB/a2rbm9lWru9Wabw0ZuhEOqbvXo582bRqvH3md+KPx3DviXi71v7RN9ej9/f3JzMzk66+/bnT7nVGP/lwMGDCAw4cPt2rZcePGWb851b7fLTm71rzWmoyMjHq3JbQHEvTCIXXnevRaa3b32E31oGr4BV6+6eU21aMfMWIEsbGx3HjjjcTFxTW6j86oR38uJk6cyM6dOxu9s9TZlixZwvPPP8/QoUM5fPhwi3XmAe644w5mzJhhvaVgQkIC48aNw8XFvgY7pKiZMIQtFzXz8fGx9rJt0au7XmVp4lJujL2RRWMWNTosZE8WLlzInDlzWjy1tbS0FE9PT5RSrFq1ig8//JAvv/yyzfu64oorGq0g2lq2WNTMvv7bEsLOvbX3LZYmLuVXfX/Fw2MetvuQB/jzn//ML7/80uJyCQkJ3HPPPWitCQgI4K233mrzvgYPHtyukLdV0qMXhrDlHr2RmqtHvz57Pf/45R/MjJ7JPyf8E2en5m+w0RmkHn3LbLFHL0EvDCFB3zZfHP6CRzc/yqSoSTw/6fkGN/MWtsMWg14Oxgph49alrOPxnx5nfMR4Fl+yWEJetJkEvRA2bNPJTTzyv0cYHjKcFy99EXdn9xbXEeJsEvRC2Kif037mj5v+SP8e/Xl1yqt4utjXRTyi60jQC2GDdmTuYOHGhfT2783rU1/Hx63zShsL+ydBL0QrdVU9+qScJH7//e8J8wpj2dRl+Lu3fOFPZ+uoevTtdf/99zdZ+K01/z5PPPEEixcvbjA/Pz+/Xnni7Oxsa60eeyBBL0QrdUU9+kN5h7jzuzvxd/dn+bTlBHsGN1imqXr0Hamz6tG3R25uLlu2bGm06FtNTU27/n3ODvqQkBAiIiLYvHnzObfXlsgFU8Jw/9r6L5JPJ3foNmN7xPLwmIebXcbW6tFf+asr+WXIL7g7ubN82nLCvcOty7VUj762lv1zzz2Hu7s79913n03Wo3/qqaeIj4+nrKyMiy66iNdff52amhrGjx/Pc889x6RJk3jkkUdwcnLi6aefrrePTz/9tF4v++z2rlu3zvrv89VXX/HAAw/g7e1NXFwcR48eZe3atQDs27ePSZMmceLECe6//37uu+8+Fi1axJEjRxg+fDhTp07lueee46qrrmLlypVNlonoTqRHLxySLdaj/yH7B9JL0nnioieI8o1q0Oam6tHXrWU/YcIEfvjhB8A269Hfc889bNu2jb1791JWVsbatWtxcXHhnXfe4a677uK7775j3bp1PP744w22v3nzZkaNGlVvXm17r7/+euu88vJy7rzzTr7++msSEhLIzs6ut05ycjLffPMNW7du5cknn6SqqopnnnmG888/n127dvHcc88BMHr0aOt72d1Jj14YrqWed2ewtXr0Gk31ddUE6SDiejXeg2ysHv3ZtexvueUWEhISbLYe/caNG3n22WcpLS3l9OnTDBo0iDlz5jBo0CBuvvlmZs+ezc8//4ybm1uD7TdWm762vXUlJyfTp08fYmJiALjhhhvqtX3WrFm4u7vj7u5OaGgomZmZjf4+tXXpu4pSyhnYDpzSWs9WSsUAq4AgIAG4WWtdqZRyB94DRgG5wHVa65Tmti1BLxySrdWj35m1k1u+voV7xt2Dk2r8i/bZ9eibqmVvq/Xoy8vLufvuu9m+fTtRUVE88cQT9fa9Z88eAgICmizV7OnpWW/5c21va2vTG1CXfiGwH/CzPP8X8ILWepVSailwG/B/lsc8rXVfpdT1luUa/o9XhwzdCIdka/XoV+xbgY+LD2N9x7aq/c3VsrfVevS1IR0cHExxcbH1PQb47LPPOH36NP/73/+49957yc/Pb7B+a2vT9+/fn6NHj1q/RXz00UctrnN2XXow36928ODBLa7bEZRSkcAs4A3LcwVcCtS+Se8CV1mmr7Q8x/L6FNVCdTsJeuGQbKoe/fTxfHvsWyq2VVBd1vSdj+pqrpa9rdajDwgIYMGCBQwePJjp06dbh81ycnJYtGgRb7zxBhdccAH33HMPCxcubLD+rFmz2LRpU4v78fT05LXXXmPGjBmMGjUKX1/fFmvTBwUFERcXx+DBg/nTn/4EmIeZZs2a1fZf9NwsAR4Cak+nCgLytda1H4hUoJdluhdwEsDyeoFl+aZprVv94+XlpYXoCPv27TO6CU3y9vbu0v0t3rZYD3t3mE4vTu/S/XZHcXFxOi8vr8XlioqKtNZam0wmfdddd+nnn3++zfuaMGGCPn36dJvXa+yzDVRgHn+v/blDW3IVmA28ZpmeBKwFgoHDdZaJAvZapvcCkXVeOwIE62ayW8bohTBQaVUpnx78lKm9p9Y7nVI07t///jcnTpwgICCg2eWWL1/Ou+++S2VlJSNGjGj1fXlrZWdn88ADDxAYGNiO1tZTrbUe3cRrccAVSqnLAQ/MY/QvAgFKKRdt7rVHArVfB09hDv5UpZQL4I/5oGyTpEyxMISUKTb7MPlD/vHLP3j/8vcZFjKs2Xr0QUHNfzvvClKPvmXtKVOslJoEPKjNZ918AnyqzxyM3a21fk0p9XtgiNb6d5aDsVdrrX/T7HYl6IUR9u/fT2xsrEPcIakpJm3iii+uwN/Nn5WzVra8grB5WmuSk5M7Kuj7YD69sgewE5irta5QSnkAK4ARwGngeq310ea2K0M3whAeHh7k5uYSFBTksGH/46kfOV54nGcnPmt0U0QH0FqTm5uLh4dHe7axCdhkmT4KjGlkmXLg2rZsV4JeGCIyMpLU1NQGVy06kqXJS+nh2oPI0kj2799vdHNEB/Dw8CAyMtLoZjQgQS8M4erqar1y0REdyjvEnq17WDhyIUMGDTG6OcLOyXn0Qhhg5f6VeDh7cE2/a4xuinAAEvRCdLHT5aeJPxLPnPPnEOARYHRzhAOQoBeii60+uJpKUyU3DbjJ6KYIByFBL0QXqqqpYlXyKuJ6xnF+wPlGN0c4CAl6IbrQN8e/Ibssm7kD5xrdFOFAJOiF6CJaa97f9z4x/jFc1PMio5sjHIgEvRBdZFf2LpJyk5g7YG6TNeeF6AzyaROii6zYtwI/Nz9m95ltdFOEg5GgF6ILpBWn8f2J77nmgmvwcvUyujnCwUjQC9EFPkz+EIXihtgbjG6KcEAS9EJ0Mqk5L4wmQS9EJ/vyyJcUVRXJKZXCMBL0QnQikzaxcv9KhgYPZVjIMKObIxyUBL0Qnai25rz05oWRJOiF6EQr9q0g1CuUy3pfZnRThAOToBeikxzKO8SW9C3cEHsDrk6uRjdHODAJeiE6idScF7ZCgl6ITiA154UtkaAXohNIzXlhSyTohehgUnNe2BoJeiE6mNScF7ZGgl6IDiQ154UtkqAXogNJzXlhi+STKEQHkprzwhZJ0AvRQaTmvLBVEvRCdBCpOS9slQS9EB1Aas4LWyZBL0QHkJrzwpZJ0AvRTlJzXtg6CXoh2klqzgtbJ0EvRDtJzXnRHkopD6XUVqVUolIqSSn1pGV+jFLqF6XUYaXUR0opN8t8d8vzw5bXo1vahwS9EO0gNedFB6gALtVaDwOGAzOUUuOAfwEvaK37AnnAbZblbwPyLPNfsCzXLAl6IdpBas6L9tJmxZanrpYfDVwKrLbMfxe4yjJ9peU5ltenKKVUc/uQoBfiHEnNedEGLkqp7XV+7qj7olLKWSm1C8gCvgWOAPla62rLIqlAL8t0L+AkgOX1AiCo2Z132K8hhIORmvOiDaq11qObelFrXQMMV0oFAJ8DsR25c+nRC3EOpOa86Axa63xgIzAeCFBK1XbGI4FTlulTQBSA5XV/ILe57UrQC3EOpOa86ChKqRBLTx6llCcwFdiPOfBrD/7cCnxpmV5jeY7l9Q1aa93cPmToRog2kprzooNFAO8qpZwxd74/1lqvVUrtA1Yppf4O7ATetCz/JrBCKXUYOA1c39IOJOiFaKPamvOPjntUas6LdtNa7wZGNDL/KDCmkfnlwLVt2Yd8SoVoI6k5L7obCXoh2kBqzovuSIJeiDaQmvOiO5KgF6KVpOa86K4k6IVoJak5L7orCXohWkFqzovuTIJeiFaQmvOiO5OgF6IVpOa86M4k6IVogdScF92dBL0QLZCa86K7k6AXohlSc17YAwl6IZohNeeFPZCgF6IJUnNe2AsJeiGaIDXnhb2QoBeiEVJzXtgTCXohGlFbc37ugLlSc150e/IJFqIRUnNe2BMJeiHOIjXnhb2RoBfiLFJzXtgbCXoh6pCa88IeSdALUYfUnBf2SIJeCAupOS/slQS9EBZSc17YKwl6ISyk5rywVxL0QiA154V9k6AXDi+/PJ9/J/xbas4Lu+VidAOEMIpJm/ji8Be8kPACRZVF/HH0H6XmvLBLEvTCISWfTubvW/5OYnYiI0NH8pdxf+GCwAuMbpYQnUKCXjiUosoiXt31Kh8mf0iAewBPX/w0c/rMQSlldNOE6DQS9MIhaK356thXLN6+mNyyXH7T/zfcO+Je/N39jW6aEJ1Ogl7YvaP5R3n6l6fZmrGVwUGDeeXSVxgUPMjoZgnRZSTohd0qrSrl9d2v817Se3i5evHouEf5db9f4+zkbHTThLBSSkUB7wFhgAaWaa1fVEr1AD4CooEU4Dda6zxlHmd8EbgcKAXmaa13NLcPCXphd7TWbDixgX9t+xfpJelc1fcq/jDqD/Tw6GF004RoTDXwR631DqWUL5CglPoWmAd8r7V+Rim1CFgEPAzMBPpZfsYC/2d5bJIEvbArJwtP8o+t/+DHUz/SL7Af7018jxGhI4xulhBN0lqnA+mW6SKl1H6gF3AlMMmy2LvAJsxBfyXwntZaA1uUUgFKqQjLdholQS/sQkVNBW/teYs39ryBi5MLfxr9J24ccCMuTvIRF+1XXFGNj3u7PksuSqntdZ4v01ovO3shpVQ0MAL4BQirE94ZmId2wPyfwMk6q6Va5knQC/v146kf+ccv/+Bk0UlmRs/kwQsfJNQr1OhmiW7udEklX+9NJz4xjaS0Qrb95TI8XM/5+E611np0cwsopXyAT4H7tdaFdU/51VprpZQ+151L0ItuK6Mkg39t/RffnfiOaL9olk1dxvie441ulujGisqrWJ+USfzuNH48lEO1SdMnxJvbLo6hssbUnqBvllLKFXPIr9Raf2aZnVk7JKOUigCyLPNPAVF1Vo+0zGuSBL3odqpqqlixfwVLE5eitea+Efdx66BbcXN2M7ppohsqq6xhQ3IW8YlpbDiQRWW1iV4Bntw+oQ9zhkUwMMKvUy+os5xF8yawX2v9fJ2X1gC3As9YHr+sM/8epdQqzAdhC5obnwdQ5vH81vH29tYlJSWt/w2E6GDbMrbx9JanOVJwhElRk1g0ZhG9fHoZ3SzRzVRWm/jhUDbxiWl8uy+TksoaQnzdmTUkgjnDejLyvIAODXelVKnW2ruJ1y4GfgD2ACbL7D9jHqf/GDgPOI759MrTlv8YXgFmYD698rda6+0NNlx3HxL0ojvIKcvh39v/zdqja+nl04tFYxYxKWqS0c0S3UiNSbPlaC5rdqWxLimDgrIq/D1duXxIOHOG9mRsnyCcnTqn595c0HcFGboRNq3aVM1HBz7ilZ2vUFFTwR1D7+D2Ibfj6eJpdNNEN2AyaXaezGPNrjT+syeDnOIKvN2cmTYonDnDIri4bwhuLvZfrV2CXtisxOxEnt7yNPtP72d8xHj+PPbPRPtHG90sYeO01iSlFRKfmMba3emcyi/D3cWJS2NDuWJYTybHhnbaQVVbJUEvbE5+eT5Ldizh00OfEuoZyuJLFjOt9zSpMCmadTiriDWJ6axNTONoTgkuToqJF4Tw4PQLuGxAGL4ejnvnMAl6YTNM2sTnhz5nyY4lFFUWccvAW7h7+N14uxo2tCls3MnTpcTvTmPNrjSSM4pQCsb3CWLBxD7MGBROoLeciQUS9MJGJJ9O5m9b/sbu7N1yIxDRrMzCctbuNl/ItOtkPgAjzwvg8TkDmTUkglA/D2MbaIPkrBvR5bTWFFUVUVBRQEFFAWuPrrXeCOSBUQ9wxflXyDCNqKfuVaq/HDuN1jAwwo85w3oye2gEUT28jG5is+SsG9Ft1Q3swopC8ivyzeFdWWAN8drn+RX5FFYUmpetLKRG11i3o1ByIxDRQEFZFd/tO+sq1WBv7ru0H3OG9aRvqI/RTew2JOhFhwX22bxdvfF388ff3fwT4R1BgHsAfm5+1nkB7gFE+0XL2TQCgOyiCr7dl8m6pAx+OmwO9668StVeSdA7oPzyfFYmr2R9ynryyvM6LLD93f2ty/m5++Hq5LhnOYjWS80r5ZukTL7Zm8G24+Zhmd5BXtx2cQzTB4czIqpjr1J1RDJG70AySzJ5b997fHLwE8qqyxgfMZ4o3yhrWJ8d2H7u5iCXwBYd7XBWMd8kZbBubwZ7ThUAEBvuy/RB4cwYHE5suK9dhbuM0YtOd7LwJG8lvcWXh7/EpE3MjJnJ/MHz6RfYz+imCQdRexHTur0ZrEvK4HBWMQDDowJYNDOW6YPCiQmW02g7iwS9HTuYd5A397zJupR1OCtnftX3V8wbPI8o36iWVxainWpMmh0n8szhvjeDU/llOCkYGxPELeN7M21gOOH+cipkV5Cgt0OJ2Ym8secNNp3chKeLJ7cMvIVbBt5CiFeI0U0Tdq6y2sSWo7msS8pgfVImOcUVuDk7cXG/YBZO6cdlA8PoIRcxdTkJejuhtWZL+hbe3PMmv2T8gr+7P3cPu5sbB9wopyyKTlVWWcN/D2azPimD7/ZnUlhejZebM5NjQ5k+KJzJ/UMcuvyALZCg7+ZM2sTGkxt5Y/cb7M3dS4hnCA+OfpBrL7gWL1fbvohEdF+F5VVs2J/Fur0ZbDqYRXmVCX9PV6YNCmfGoHAu7hfscIXDbJkEfTdVbarm62Nf8+aeNzlScIRIn0geG/8YV55/pdxpSXSKnGLLOe57M/jpSA5VNZpQX3euHRXFjMHhjInpgauz/Zf87Y7k9MpupqKmgi8Pf8lbe9/iVPEp+gb05fYhtzM9ejouTvL/tuhYp/LL+MZypsz2lNOYNET18GTm4AimDzKf4+7USTfrsCdGn14pQd9NlFSV8MmBT3h337vklOUwNHgotw+5nUuiLsFJSS9KdAytNckZRWxIzuKbpAx2p5rPce8f5sv0weZhmQER9nWOe1eQoBfNqr2K9YP9H1BYWci4iHEsGLKAC8MvlD820SGKK6r58VAOmw5kselANhmF5QAMiwpgxqBwpg8Ko0+I1JVpD6ODXr7r26izr2K9NOpSbh9yO0NChhjdNNHNaa05kl3MxuRsNh7IYlvKaapqND7uLkzoF8zk/qFc0j+EMCn3azck6G1MY1ex3jb4NvoG9jW6aaIbK62s5ucjuWw6YA731LwywDwkMz8uhkn9QxkdHSgHU+2UBL2NkKtYRUdLySlh44EsNh7IZsvRXCqrTXi6OhPXN5i7Jp3PpP6h9AqQm6w7Agl6g9W9itXLxUuuYhXnrLyqhq3HTrPRMtZ+LMd8PK1PiDdzx/ZmcmwIY2J64O4i57c7GjkYa4DGrmK9acBN3BgrV7GKtknNK2XjgWw2JWfx05FcyqpqcHdxYvz5QUzuH8qk/iH0DpJiYUaTg7EOpKqmim+Pf8uKfSvkKlZxTiqrTWw/fto81p6cxSFLFcioHp5cOzqSyf1DGdcnCE836bWLM6RH3wVyynJYfXA1Hx/4mOyybM7zPY95g+fJVayiVTIKyq2nPv54OIfiimpcnRVjY4KY1D+EybGh9An2ltNtbZjRPXoJ+k60N2cvH+z/gHUp66gyVRHXK46bYm8irlecXOQkmlRdY2LnyXw2JpsPpO5PLwQgwt+DSf1Dmdw/hLi+wXi7yxfy7kKC3s7UDs+sTF7J7uzdeLl4cVXfq7g+9npi/GOMbp6wURXVNfzvYA7xiWlsOpBFYXk1zk6K0b0DmRwbyuT+oVwQ5iO99m7K6KCXLkEHOXt4prdfbxaNWcSV51+Jj5tcVSgaqq4x8dORXOIT01iXlEFReTWBXuYKkJfGhhLXNxh/TynvK9pPgr6dknKSWLl/Zb3hmSdjn5ThGdEok0mz/Xge8YlpfLUnndySSnzdXZg2KJw5wyKI6xssFy2JDidBfw5qh2c+SP6AxOxEvFy8uPaCa2V4RjRKa83u1ALiE9NYuzudjMJyPFyduGxAGHOG9eSSC0KkdrvoVDJG3waNDc/cEHuDDM+IRh3IKCI+MY343Wkczy3F1VlxyQWhzBkWwWUDwuRgqgNpaYxeKfUWMBvI0loPtszrAXwERAMpwG+01nnKfKDmReByoBSYp7Xe0ez+Jehb1tjwjJw9IxqTklNiDfeDmcU4OykuOj+IOcN6Mn1gOP5eMubuiFoR9BOBYuC9OkH/LHBaa/2MUmoREKi1flgpdTlwL+agHwu8qLUe2+z+Jegb19jwjJw9IxqTll/Gf3anE787zVq/fUx0D+YMi2DmkAiCfdwNbqEwWmvOulFKRQNr6wT9AWCS1jpdKRUBbNJa91dKvW6Z/vDs5Zratnx3PIucPSNaI6e4gq/3pLMmMY1tKXkADI3056+zBnD5kAh6SrEwUZ+LUmp7nefLtNbLWlgnrE54ZwBhlulewMk6y6Va5knQt0TOnhEtKSit4pukDOJ3p7H5cA4mbS7z++C0C5g9tCfRwVJTRjSpWms9+lxX1lprpVTrh1/O4tBBL2fPiJaUVFTz3f5M4hPT+e/BLKpqNL2DvLh7Ul/mDOtJ/3Bfo5so7FemUiqiztBNlmX+KaBu/fJIy7wmOWTQy/CMaE55VQ2bDmQTvzuN7/dnUl5lIsLfg3kXRTNnWE+G9PKXK1RFV1gD3Ao8Y3n8ss78e5RSqzAfjC1obnweHOhgbFFlEQdOH+Dzw5/z9bGv5ewZUU9VjYnNh3OIT0xnfVIGRRXVBHm7cfmQCOYM68no3oE4OUm4i3PTirNuPgQmAcFAJvA48AXwMXAecBzz6ZWnLadXvgLMwHx65W+11tsb2eyZ7dtT0Ju0ifSSdI4VHONYwTFSClI4VmiezinLAZCzZwQAeSWV7DyZR8Jx80/iyQLKqmrw9XBh5uBw5gzryfg+QbjIVaqiAxhd66ZbBn1pVSnHCi1BXhvqhSkcLzxORU2FdTk/Nz/6+Pch2j+aGP8Yov2iGRM+RoZnHIzJpDmaU2wN9YTjeRzJNn+OnZ0Ug3r6MfK8QOL6BjPxgmC5A5PocBL0TTBpE5klmdYeed3eeVZplnU5J+VEpE+kNchj/GPM0/7RBLoHyliqAyqpqCYxNZ8dllDfcSKfgrIqAAK9XBnVO5CRvQMZdV4gQyMD5CYdotM5fNCXVZdxvPD4mTC39M5TClMoqy6zLufj6nMmxOsEepRvlNy8w4FprUnNK2PHiTO99f3phZgsH+sLwnzMwX5eIKN6BxIjN+gQBnCIoNdak1WaRUrhmSCvHXJJLzlzsFih6OnT0zzU4hdjDfMY/xiCPILkD1RQUV1DUlqhtbeecDyPrCLzcJ2XmzMjzgtg1HnmHvuIqEApOSBsgkME/VVfXMWRgiPW554ung2HWvyi6e3XGw8XjzZvX9iv7KIKdpzIswb77lMFVFabAPN9UkdZeuojewfSP8xXDp4Km2R00HfJefTXxV4HYA51vxhCvUKldy4aqDFpDmYWmcfVj+eRcCKP47mlALg5OzG4lx+3ju9tHYoJ9ZNOgRCtYfgYvXBMpZXVHM8tJSWnhOSMInacyGPniXyKK6oBCPZxZ1TvAEb1NvfYB/X0l5rtottyiB69cEwlFZYwzy0hJbeE4zmlHMst4XhuCZmFZ06DdVIQG+7Hr0b0sgZ7ZKCnfOsTooNI0It2Ka6o5nhuCSk5lkDPKbGGe+1B0lrBPu7EBHsxoV8I0UFeRAd7Ex3kTUywt9yEQ4hOJH9dokXFFdWk5Fh65bmlHMsx98pTckvJPivMQ3zdiQny5pILQqxB3tsS6j4S5kIYQv7yBABF5VX1QvxYTqklzEvIKa6st2yorzvRwd5M7n8mzGsDXXrmQtge+at0IFprsosrSE4vYn96IQcyi6wHRHNL6od5mJ870UHeTIkNs4S5uVfeO8gLLzf52AjRnchfrJ2qqK7hcFYx+9OLSE4vJDnDHO51Az3Mz52YYG+mDqwf5uf1kDAXwp7IX3M3p7Umq6iC/emF5lDPKCQ5vYgj2cVUW+oAuLs40T/clykDQhkQ4UdsuB+x4b4EekvpCCEcgQR9N1JeZe6l70s3h3lyRiH70wvJK62yLtMrwJPYcF+mDgwjNsKX2HA/YoK9cZZa6kLUV1kCJdlQkgulOZbpHMt0Tv3p0tOw6AQ4d8/I7J6ttnNaazIKy0lOLzKHeoZ5+OVoTgk1ll66h6sT/cP9mD4o3NJL9yU2wg9/T6ntIhyQ1ubgLs0xB3dJdp3AzobS3IYhXqdoYj0unuAdbP7xCoaQWPO0qUqCXpyb8qoaDmYW1Ql1c7Dn1+mlRwZ6Ehvux8zB4cRaQr13kPTShZ2rKofijOaDu+50k8HtAd4h4BVkfgyJPTNdG+beIeAdZJ528wY7u1hPgr4L1J7tcvJ0Gal5pZzILSU509xLP5ZTYi2p6+XmTP9wXy4fEsEASw+9f7gvfh7SSxd2RGuoKITCtDM/RelQeAoK06HIMq80t/H16wV3MIQOODPtHWIJ7jo9cjsM7raSWjcdQGvN6ZJKTuaZgzw1r4yTpy2PeaWcyiujwlJxsdZ5PbyIDfdlQIQfAyxj6ef18JL7koruzVRj7mVbw7tumNdOp0NVIzniFQR+PcG3J/hFgF8v8A0H79BuH9xG17qRoG8FrTUFZVUNAjy1TrCXVtbUWyfQy5XIQC+ieniaHwPNj5GWR7mrkeh2qisaBnjdnnhhmnmoxVRdfz0nF/AJN4e4NcAjLM97mqd9I8DVfquRStDbiMLyKlItQyu1PfPaoZZTeWUUVdT/8Pp6uBBlCe6oHmcCPKqHJ70CPPGV4RbRnVRXmnvc+SehIBUKTrZuKMXV+0yA+/Y8E961Ae7Xyzyc4uTY9wkwOugdYoxea01+aRWZReWcqtMrr9szr72naC0vN2eiLME9rk+QNchrg13ObhHdSnlB/RAvOFn/eVEGcFanr+5QSq/RdcK7Tpi7+3W7YRRH1K179FpriiqqySosJ7Owgsw6j1lFdacrrHclquXh6nQmuAPr98gjA70I9HKVMrmiezDVQHGmJbhrf1LrB3lFYf11nN3Mve2AKPCPAv/IM48B55lD3NXTmN/HDhndo7fZoC+pqLYGtzm06wR3YQWZRebHsqqaBuv6ursQ6udOmJ8Hob6WR8t0L0uwB/u4SZCL7qGy1DyMkn+iTo889UywF6aZz/GuyyPAHNwBjYS4f6T5AKeDD6d0JaODvsuHbsoqa+r1tjMLy8kuqtMbtwR48Vlj4gCers6E+3sQ4uvO0MgAwqwhbn6sDXapoCi6lKkGaiotP1Xmx+qKM9N15zeYbmS9kuz6QyulOfX3p5zMwykBURA1pmGI+0eCu68x74WwSV3So79/1U6S0grJLCynsLxhgLu5OBHm506Yr8dZwW2eF2qZ9nF3kV64OHdaQ2UxFGWahzrq/WSZx7GtIdxYMFc0/ro2tbzvtnD1btgTr9s79+3Zba/QdFQO0aN3c3Hi/BAfLjo/yBLalhD38yDM1wM/Twlw0Q411eZeb1GGObCLM86Et3WeJdCrShuu7+QCPmHmA4sububxa2c38/nazoHg7HpmXmPTLu4tL2N9bMWyrp5ygFN0KJsdoxcOrrned1GdXnhxhvny97PPGAHw8DcHeN0f39rpUPO53T5h4Bko49WiUzlEj15YaG2+mKSm0vxoqjF/7TfVgK4567Gx+Sbzeq1atjXzO3jI4VxVl53V+7Y8Ntf79gkzD2P0Gmm+erJucPuEmh/t+AIcIdrCtoNe63aE2FkBWfvcVHvQq7UHyZpYprqF12uqLGO6Z70uGucRcCakIy88q9dtCW7fcPNy0vsWok26Jug/uB5yD7c+jHWNpedqcI/TyaWRcVS3xsdfXf0bLuvSxLJOruZtOzmDcjYHl3Ku87yZ+Q2WOYdtWOc7ATYwFuzsJr1vITpR1wR9jxjzH3JTAeTkUuc1p7aFVb11W7GNpg6CubjXn+/kKj1HIUSXUErNAF4EnIE3tNbPdOj25WCsEEJ0ruYOxiqlnIGDwFQgFdgG3KC13tdR+5cuqxBCGGsMcFhrfVRrXQmsAq7syB1I0AshhLF6ASfrPE+1zOswtn3WjRBC2AcXpdT2Os+Xaa2XddnOu2pHQgjhwKq11qObeO0UEFXneaRlXoeRoRshhDDWNqCfUipGKeUGXA+s6cgdSI9eCCEMpLWuVkrdA3yD+fTKt7TWSR25Dzm9UgghOpnRtW5k6EYIIexcm3r0SikTUHaO+3IBGhajd1zyfpwh70V98n6cYS/vhafW2rCOdZuCvl07Ump7M0edHY68H2fIe1GfvB9nyHvRMWToRggh7JwEvRBC2LmuDPouuwqsm5D34wx5L+qT9+MMeS86QJeN0QshhDCGDN0IIYSdk6AXQgg71+lBr5SaoZQ6oJQ6rJRa1Nn7s2VKqSil1Eal1D6lVJJSaqHRbbIFSilnpdROpdRao9tiJKVUgFJqtVIqWSm1Xyk13ug2GUkp9QfL38lepdSHSim53+Q56tSgt9w55VVgJjAQuEEpNbAz92njqoE/aq0HAuOA3zv4+1FrIbDf6EbYgBeBdVrrWGAYDvyeKKV6AfcBo7XWgzHXgLne2FZ1X53do+/0O6d0J1rrdK31Dst0EeY/5A69wUB3o5SKBGYBbxjdFiMppfyBicCbAFrrSq11vqGNMp4L4KmUcgG8gDSD29NtdXbQd/qdU7orpVQ0MAL4xeCmGG0J8BBgMrgdRosBsoG3LcNYbyilDCuCZTSt9SlgMXACSAcKtNbrjW1V9yUHYw2glPIBPgXu11oXGt0eoyilZgNZWusEo9tiA1yAkcD/aa1HACWAwx7TUkoFYv72HwP0BLyVUnONbVX31dlB3+l3TululFKumEN+pdb6M6PbY7A44AqlVArmYb1LlVLvG9skw6QCqVrr2m94qzEHv6O6DDimtc7WWlcBnwEXGdymbquzg77T75zSnSilFOYx2P1a6+eNbo/RtNaPaK0jtdbRmD8bG7TWDtlr01pnACeVUv0ts6YA+wxsktFOAOOUUl6Wv5spOPDB6fbq1DtMdcWdU7qZOOBmYI9Sapdl3p+11l8Z1yRhQ+4FVlo6RUeB3xrcHsNorX9RSq0GdmA+W20nUg7hnEkJBCGEsHNyMFYIIeycBL0QQtg5CXohhLBzEvRCCGHnJOiFEMLOSdALIYSdk6AXQgg7J0EvujWlVLSldvtyS+3y9UopT6PbJYQtkaAX9qAf8KrWehCQD/za2OYIYVsk6IU9OKa13mWZTgCijWuKELZHgl7Yg4o60zV0cg0nIbobCXohhLBzEvRCCGHnpHqlEELYOenRCyGEnZOgF0IIOydBL4QQdk6CXggh7JwEvRBC2DkJeiGEsHMS9EIIYef+HyeZUbswVL70AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Also, print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 100) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(100,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(4, 100) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(100,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(100, 50) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(50,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(100, 50) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(50,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(50, 2) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(2,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(50, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " observations (InputLayer)      [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 100)          500         ['observations[0][0]']           \n",
      "                                                                                                  \n",
      " fc_value_1 (Dense)             (None, 100)          500         ['observations[0][0]']           \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 50)           5050        ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      " fc_value_2 (Dense)             (None, 50)           5050        ['fc_value_1[0][0]']             \n",
      "                                                                                                  \n",
      " fc_out (Dense)                 (None, 2)            102         ['fc_2[0][0]']                   \n",
      "                                                                                                  \n",
      " value_out (Dense)              (None, 1)            51          ['fc_value_2[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,253\n",
      "Trainable params: 11,253\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "\n",
    "Next we'll use the [RLlib rollout CLI](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies), to evaluate the trained policy.\n",
    "\n",
    "This visualizes the `CartPole` agent operating within the simulation: moving the cart left or right to avoid having the pole fall over.\n",
    "\n",
    "We'll use the last saved checkpoint, `checkpoint_10` (or whatever you set for `N_ITER` above) for the rollout, evaluated through `2000` steps.\n",
    "\n",
    "> **Notes:** \n",
    ">\n",
    "> 1. If you changed `checkpoint_root` above to be different than `tmp/ppo/cart`, then change it here, too. Note that bugs in variable substitution in Jupyter notebooks, we can't use variables in the next cell, unfortunately.\n",
    "> 2. If you changed the model parameters, specifically the `fcnet_hiddens` array in the `config` object above, make the same change here.\n",
    "\n",
    "You may need to make one more modification, depending on how you are running this tutorial:\n",
    "\n",
    "1. Running on your laptop? - Remove the line `--no-render`. \n",
    "2. Running on the Anyscale Service? The popup windows that would normally be created by the rollout can't be viewed in this case. Hence, the `--no-render` flag suppresses them. The code cell afterwords provides a sample video. You can try adding `--video-dir tmp/ppo/cart`, which will generate MP4 videos, then download them to view them. Or copy the `Video` cell below and use it to view the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib evaluate tmp/ppo/cart/checkpoint_10/checkpoint-10 \\\n",
    "    --config \"{\\\"env\\\": \\\"CartPole-v1\\\", \\\"model\\\": {\\\"fcnet_hiddens\\\": [100, 50]}}\" \\\n",
    "    --run PPO \\\n",
    "    --no-render \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample episode. \n",
    "\n",
    "> **Note:** This video was created by running the previous `rllib rollout` command with the argument `--video-dir some_directory`. It creates one video per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_sample_video = \"../../images/rllib/Cart-Pole-Example-Video.mp4\"\n",
    "Video(cart_pole_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "Finally, launch [TensorBoard](https://ray.readthedocs.io/en/latest/rllib-training.html#getting-started) as discussed in [02 Introduction to RLlib](../02-Introduction-to-RLlib.ipynb). Select the Cart Pole runs and visualize the key metrics from training with RLlib.\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=$HOME/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples of working with Gym environments, go through the next lesson, [Bipedal Walker](02-Bipedal-Walker.ipynb), then any of the \"extra\" lessons:\n",
    "\n",
    "* [Extras: Application - Mountain Car](extras/Extra-Application-Mountain-Car.ipynb) -- Based on the `MountainCar-v0` environment from OpenAI Gym.\n",
    "* [Extras: Application - Taxi](extras/Extra-Application-Taxi.ipynb) -- Based on the `Taxi-v3` environment from OpenAI Gym.\n",
    "* [Extras: Application - Frozen Lake](extras/Extra-Application-Frozen-Lake.ipynb) -- Based on the `FrozenLake-v0` environment from OpenAI Gym.\n",
    "\n",
    "Use TensorBoard to visualize their training runs, metrics, etc., as well. (These notebooks won't mention this suggestion.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (\"Homework\")\n",
    "\n",
    "In addition to _Cart Pole_, _Bipedal Walker_, and _Mountain Car_, there are other so-called [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples you can try. Make a copy of this notebook and edit as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
