{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of the notebook\n",
    "\n",
    "- [Defining a MDP for recommendation system using gym API](#recsys) (15 min)\n",
    "- [Online RL - Bandits](#bandits) (15 min)\n",
    "- [Online RL - DQN](#dqn) (15 min)\n",
    "- [Break](#break) (5 min)\n",
    "- [Offline RL](#offline-rl) - (15 min)\n",
    "\n",
    "- slides [Link](https://github.com/anyscale/academy/blob/main/ray-rllib/acm_recsys_tutorial_2022/slides/rllib_acm_recsys_2022_slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a RecSys gym environement <a class=\"anchor\" id=\"recsys\"></a>\n",
    "\n",
    "RL is usually a fit when it comes to sequential decision making problems. For example in recommendation systems, the particular items that our AI recommends could impact the interest profile of the users that it interacts with and as result, can have consequences on the next time-step that it is making a decision. This is in contrast to the passive prediction problems where the task is simply to predict the future and that prediction does not change the outcome.\n",
    "\n",
    "To successfully train RL agents we usually need a good simulator that can approximate real-world behavior about what is going to happen if your agent takes certain actions. It is always recomended to start with an environement that can be used to emulate real-world behavior. \n",
    "\n",
    "In this section, we will learn how to create and evaluate an exemplar RecSys environment using gym API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecSim\n",
    "\n",
    "In this notebook, we will use <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  It is a configurable environment, where ideally you would plug in your own users, products, and embedding features.\n",
    "\n",
    "**Some further readings**\n",
    "\n",
    "- <a href=\"https://github.com/google-research/recsim\">RecSim github</a>\n",
    "- <a href=\"https://arxiv.org/pdf/1909.04847.pdf\">RecSim paper</a>\n",
    "\n",
    "The following image depicts all the components of the recsim packages:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"70%\" />\n",
    "\n",
    "The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (10) of videos are watched. \n",
    "\n",
    "### Document Model\n",
    "\n",
    "<img src=\"./images/document_model.png\" width=\"70%\" />\n",
    "\n",
    "Documents represent the candidate pool of items that need to be recommended with features sampled in the range [0, 1].  In this tutorial, we use <b>1 single feature \"sweetness\"</b> drawn from a uniform distribution between [0.8, 1.0] to represent \"chocolaty\" items and [0, 0.2] for the \"kaley\" options. \n",
    "- The documents can be different at each step (produced by some \"candidate generation\" process), or fixed throughout the simulation.\n",
    "- The recommendation algorithm observes the D candidate documents.  It then makes a selection (possibly ordered) of k documents and presents them in a \"slate\" to the user. We will focus on **slate size of 1** in this tutorial. \n",
    "\n",
    "### User Model\n",
    "\n",
    "<img src=\"./images/user_model.png\" width=\"70%\" />\n",
    "\n",
    "In RecSim users are representation by a set of features some of which could be latent hidden variables not observed by our recommendation system during live interaction. In this tutorial we assume that a sampled user from the world does not own any observable features like age, gender, etc. and our AI should infer the latent state of the user from its history of interactions in the current session. \n",
    "- The user examines a \"slate\" of recommended items and makes a choice of one item. After making their choice, the user emits an engagement score which indicates how much that user was engaged with that particular item they chose. The agent has to learn to estimate the latent states of the user that shape their choice model in the future. \n",
    "\n",
    "### User Choice\n",
    "\n",
    "<img src=\"./images/user_choice_model.png\" width=\"70%\" />\n",
    "\n",
    "This module controls how a particular user would respond to a set of recommendations and how its latent state would evolve as a function of this interaction. \n",
    "In the environment in this tutorial, engagement is assumed to be a function of two competing phenomenas:\n",
    "-   The love of the user for sweet items ($sweetness(item_t)$)\n",
    "-   The long term satisfaction which cares about healthier options. It is inversely correlated with the sweetness of items suggested so far ($satisfaction_{t-1}$)\n",
    "\n",
    "$$satisfaction_t := satisfaction_{t-1} * \\sigma(-sweetness(item_{t}))$$\n",
    "$$r(item_t) \\propto sweetness(item_t) * satisfaction_{t-1}$$\n",
    "\n",
    "i.e. If a user who loves chocolates have not had chocolate for a while, a chocolate item would be more engaging than a kaley item. On the other hand if we keep recommending chocolate to the same person, they may lose interest and not use our recommendations. \n",
    "\n",
    "\n",
    "Let's look at this enviornement properties closer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for running on anyscale\n",
    "# import os; os.environ[\"PATH\"] = \"/home/ray/anaconda3/bin:\" + os.environ[\"PATH\"]\n",
    "# !pip uninstall -y torch\n",
    "# !python -m pip install torch==1.12.1\n",
    "# !python -m pip install seaborn\n",
    "# import torch\n",
    "# print(f\"torch: {torch.__version__}\")\n",
    "\n",
    "# !pip uninstall -y -r matplotlib\n",
    "# !python3 -m pip install matplotlib==3.5.3\n",
    "# import matplotlib\n",
    "# print(f\"matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray import tune, air, data\n",
    "\n",
    "import recsim \n",
    "from rllib_recsim.rllib_recsim import ModifiedLongTermSatisfactionRecSimEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main parameters\n",
    "seed = 100\n",
    "num_candidates = 20\n",
    "reward_scale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first instantiate the environment\n",
    "\n",
    "config = {\n",
    "    # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"num_candidates\": num_candidates,  \n",
    "    # The number of recommendations that we will be making\n",
    "    \"slate_size\": 1, \n",
    "    # Set to False for re-using the same candidate documents each timestep.\n",
    "    \"resample_documents\": True,\n",
    "    # Use consistent seeds for the environment ...\n",
    "    \"seed\": seed,\n",
    "    # scale rewards with this factor\n",
    "    \"reward_scale\": reward_scale,\n",
    "}\n",
    "\n",
    "env = ModifiedLongTermSatisfactionRecSimEnv(config)\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed environement shows a hierarchy of wrappers around the main RecSim environment. Next we'll investigate the behavior of observation and action space. For more info on how the environement is actually implemented we refer you to the source code acompanied with this tutorial [link](https://github.com/anyscale/academy/blob/main/ray-rllib/acm_recsys_tutorial_2022/rllib_recsim/rllib_recsim.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's checkout the observation space and action space\n",
    "print(\"observation_space:\")\n",
    "print(\"-\"*20)\n",
    "print(env.observation_space)\n",
    "print(\"observation_space example:\")\n",
    "print(\"-\"*20)\n",
    "print(env.observation_space.sample())\n",
    "print(\"observation space keys:\")\n",
    "print(\"-\"*20)\n",
    "print(list(env.observation_space.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is a dictionary of four keys. \n",
    "- The `user` key is empty, because there is no observable user features. \n",
    "- The `doc` key contains a set of documents presented with numerical keys. Each doc item has a scalar score representing its sweetness. \n",
    "- The `response` key includes a single record of `click` and `engagement` from the immediate previous interaction. \n",
    "- The `time` shows a normalized timestep within the session for this user. -0.5 corresponds to the beginnig of the interaction and +0.5 corresponds to the end of the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's checkout the action space\n",
    "print(\"action_space:\")\n",
    "print(\"-\"*20)\n",
    "print(env.action_space)\n",
    "print(\"action_space example:\")\n",
    "print(\"-\"*20)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is simply an integer number between 0, 19 (`Discrete(20)`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now checkout environement's reward behavior. Let's see what happens if we always pick the sweetest item and plot the reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's checkout the reward space\n",
    "# TODO: code here\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('engagement per step for maximum greedy policy')\n",
    "plt.xlabel('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note a couple of things here already:\n",
    "\n",
    "1. The immediate engagement would be high initially when the sweetest item is suggested for the first time.\n",
    "2. As we keep recommending the sweetest items, the user satisfaction significantly tampers off and as a result engagement quickly drops.\n",
    "3. Episodes seem to last for 10 timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise (2 min):\n",
    "Instead of picking the item with the highest feature, pick the item with the lowest feature and see what happens?\n",
    "\n",
    "- What do your observations imply about this environment?\n",
    "- What policy maximizes engagement with the user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "rewards = []\n",
    "done = False\n",
    "while not done:\n",
    "    # TODO (exercise): code here\n",
    "    action = ...\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('engagement per step for minimum greedy policy')\n",
    "plt.xlabel('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some baseline policies\n",
    "Next we will run some simple baselines to get a feeling of the reward we can accumulate in these enviornements using simple policies.\n",
    "\n",
    "- Greedy minimum feature value (recommending the kaliest option)\n",
    "- Greedy maximum feature value (recommending the chocoletiest option)\n",
    "- random policy (recommending random items from the pool)\n",
    "- even argmin (recommmending alternations between argmax and argmin to keep the engagement high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This computes  the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def calc_baseline(baseline_type=\"random\",\n",
    "                  episodes=100):\n",
    "\n",
    "    env_config = {\n",
    "        # The number of possible documents/videos/candidates that we can recommend\n",
    "        # no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"num_candidates\": num_candidates,  \n",
    "        # The number of recommendations that we will be making\n",
    "        \"slate_size\": 1, \n",
    "        # Set to False for re-using the same candidate documents each timestep.\n",
    "        \"resample_documents\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": seed,\n",
    "        # scale rewards with this factor\n",
    "        \"reward_scale\": reward_scale,\n",
    "    }\n",
    "\n",
    "    env = ModifiedLongTermSatisfactionRecSimEnv(env_config)\n",
    "    # Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    epsiode_satisfaction = []\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "    episode_satisfactions = []\n",
    "    \n",
    "    # Enter while loop (to step through the episode).\n",
    "    time_step = 0\n",
    "    while num_episodes < episodes:\n",
    "        # Produce an action\n",
    "        # TODO: code here\n",
    "        random_action = ...\n",
    "        argmax_action = ...\n",
    "        argmin_action = ...\n",
    "\n",
    "        action_dict = {\n",
    "            'argmax': argmax_action, # greedy choc\n",
    "            'argmin': argmin_action, # greedy kale\n",
    "            'random': random_action,\n",
    "        }\n",
    "        # a baseline that performs argmax in even time steps and argmin in odd time steps\n",
    "        action_dict[\"even_argmin\"] = (\n",
    "            action_dict[\"argmin\"] if time_step % 2 == 0 else action_dict[\"argmax\"]\n",
    "        )\n",
    "        action = action_dict[baseline_type]\n",
    "        \n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Accumulate the rewards\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Append satisfaction to episode_satiscation\n",
    "        epsiode_satisfaction.append(\n",
    "            env.environment._user_model._user_state.satisfaction\n",
    "        )\n",
    "\n",
    "        time_step += 1\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if num_episodes % 99 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 9 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "                \n",
    "            # increment on end of episode\n",
    "            num_episodes += 1\n",
    "            time_step = 0\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            episode_satisfactions.append(np.mean(epsiode_satisfaction))\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_reward = np.mean(episode_rewards)\n",
    "    env_sd_reward = np.std(episode_rewards)\n",
    "\n",
    "    # Print out the satisfaction over the episodes\n",
    "    env_mean_satisfaction = np.mean(episode_satisfactions)\n",
    "    env_sd_satisfaction = np.std(episode_satisfactions)\n",
    "    \n",
    "    print(f\"\\nMean {baseline_type} baseline reward: {env_mean_reward:.2f}+/-{env_sd_reward:.2f}, satisfaction: {env_mean_satisfaction:.2f}+/-{env_sd_satisfaction:.2f}\")\n",
    "\n",
    "    return env_mean_reward, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "kaliest_baseline, _ = calc_baseline(baseline_type=\"argmin\", episodes=num_episodes)\n",
    "sweetest_baseline,  _ = calc_baseline(baseline_type=\"argmax\", episodes=num_episodes)\n",
    "random_baseline, _ = calc_baseline(baseline_type=\"random\", episodes=num_episodes)\n",
    "even_margin_baseline, _ = calc_baseline(baseline_type=\"even_argmin\", episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion about the baselines\n",
    "\n",
    "For every baseline we have printed out not only the engagement score of the entire user session but also the average of the satisfaction term over the entire session as well.\n",
    "- **Random policy beats greedy options**\n",
    "- **Alternation between argmax and argmin beats random**\n",
    "\n",
    "The question is whether we automatically learn an optimal policy in this recommendation enviornement?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions (2 min)\n",
    "\n",
    "- Any questions so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a contextual bandit on the environement <a class=\"anchor\" id=\"bandits\"></a>\n",
    "\n",
    "Bandit is a classical algorithm used in RecSys that is known to optimize single-step objectives. **They maximize immediate engagement not accumulated engagement over the user session.**\n",
    "\n",
    "Any RL algorithm can be turned into a contextual bandit algorithm if the discount factor of the Markov Decision Process is set to 0.0. This will result in maximizing the immediate reward and hence a bandit solution. \n",
    "\n",
    "In this section, we will use [DQN](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#dqn) to train an agent both with $\\gamma = 0$ and $\\gamma = 0.99$ to see the difference between a bandit solution and an RL solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the config below we set a few settings:\n",
    "- For the environement, we specify 20 candidates that are randomly re-sampled at each time-step.\n",
    "- We use the torch implementation of the algorithm in RLlib\n",
    "- For evaluation, we rollout 100 complete episodes at the end of each training iteration and compute the average of un-discounted reward over the episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tune to register the environment\n",
    "tune.register_env(\"modified-lts\", \n",
    "    lambda config: ModifiedLongTermSatisfactionRecSimEnv(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQNConfig, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment config\n",
    "env_config = {\n",
    "    \"num_candidates\": num_candidates,  \n",
    "    \"slate_size\": 1, \n",
    "    \"resample_documents\": True,\n",
    "    \"seed\": seed,\n",
    "    \"reward_scale\": reward_scale\n",
    "}\n",
    "\n",
    "bandit_config = DQNConfig()\n",
    "# setup the env\n",
    "bandit_config = bandit_config.environment(env=\"modified-lts\", env_config=env_config)\n",
    "# setup framework to be torch\n",
    "bandit_config = bandit_config.framework(\"torch\")\n",
    "# setup the gamma\n",
    "bandit_config = bandit_config.training(gamma=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(bandit_config.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RLlib we can run training loops in two ways:\n",
    "1. Ad-hoc for-loop via calling algo.train()\n",
    "2. Using `tune.Tuner()` with stopping condition (recommended)\n",
    "\n",
    "The code below shows the differences. Moving forward (and in all scripts) we use the later option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-hoc for-loop\n",
    "# TODO: Code here\n",
    "results_bandit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "print(results_bandit.keys())\n",
    "pprint(results_bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tune.Tuner(param_space=..., run_config=air.RunConfig)\n",
    "# air.RunConfig(local_dir=..., stop=...)\n",
    "# TODO: Code here\n",
    "results_bandit =  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results_bandit[0].metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune collects these results from every iteration and puts them in the output directory where the other logging artifcats are stored. \n",
    "\n",
    "To run this experiment longer until it's trained you can run the following command to launch the bandit experiment:\n",
    "\n",
    "\n",
    "```bash\n",
    "python tutorial_scripts/run_online_rl.py --seed 0 --gamma 0.0 --exp_name bandits --timesteps 10_000\n",
    "```\n",
    "\n",
    "This script take 5 minutes to run. It will create experiment artifacts under `./results_scripts/` which includes the checkpoints as well as tensorboard and tabular logs. You can later inspect this folder to monitor your experiments.\n",
    "\n",
    "The suggested way is to use tensorboard to monitor the metrics of your run and look for `episode_reward_mean`.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir \"./results_scripts\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained results and plot the metrics in notebook\n",
    "import pandas as pd\n",
    "\n",
    "# Load the results from the progress.csv in the result folder of your running script\n",
    "df = pd.read_csv(\"saved_runs/bandits/progress.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df, x=\"training_iteration\", y=\"episode_reward_mean\", label=\"bandits\")\n",
    "plt.axhline(sweetest_baseline, color=\"red\", linestyle='--', label=\"sweetest baseline\")\n",
    "plt.axhline(random_baseline, color=\"k\", linestyle='--', label=\"random baseline\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Bandits training performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Bandit Training results to Baseline\n",
    "- Bandit Mean Reward=~56 \n",
    "- Kaleist (Argmin) Baseline Mean Reward = ~10.87+/-0.26\n",
    "- Random Baseline Mean Reward = ~99.90+/-23.75\n",
    "- Sweetest (Argmax) Baseline Mean Reward = ~56.56+/-1.37\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    🤔  <b>Bandit mean reward is approx the same as the sweetest baseline!</b> \n",
    "</div>  \n",
    "\n",
    "Try the code block below to compare what the bandit recommends with what is the sweetest item... you will see that the bandit always recommends the sweetest item!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the algorithm and load from checkpoint\n",
    "with open(\"saved_runs/bandits/params.pkl\", 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "pprint(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_algo = DQN(params)\n",
    "bandit_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint from the result folder of your running script\n",
    "checkpoint = \"saved_runs/bandits/checkpoint_000666\"\n",
    "bandit_algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedLongTermSatisfactionRecSimEnv(env_config)\n",
    "obs = env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # use `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    # TODO: Code here\n",
    "    bandit_action = ...\n",
    "    argmax_action = int(max(obs['doc'], key=lambda x: obs['doc'][x]))\n",
    "\n",
    "    feature_value_of_bandit = obs[\"doc\"][str(bandit_action)]\n",
    "    feature_value_of_greedy = obs[\"doc\"][str(argmax_action)]\n",
    "\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(\"-\"*50)\n",
    "    print(\"observation_features: \", np.concatenate(list(obs[\"doc\"].values())))\n",
    "    print(f\"bandit's feature value={feature_value_of_bandit}; argmax feature={feature_value_of_greedy};\")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = env.step(bandit_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dummy Recsim environment, we did not have any user features.  This makes the contextual bandit without any user context, i.e. without any state.  A stateless bandit cannot remember things between timesteps, so it will sort of converge to the most greedy policy that recommends chocolotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the problem with RL <a class=\"anchor\" id=\"dqn\"></a>\n",
    "\n",
    "So far the bandit solution has just converged to the perforamnce of greedy argmax policy. **How can we improve over the random policy baseline** Now let's run the DQN algorithm with $\\gamma = 0.99$. We run this script for 1M environment timesteps till convergence. It will take ~ 1hour to run this training job.\n",
    "\n",
    "**Exercise (1 min)** How would you modify the previous config object (`bandit_config`) to train an RL agent to optimize long-term engagement (hint: use `.training()` API)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code here\n",
    "dqn_config = ...\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    DQN,\n",
    "    param_space=dqn_config.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        local_dir='./results_notebook/online_rl/dqn',\n",
    "        stop={\"training_iteration\": 1},  # this is enough for it to converge\n",
    "    )\n",
    ")\n",
    "dqn_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the same script as before, with `gamma=0.99` passed in as a parameter. We should also run the script longer (1M steps) as it will take longer for DQN to converge. \n",
    "```bash\n",
    "python tutorial_scripts/run_online_rl.py --seed 0 --gamma 0.99 --exp_name dqn --timesteps 1_000_000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Take a look at the results and compare them to bandits via tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the results\n",
    "bandit_df = pd.read_csv(\"saved_runs/bandits/progress.csv\")\n",
    "dqn_df = pd.read_csv(\"saved_runs/dqn/progress.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results and compare to baselines\n",
    "sns.lineplot(data=bandit_df, x=\"training_iteration\", y=\"episode_reward_mean\", label=\"Bandits\")\n",
    "sns.lineplot(data=dqn_df, x=\"training_iteration\", y=\"episode_reward_mean\", label=\"DQN\")\n",
    "plt.axhline(random_baseline, color=\"red\", linestyle='--', label=\"random baseline\")\n",
    "plt.axhline(sweetest_baseline, color=\"blue\", linestyle='--', label=\"sweetest baseline\")\n",
    "plt.legend()\n",
    "plt.title('RL vs. Bandits training performance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sweetest** straight line, is the mean reward achieved by the greedy policy, selecting always the item with most immediate reward value.\n",
    "- **Bandit** short term reward hovers around the \"sweetest baseline\".\n",
    "- **Random** straight line, items are randomly chosen to be recommended at each timestep. Since this baseline mixes the sweetest and kaliest options the engagement is kept higher than either of the greedy methods, obtaining larger rewards.\n",
    "- **DQN (RL)**, such as DQN that optimize for long-term engagement significantly improves upon random baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions and Break (5 min) <a class=\"anchor\" id=\"break\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline RL with RecSys <a class=\"anchor\" id=\"offline-rl\"></a>\n",
    "\n",
    "<img src=\"images/offline_rl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### If we don't have a live environment, how do we know, how well our trained policy will perform?\n",
    "\n",
    "One of the challenges in offline RL is the evaluation of the trained policy. In online RL (when a simulator\n",
    "is available), one can either use the data collected for training to compute episode total rewards. Remember\n",
    "that observations, actions, rewards, and done flags are all part of this training data. Alternatively,\n",
    "one could run a separate worker (with the same trained policy) and run it on a fresh evaluation-only environment.\n",
    "In this latter case, we would also have the chance to switch off any exploratory behavior (e.g. stochasticity used\n",
    "for better action entropy).\n",
    "\n",
    "In offline RL, no such data from a live environment is available to us. There are two common ways of addressing this dilemma:\n",
    "\n",
    "1) We deploy the learned policies into production, or maybe just a portion of our production system (similar to A/B testing), and see what happens.\n",
    "\n",
    "2) We use a method called \"off policy evaluation\" (OPE) to compute an estimate on how the new policy would perform if we were to deploy it into a real environment. There are different OPE methods available in RLlib off-the-shelf.\n",
    "\n",
    "3) The third option - which we will use here - is kind of cheating and only possible if you actually do have a simulator available (but you only want to use it for evaluation, not for training, because you want to see how cool offline RL is :) )\n",
    "\n",
    "In this tutorial, we will use the third option to show the effectiveness of offline RL in improving over existing policies running in production. We will also see how much benefit we can get by improving our dataset quality, starting from a totally random policy all the way to 20% expert demonstrations adn 80% random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the currently running policies in production to collect some \"historical data\" that we can use to train RL agents with. Offline RL can be used to improve upon the existing policies deployed in production. We have prepared some datasets in advance for the purpose of this tutorial. They were all generated using `<path to the script>`. In this script we can mix the percentage of the \"expert\" data vs. random data to investigate the effect of dataset quality on the final performance of our models.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an exemplar dataset we have prepared before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"s3://air-example-data/rllib/acm_recsys22_tutorial_data/\"\n",
    "train_data_path = prefix + \"sampled_data_train_random_transitions_small\"\n",
    "\n",
    "dset = data.read_json(train_data_path)\n",
    "df = dset.to_pandas()\n",
    "print('Colimns: ', df.columns)\n",
    "print('Number of rows: ', len(df))\n",
    "print('Value of the first row')\n",
    "print('-'*20)\n",
    "print(df.iloc[0])\n",
    "print('Value of the second row:')\n",
    "print('-'*20)\n",
    "print(df.iloc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset schema, we can see that RLlib always expects a `type` column that is `SampleBatch`. It will have the normal transition entities per each row (i.e. observation, next_observation, action, reward, done values). It will also contain an episode_id, a timestep indicator, and an action_prob that show the probablity of the action that we chosen at the time of data collection. For random policy the action prob will always be 1/20 (0.05). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understaning of the dataset example format, we can use RLlib to train an offline RL algorithm. RLlib provides several out of the box offline RL algorithms that you can use. Beside those offline-RL-specific algorithms, we can also use any off-policy algorithm (e.g. DQN) to do offline-RL. The only difference between online and offline version is that instead of using an enviornement sampler, we use a dataset sampler to get the data from. In the next section, we will use DQN, with the difference that we pass a dataset path to the input config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the differences:\n",
    "\n",
    "- Change the input config from a sample to offline dataset is configured by `.offline_data()` API:\n",
    "\n",
    "```python\n",
    "    .offline_data(\n",
    "        input_='dataset',\n",
    "        input_config={\n",
    "            'format': 'json',\n",
    "            'paths': train_data_path\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "- The environement is not passed to the enviornement anymore. Instead we need to pass in the expected action and observation space to construct the policies. We can create them manually based on our knowledge of our system. In this case we actually cheat and use the environement attributes to get the correct action and observatin spaces.\n",
    "\n",
    "```python \n",
    "    .environment(\n",
    "        action_space=action_space,\n",
    "        observation_space=observation_space,\n",
    "    )\n",
    "```\n",
    "\n",
    "- evaluation config: Since during the evaluation we still need to use the enviroenement simulations, we need to specify it explicitly here. RLlib by default will use the training settings during evaluation and to avoid that default behavior we need to explicitly specify the simulation environement configs. \n",
    "\n",
    "```python\n",
    "    .evaluation(\n",
    "        evaluation_config={\n",
    "            \"input\": \"sampler\",\n",
    "            \"explore\": False,\n",
    "            \"env\": \"modified-lts\",\n",
    "            \"env_config\": env_config,\n",
    "        },\n",
    "    )\n",
    "```\n",
    "\n",
    "- (Advanced) configuring the replay buffer to become a dummy buffer. By default RLlib uses a large replay buffer that is useful in online RL but doesn't add much value in the offline case. It is recommended to by-pass this behavior by configuring the replay buffer size to be the same as the dataset sampling size, so that the sampling flow does not get disrupted by the replay buffer behavior.\n",
    "\n",
    "```python\n",
    "    .training(\n",
    "        replay_buffer_config={\n",
    "            \"capacity\": 512,\n",
    "            \"learning_starts\": 0\n",
    "        }\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the script below takes about 1 hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedLongTermSatisfactionRecSimEnv(env_config)\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "dqn_config_offline = (\n",
    "    dqn_config\n",
    "    .offline_data(\n",
    "        input_='dataset',\n",
    "        input_config={\n",
    "            'format': 'json',\n",
    "            'paths': train_data_path,\n",
    "        }\n",
    "    )\n",
    "    .environment(\n",
    "        action_space=action_space,\n",
    "        observation_space=observation_space,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=10, \n",
    "        evaluation_duration=10, \n",
    "        evaluation_duration_unit=\"episodes\",\n",
    "        evaluation_parallel_to_training=True,\n",
    "        evaluation_config={\n",
    "            \"input\": \"sampler\",\n",
    "            \"explore\": False,\n",
    "            \"env\": \"modified-lts\",\n",
    "            \"env_config\": env_config,\n",
    "        },\n",
    "    )\n",
    "    .debugging(seed=seed, log_level=\"ERROR\")\n",
    "    .training(\n",
    "        gamma=1.0,\n",
    "        num_atoms=1,\n",
    "        double_q=True,\n",
    "        dueling=False,\n",
    "        model=dict(\n",
    "            fcnet_hiddens=[1024, 1024, 1024],\n",
    "            fcnet_activation='relu', \n",
    "        ),\n",
    "        train_batch_size=512,\n",
    "        lr=3e-4,\n",
    "        target_network_update_freq=512,\n",
    "        replay_buffer_config={\n",
    "            \"capacity\": 512,\n",
    "            \"learning_starts\": 0\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the long-running learning script run:\n",
    "\n",
    "```bash\n",
    "python tutorial_scripts/run_offline_rl.py --dataset_suffix sampled_data_train_random_transitions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    DQN,\n",
    "    param_space=dqn_config_offline.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        local_dir=\"./results_notebook/offline_rl/\",\n",
    "        stop={\"training_iteration\": 1},\n",
    "    )\n",
    ")\n",
    "dqn_offline_results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the offline training results. From the plot below we can see that by running offline RL on randomly collected transitions we can improve over the random policy. This is extremely useful in practical scenarios where our goal is to improve over existing production policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_offline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_offline_results[0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results and compare to baselines\n",
    "fname = \"saved_runs/dqn_offline/random_data/progress.csv\"\n",
    "offline_dqn_df = pd.read_csv(fname)\n",
    "print(offline_dqn_df.columns)\n",
    "offline_dqn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.lineplot(data=offline_dqn_df, x=\"training_iteration\", y=\"evaluation/episode_reward_mean\", label=\"Offline_DQN\")\n",
    "sns.lineplot(data=dqn_df, x=\"training_iteration\", y=\"episode_reward_mean\", label=\"Online_DQN\")\n",
    "sns.lineplot(data=bandit_df, x=\"training_iteration\", y=\"episode_reward_mean\", label=\"Bandits\")\n",
    "plt.axhline(random_baseline, color=\"red\", linestyle='--', label=\"random baseline\")\n",
    "plt.legend()\n",
    "plt.title('Offline RL vs. Baselines training performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=offline_dqn_df, x=\"training_iteration\", y=\"info/learner/default_policy/learner_stats/mean_q\", label=\"q_value\")\n",
    "plt.legend()\n",
    "plt.title('Average Q value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Bandits converges to a short-sighted solution\n",
    "    -  Optimizes imediate reward. \n",
    "- DQN takes long-term reward into account\n",
    "    -  Achieves a policy better than random.\n",
    "    -  Can become even better than our heuristic based baseline (even_argmin).\n",
    "- Offline RL is a viable option that works when we don't have simulators for training\n",
    "    -  Since it doesn't explore freely it won't perform as good as an online RL method. \n",
    "    -  Its performance is bounded to the quality of the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1fa47cc903de259dc4fe0eb866c1abbdf9745b83ed486d15bf7573a1fd3b3fc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
