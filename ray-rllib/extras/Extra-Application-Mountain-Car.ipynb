{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Extra Application Example - MountainCar-v0\n",
    "\n",
    "© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy with the `MountainCar-v0` environment, ([gym.openai.com/envs/MountainCar-v0/]. The idea is that a cart starts at an arbitrar point on a hill. Without any \"pushes\", it will rock back and forth between the two sides of the valley below, never rising above the starting point. However, there are three actions, accelerate to the left (by some unit), accelerate to the right, or apply no acceleration. Timing accelerations in the appropriate directions at the appropriate steps is the key to getting to the top of the hill.\n",
    "\n",
    "The primary idea demonstrated in this lesson is how to start from a previous checkpoint. A checkpoint is provided in the `mountain-car-checkpoint` directory, captured after 200 training episodes. Still, the with the provided checkpoint and addition training of 50 episodes, the cart is unable to reach the top.\n",
    "\n",
    "Hence, you should consider this lesson a big exercise to try when you aren't pressed for time (like in a class setting). Modifications you can try are discussed below.\n",
    "\n",
    "> **Note:** This rollout can only show the rollout visualization popup windows when running on a local laptop.\n",
    "\n",
    "Like _CartPole_, _MountainCar_ is one of OpenAI Gym's [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "* [\"Efficient memory-based learning for robot control\"](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf), [Andrew William Moore](https://www.cl.cam.ac.uk/~awm22/), University of Cambridge (1990)\n",
    "* [\"Solving Mountain Car with Q-Learning\"](https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2), [Tim Sullivan](https://twitter.com/ts_1829)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Ray and the PPO support, then start Ray…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, os, shutil, sys\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start up Ray as in the previous lessons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: Ray is not running. Run ../tools/start-ray.sh with no options in a terminal window to start Ray.\n",
      "INFO: (You can start a terminal in Jupyter. Click the + under the Edit menu.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 12:14:28,026\tINFO resource_spec.py:212 -- Starting Ray with 4.0 GiB memory available for workers and up to 2.02 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-13 12:14:28,345\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.149',\n",
       " 'raylet_ip_address': '192.168.1.149',\n",
       " 'redis_address': '192.168.1.149:43983',\n",
       " 'object_store_address': '/tmp/ray/session_2020-06-13_12-14-28_013436_4596/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-06-13_12-14-28_013436_4596/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-06-13_12-14-28_013436_4596'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard is useful for monitoring Ray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8265\n"
     ]
    }
   ],
   "source": [
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy with the `MountainCar-v0` environment.\n",
    "\n",
    "By default, training runs for `10` iterations. Increase the `N_ITER` setting if you want to train longer and see the resulting rewards improve.\n",
    "Also note that *checkpoints* get saved after each iteration into the `tmp/ppo/mountain-car` directory.\n",
    "\n",
    "For `MountainCar`, the environment has these parameters and behaviors (from this [source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py)):\n",
    "\n",
    "```\n",
    "Observation\n",
    "    Type: Box(2)\n",
    "    Num    Observation               Min            Max\n",
    "    0      Car Position              -1.2           0.6\n",
    "    1      Car Velocity              -0.07          0.07\n",
    "Actions:\n",
    "    Type: Discrete(3)\n",
    "    Num    Action\n",
    "    0      Accelerate to the Left\n",
    "    1      Don't accelerate\n",
    "    2      Accelerate to the Right\n",
    "    Note: This does not affect the amount of velocity affected by the\n",
    "    gravitational pull acting on the car.\n",
    "Reward:\n",
    "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "     on top of the mountain.\n",
    "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "Starting State:\n",
    "     The position of the car is assigned a uniform random value in\n",
    "     [-0.6 , -0.4].\n",
    "     The starting velocity of the car is always assigned to 0.\n",
    " Episode Termination:\n",
    "     The car position is more than 0.5\n",
    "     Episode length is greater than 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up previous stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = 'tmp/ppo/mountain-car'\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the default configuration for PPO applied to this environment. There are no configuration parameters that are passed to _MountainCar_ itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'sample_batch_size': -1,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action_reward': False,\n",
       "  'state_shape': None,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_action_dist': None,\n",
       "  'custom_options': {},\n",
       "  'custom_preprocessor': None},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {},\n",
       " 'env': None,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'use_pytorch': False,\n",
       " 'eager': False,\n",
       " 'eager_tracing': False,\n",
       " 'no_eager_on_workers': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'use_exec_api': False,\n",
       " 'sample_async': False,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None},\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': False,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell copies the default configuration and makes a few modifications, like a larger training batch size. Other changes you might consider are the following:\n",
    "\n",
    "* Tweak the `model` parameters for the neural net.\n",
    "* Try other `train_batch_size` values (default: `4000`).\n",
    "* SGD parameters: `num_sgd_iter` and `sgd_minibatch_size`.\n",
    "\n",
    "To speed up training:\n",
    "\n",
    "* Increase the `num_workers` to fully utilize your available machine or cluster, \n",
    "* Use GPUs if you have them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_ENV = \"MountainCar-v0\"\n",
    "N_ITER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 13:52:03,349\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-13 13:52:03,350\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"            # the default, at this time\n",
    "config[\"num_workers\"] = 4               # default = 2\n",
    "config[\"train_batch_size\"] = 10000      # default = 4000\n",
    "config[\"sgd_minibatch_size\"] = 256      # default = 128\n",
    "config[\"evaluation_num_episodes\"] = 50  # default = 10\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 13:52:03,555\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-13 13:52:03,562\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: mountain-car-checkpoint/checkpoint-200\n",
      "2020-06-13 13:52:03,568\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 200, '_timesteps_total': 1100000, '_time_total': 1285.8826241493225, '_episodes_total': 5500}\n"
     ]
    }
   ],
   "source": [
    "agent.restore('mountain-car-checkpoint/checkpoint-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_201/checkpoint-201\n",
      "  2: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_202/checkpoint-202\n",
      "  3: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_203/checkpoint-203\n",
      "  4: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_204/checkpoint-204\n",
      "  5: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_205/checkpoint-205\n",
      "  6: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_206/checkpoint-206\n",
      "  7: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_207/checkpoint-207\n",
      "  8: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_208/checkpoint-208\n",
      "  9: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_209/checkpoint-209\n",
      " 10: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_210/checkpoint-210\n",
      " 11: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_211/checkpoint-211\n",
      " 12: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_212/checkpoint-212\n",
      " 13: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_213/checkpoint-213\n",
      " 14: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_214/checkpoint-214\n",
      " 15: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_215/checkpoint-215\n",
      " 16: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_216/checkpoint-216\n",
      " 17: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_217/checkpoint-217\n",
      " 18: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_218/checkpoint-218\n",
      " 19: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_219/checkpoint-219\n",
      " 20: Min/Mean/Max reward: -200.0000/-200.0000/-200.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/mountain-car/checkpoint_220/checkpoint-220\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Training gives up on an episode after 200 steps. The reward is `-1*N` when the cart doesn't reach the top of the hill. The reward is zero if it does reach the top. Hence, there are no incremental rewards here; it's success or failure.\n",
    "\n",
    "Let's print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(2, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(2, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 3) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(3,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          768         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          768         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 3)            771         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 134,148\n",
      "Trainable params: 134,148\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "This visualizes the \"car\" agent operating within the simulation: rocking back and forth to gain momentum to overcome the mountain, using the last checkpoint. Edit the number in the checkpoint path if necessary! Also change the configuration to match the changes above.\n",
    "\n",
    "> **Note:** This rollout can only show the visualization popup windows when running on a local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-13 13:55:39,612\tWARNING worker.py:809 -- When connecting to an existing cluster, _internal_config must match the cluster's _internal_config.\n",
      "2020-06-13 13:55:39,634\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-06-13 13:55:39,651\tINFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-06-13 13:55:43,327\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-13 13:55:43,327\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-06-13 13:55:43,399\tINFO trainable.py:217 -- Getting current IP.\n",
      "2020-06-13 13:55:43,400\tINFO trainable.py:423 -- Restored on 192.168.1.149 from checkpoint: tmp/ppo/mountain-car/checkpoint_200/checkpoint-200\n",
      "2020-06-13 13:55:43,400\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 200, '_timesteps_total': 1100000, '_time_total': 1285.8826241493225, '_episodes_total': 5500}\n",
      "Episode #0: reward: -200.0\n",
      "Episode #1: reward: -200.0\n",
      "Episode #2: reward: -200.0\n",
      "Episode #3: reward: -200.0\n",
      "Episode #4: reward: -200.0\n",
      "Episode #5: reward: -200.0\n",
      "Episode #6: reward: -200.0\n",
      "Episode #7: reward: -200.0\n",
      "Episode #8: reward: -200.0\n",
      "Episode #9: reward: -200.0\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0613 14:05:02.436301 250228160 process.cc:274] Failed to wait for process 10635 with error system:10: No child processes\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0613 14:05:02.438416 250228160 process.cc:274] Failed to wait for process 10636 with error system:10: No child processes\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0613 14:05:02.438462 250228160 process.cc:274] Failed to wait for process 10633 with error system:10: No child processes\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0613 14:05:02.438496 250228160 process.cc:274] Failed to wait for process 10634 with error system:10: No child processes\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0613 14:05:02.438529 250228160 process.cc:274] Failed to wait for process 10809 with error system:10: No child processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-13 14:05:02,754\tERROR worker.py:1092 -- listen_error_messages_raylet: Connection closed by server.\n",
      "2020-06-13 14:05:02,758\tERROR import_thread.py:93 -- ImportThread: Connection closed by server.\n",
      "2020-06-13 14:05:02,759\tERROR worker.py:992 -- print_logs: Connection closed by server.\n"
     ]
    }
   ],
   "source": [
    "!RAY_ADDRESS=auto rllib rollout \\\n",
    "    tmp/ppo/mountain-car/checkpoint_200/checkpoint-200 \\\n",
    "    --config '{\"env\": \"MountainCar-v0\", \"num_workers\":4, \"train_batch_size\":10000, \"sgd_minibatch_size\":256, \"evaluation_num_episodes\":50}' --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tI9vJ1vU6Mj1"
   },
   "source": [
    "The rollout uses the second saved checkpoint, evaluated through `2000` steps.\n",
    "Modify the path to view other checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (\"Homework\")\n",
    "\n",
    "In addition to _Mountain Car_ and _Cart Pole_, there are other so-called [\"classic control\"](https://gym.openai.com/envs/#classic_control) examples you can try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
