{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to RLlib\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "In the [previous lesson](01-Introduction-to-Reinforcement-Learning.ipynb), we learned the basic concepts of reinforcement learning, with a \"taste\" of [RLlib](https://rllib.io) and [OpenAI Gym](https://gym.openai.com). This lesson takes a step back to provide more information about RLlib and the features it provides. The subsequent lessons will continue our exploration of RL concepts and RLlib tools.\n",
    "\n",
    "For more detailed information about RLlib and its open source community, see the following:\n",
    "\n",
    "* [rllib.io](http://rllib.io) (the documentation)\n",
    "* [GitHub repo](https://github.com/ray-project/ray/tree/master/rllib#rllib-scalable-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLlib is structured conceptually like this:\n",
    "\n",
    "![RLlib Stack](../images/rllib/RLlib-Stack-smaller.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _(1) Application Support_ boxes are components used for particular RL algorithms. The _(2) Abstractions for RL_ provide building blocks used by the many algorithms that are implemented in RLlib (listed below). They also provide hooks for implementing your own algorithms. RLlib leverages Ray for efficient, cluster-wide, _(3) Distributed Execution_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib in 60 Seconds (plus some...)\n",
    "\n",
    "Here is a fast introduction to using RLlib from a command line, adapted from the [documentation](https://docs.ray.io/en/latest/rllib.html#rllib-in-60-seconds).\n",
    "\n",
    "First you would install [PyTorch](http://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/), whichever you prefer.  Then install RLlib. **All of these items are already installed in this tutorial environment.**\n",
    "\n",
    "```shell\n",
    "pip install ray[rllib]  # or consider using: ray[debug]\n",
    "```\n",
    "\n",
    "Then **train** `CartPole` using _PPO_ with the `rllib` CLI. We connect to the running Ray cluster, we'll stop at 20 iterations, and we'll save checkpoints every 10 iterations and at the end:\n",
    "\n",
    "```shell\n",
    "rllib train --run PPO --env CartPole-v1 --stop='{\"training_iteration\": 20}' --ray-address auto --checkpoint-freq 10 --checkpoint-at-end\n",
    "```\n",
    "\n",
    "The `rllib` CLI has a `--help` flag that prints details about the supported options:\n",
    "\n",
    "```shell\n",
    "rllib --help          # general help\n",
    "rllib train --help    # specific help on the training options\n",
    "rllib rollout --help  # specific help on the rollout options\n",
    "```\n",
    "\n",
    "_Rollout_ means running an episode with the trained model, which you specify by passing a checkpoint directory to the command. During rollout a continuous loop of taking an action and observing the new state and reward continues until some final state or number of iterations is reached.\n",
    "\n",
    "You can execute the same training logic using the following Python code, which leverages [Ray Tune](http://tune.io), specifically the [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run) method:\n",
    "\n",
    "```python\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "tune.run(PPOTrainer, \n",
    "    config={\"env\": \"CartPole-v1\"},\n",
    "    stop={\"training_iteration\": 20},\n",
    "    checkpoint_at_end=True,\n",
    "    verbose=2            # 2 for INFO; change to 1 or 0 to reduce the output.\n",
    "    )  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the `rllib` CLI just shown. The following cell will take between one and two minutes to run.\n",
    "\n",
    "You could also run this command in a separate terminal window.\n",
    "\n",
    "> **Tip:** The output will be long. When this happens for a cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib train --run PPO --env CartPole-v1 --stop='{\"training_iteration\": 20}' --checkpoint-freq 10 --checkpoint-at-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view the training results using [TensorBoard](https://www.tensorflow.org/tensorboard). The results during training were written to a directory under `$HOME/ray_results`\n",
    "\n",
    "If you are viewing this lesson on the Anyscale hosted platform, use the provided link to open TensorBoard.\n",
    "\n",
    "If you are viewing this lesson on a laptop, open a terminal and run the following command, then open the URL shown in the output. (You can open a terminal using the `+` in the upper left-hand corner of Jupyter Lab.)\n",
    "\n",
    "```shell\n",
    "tensorboard --logdir=~/ray_results\n",
    "```\n",
    "\n",
    "Here is a [TensorBoard screenshot](../images/rllib/TensorBoard-CartPole-PPO.png).\n",
    "\n",
    "The directory `$HOME/ray_results` will contain the results for all the RL training we'll do in this tutorial. You may wish to clean out old results periodically. For the run we just did, look for the results in `$HOME/ray_results/default/PPO-CartPole-V1_0_YYYY-MM-DD_HH-MM-SS*`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollout\n",
    "\n",
    "> **WARNING:** The `rllib rollout` command discussed next won't work in a cloud environment, because it attempts to pop up a window. If you are taking a live class, the instructor will demonstrate what you would see. You can also watch the next video of a single _episode_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_sample_video='../images/rllib/Cart-Pole-Example-Video.mp4'\n",
    "Video(cart_pole_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `rllib rollout <checkpoint> --run PPO` to run episodes from a `<checkpoint>`, which in this case will be a directory with a name like this:\n",
    "\n",
    "```\n",
    "$HOME/ray_results/default/PPO_CartPole-v1_0_YYYY-MM-DD_HH-MM-SS.../checkpoint_20/checkpoint-20/\n",
    "```\n",
    "\n",
    "If you are on a MacOS or Linux machine, the shell script in the next cell will find the correct directory for you and run the `rllib rollout <checkpoint> --run PPO` command. It will also print the full command with the correct checkpoint directory. If it finds more than once checkpoint directory, for example from a previous run, it uses the latest one.\n",
    "\n",
    "If you are working in a cloud environment, like the Anyscale platform, add the flag `--no-render` to the command. Otherwise, an error will occur because RLlib won't be able to open the window for _CartPole_.\n",
    "\n",
    "If you are on Windows, Change the following cell to this command:\n",
    "\n",
    "```\n",
    "rllib rollout %HOMEDRIVE%%HOMEPATH%\\ray_results\\default\\PPO_CartPole-<fix>\\checkpoint_20\\checkpoint-20 --run PPO --episodes 5\n",
    "```\n",
    "\n",
    "Where you'll need to determine the correct string to use for `<fix>` by looking in the `...\\ray_results\\default\\` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rollout.sh --episodes 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [this RLlib page on training policies](https://docs.ray.io/en/master/rllib-training.html) for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on RLlib Concepts: Policies, Environments, Samples, and Trainers\n",
    "\n",
    "### Policies and Environments\n",
    "\n",
    "[Policies](https://docs.ray.io/en/latest/rllib-concepts.html#policies) in RLlib are Python classes that define how an agent acts in an environment.\n",
    "\n",
    "[Rollout workers](https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation) query the policy to determine agent actions. \n",
    "\n",
    "In a [gym](https://docs.ray.io/en/latest/rllib-env.html#openai-gym) environment, there is a single agent and policy. In [vector environments](https://docs.ray.io/en/latest/rllib-env.html#vectorized), policy inference is for multiple agents at once, and in [multi-agent and hierachical environments](https://docs.ray.io/en/latest/rllib-env.html#multi-agent-and-hierarchical), there may be multiple policies, each controlling one or more agents.\n",
    "\n",
    "![Environments and Policies in RLlib](../images/rllib/multi-flat.svg)\n",
    "\n",
    "The RLlib documentation on [environments](https://docs.ray.io/en/latest/rllib-env.html#rllib-environments) provides more details.\n",
    "\n",
    "Policies can be implemented using any framework ([RLlib policy.py code](https://github.com/ray-project/ray/blob/master/rllib/policy/policy.py)). However, for TensorFlow and PyTorch, RLlib has [build_tf_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-tensorflow) and [build_torch_policy](https://docs.ray.io/en/latest/rllib-concepts.html#building-policies-in-pytorch) helper functions, respectively, that let you define a trainable policy with a functional-style API. This example is taken from the [documentation](https://docs.ray.io/en/latest/rllib.html#policies):\n",
    "\n",
    "```python\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits, model)\n",
    "    return -tf.reduce_mean(\n",
    "        action_dist.logp(train_batch[\"actions\"]) * train_batch[\"rewards\"])\n",
    "\n",
    "# <class 'ray.rllib.policy.tf_policy_template.MyTFPolicy'>\n",
    "MyTFPolicy = build_tf_policy(\n",
    "    name=\"MyTFPolicy\",\n",
    "    loss_fn=policy_gradient_loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batches\n",
    "\n",
    "From single processes to large clusters, all data interchange in RLlib uses [sample batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py). Sample batches encode one or more fragments of a trajectory. Typically, RLlib collects batches of size `rollout_fragment_length` from rollout workers, and concatenates one or more of these batches into a batch of size `train_batch_size` that is the input to SGD (stochastic gradient descent).\n",
    "\n",
    "A typical sample batch looks something like the following when summarized. Since all values are kept in arrays, this allows for efficient encoding and transmission across the network:\n",
    "\n",
    "```python\n",
    " { 'action_logp': np.ndarray((200,), dtype=float32, min=-0.701, max=-0.685, mean=-0.694),\n",
    "   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
    "   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.055),\n",
    "   'infos': np.ndarray((200,), dtype=object, head={}),\n",
    "   'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.018),\n",
    "   'obs': np.ndarray((200, 4), dtype=float32, min=-2.46, max=2.259, mean=0.016),\n",
    "   'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
    "   't': np.ndarray((200,), dtype=int64, min=0.0, max=34.0, mean=9.14)}\n",
    "```\n",
    "\n",
    "In [multi-agent mode](https://docs.ray.io/en/latest/rllib-concepts.html#policies-in-multi-agent), sample batches are collected separately for each individual policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainers\n",
    "\n",
    "At a high level, RLlib provides [trainer classes](https://docs.ray.io/en/latest/rllib-concepts.html#trainers) ([Trainer source code](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py)) that hold a policy for environment interaction. Through the trainer interface, the policy can be trained, checkpointed, or an action computed. In multi-agent training, the trainer manages the querying and optimization of multiple policies at once.\n",
    "\n",
    "![RLlib API](../images/rllib/RLlib-API.svg)\n",
    "\n",
    "the trainer classes coordinate the distributed workflow of running rollouts and optimizing policies. They do this by leveraging Ray [parallel iterators](https://docs.ray.io/en/latest/iter.html) (see also this lesson: [Ray Crash Course: 05 Ray Parallel Iterators](../ray-crash-course/05-Ray-Parallel-Iterators.ipynb)) to implement the desired computation pattern. The following figure shows *synchronous sampling*, the simplest of [these patterns](https://docs.ray.io/en/latest/rllib-algorithms.html):\n",
    "\n",
    "![Synchronous Sampling](../images/rllib/a2c-arch.svg)\n",
    "\n",
    "    Synchronous Sampling (e.g., A2C, PG, PPO)\n",
    "\n",
    "RLlib uses [Ray actors](https://docs.ray.io/en/latest/actors.html) to scale training from a single core to many thousands of cores in a cluster. You can [configure the parallelism](https://docs.ray.io/en/latest/rllib-training.html#specifying-resources) used for training by changing the `num_workers` parameter. Check out the [scaling guide](https://docs.ray.io/en/latest/rllib-training.html#scaling-guide) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies \n",
    "\n",
    "Each policy implementation defines a `learn_on_batch()` method that improves the policy given a sample batch of input. For TensorFlow and PyTorch policies, this is implemented using a _loss function_ that takes as input sample batch tensors and outputs a scalar loss value. Here are a few example loss functions:\n",
    "\n",
    "* Simple [policy gradient loss](https://github.com/ray-project/ray/blob/master/rllib/agents/pg/pg_tf_policy.py).\n",
    "* Simple [Q-function loss](https://github.com/ray-project/ray/blob/a1d2e1762325cd34e14dc411666d63bb15d6eaf0/rllib/agents/dqn/simple_q_policy.py#L136)\n",
    "* Importance-weighted _APPO surrogate loss_ for [TensorFlow](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_tf_policy.py), [PyTorch](https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo_torch_policy.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Data\n",
    "\n",
    "Beyond environments defined in Python,  batch training on [offline datasets](https://docs.ray.io/en/latest/rllib-offline.html) is supported. This is an important use case for RL when it's not possible to run traditional training and rollout in a physical environment (like a chemical plant or assembly line) and a suitable simulator doesn't exist. In this approach, data for past activity is used to train a policy.\n",
    "\n",
    "This is sometimes called [imitation learning](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil).\n",
    "\n",
    "## Application Support and Customization\n",
    "\n",
    "[RLlib supports]((https://docs.ray.io/en/latest/rllib.html#application-support)) a variety of integration strategies for [external applications](https://docs.ray.io/en/latest/rllib-env.html#external-agents-and-applications).\n",
    "\n",
    "RLlib provides ways to customize almost all aspects of training, including the [environment](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments), [neural network model](https://docs.ray.io/en/latest/rllib-models.html#tensorflow-models), [action distributions](https://docs.ray.io/en/latest/rllib-models.html#custom-action-distributions), and [policy definitions](https://docs.ray.io/en/latest/rllib-concepts.html#policies>).\n",
    "\n",
    "![RLlib components](../images/rllib/RLlib-components.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Implemented in RLlib\n",
    "\n",
    "Here is the current list of supported algorithms in RLlib. The links go to the corresponding RLlib documentation, which includes links to the original papers and other references.\n",
    "\n",
    "In this tutorial, we will mostly use [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo), [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn), and the contextual bandit algorithms, [Linear Upper Confidence Bound (LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb) and [Linear Thompson Sampling (LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints).\n",
    "\n",
    "See also the documentation's [Feature Compatibility Matrix](https://docs.ray.io/en/latest/rllib-algorithms.html#feature-compatibility-matrix), which lists the algorithms and useful properties for them. It appears at the beginning of the descriptions of all the algorithms, with links to the research papers that introduced them and discussions of their strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-throughput Architectures\n",
    "\n",
    "* [Distributed Prioritized Experience Replay (Ape-X)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#distributed-prioritized-experience-replay-ape-x)\n",
    "* [Importance Weighted Actor-Learner Architecture (IMPALA)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#importance-weighted-actor-learner-architecture-impala)\n",
    "* [Asynchronous Proximal Policy Optimization (APPO)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#asynchronous-proximal-policy-optimization-appo)\n",
    "* [Decentralized Distributed Proximal Policy Optimization (DD-PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based\n",
    "\n",
    "* [Advantage Actor-Critic (A2C, A3C)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-actor-critic-a2c-a3c)\n",
    "* [Deep Deterministic Policy Gradients (DDPG, TD3)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-deterministic-policy-gradients-ddpg-td3)\n",
    "* [Deep Q Networks (DQN, Rainbow, Parametric DQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)\n",
    "* [Policy Gradients](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#policy-gradients)\n",
    "* [Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo)\n",
    "* [Soft Actor-Critic (SAC)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#soft-actor-critic-sac)\n",
    "* [Model-Agnostic Meta-Learning (MAML)](https://docs.ray.io/en/latest/rllib-algorithms.html#model-agnostic-meta-learning-maml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative-free\n",
    "\n",
    "* [Augmented Random Search (ARS)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#augmented-random-search-ars)\n",
    "* [Evolution Strategies](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#evolution-strategies)\n",
    "* [QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#qmix-monotonic-value-factorisation-qmix-vdn-iqn)\n",
    "* [Multi-Agent Deep Deterministic Policy Gradient (contrib/MADDPG)](https://docs.ray.io/en/latest/rllib-algorithms.html#multi-agent-deep-deterministic-policy-gradient-contrib-maddpg)\n",
    "* [Advantage Re-Weighted Imitation Learning (MARWIL)](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#advantage-re-weighted-imitation-learning-marwil)\n",
    "* [Single-Player Alpha Zero (contrib/AlphaZero)](https://docs.ray.io/en/latest/rllib-algorithms.html#single-player-alpha-zero-contrib-alphazero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Bandits (contrib/bandits)\n",
    "\n",
    "* [Linear Upper Confidence Bound (contrib/LinUCB)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-upper-confidence-bound-contrib-linucb)\n",
    "* [Linear Thompson Sampling (contrib/LinTS)](https://docs.ray.io/en/latest/rllib-algorithms.html#linear-thompson-sampling-contrib-lints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Overview](00-Ray-RLlib-Overview.ipynb) for recommendations on which lessons to study next."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
