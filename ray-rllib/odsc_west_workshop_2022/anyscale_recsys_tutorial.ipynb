{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademyLogo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Notebook for ODSC West 2022\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn about:\n",
    " * [Defining a MDP for recommendation system](#recsys) -10 min\n",
    " * [Offline Bandit](#offline-bandit) -10min\n",
    " * [Offline RL](#offline-rl) -10 min\n",
    "\n",
    "[Link to slides](https://github.com/anyscale/academy/blob/main/ray-rllib/odsc_west_workshop_2022/slides.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for running on anyscale\n",
    "#import os; os.environ[\"PATH\"] = \"/home/ray/anaconda3/bin:\" + os.environ[\"PATH\"]\n",
    "#!pip uninstall -y torch\n",
    "#!python -m pip install torch==1.12.1\n",
    "#!python -m pip install seaborn\n",
    "#import torch\n",
    "#print(f\"torch: {torch.__version__}\")\n",
    "\n",
    "#!pip uninstall -y -r matplotlib\n",
    "#!python3 -m pip install matplotlib==3.5.3\n",
    "#import matplotlib\n",
    "#print(f\"matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from rllib_recsim.utils import pretty_print_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune, rllib, data, air\n",
    "from ray.tune import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recsim \n",
    "# import custom long-term satisfaction recommendation system gym env\n",
    "from rllib_recsim.rllib_recsim import ModifiedLongTermSatisfactionRecSimEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecSim <a class=\"anchor\" id=\"recsys\"></a>\n",
    "\n",
    "In this notebook, we will use data generated with <b><a href=\"https://github.com/google-research/recsim\">Google's RecSim environment</a></b>, which was developed for the YouTube recommendation problem.  It is a configurable environment, where ideally you would plug in your own users, products, and embedding features.\n",
    "\n",
    "**Some further readings**\n",
    "\n",
    "- <a href=\"https://github.com/google-research/recsim\">RecSim github</a>\n",
    "- <a href=\"https://arxiv.org/pdf/1909.04847.pdf\">RecSim paper</a>\n",
    "\n",
    "The following image depicts all the components of the recsim packages:\n",
    "\n",
    "<img src=\"./images/recsim_environment.png\" width=\"70%\" />\n",
    "\n",
    "The environment is <i>Timelimit-based</i>, meaning the termination condition for an episode will be after a fixed number (10) of videos are watched. \n",
    "\n",
    "### Document Model\n",
    "\n",
    "<img src=\"./images/document_model.png\" width=\"70%\" />\n",
    "\n",
    "Documents represent the candidate pool of items that need to be recommended with features sampled in the range [0, 1].  In this tutorial, we use <b>1 single feature \"sweetness\"</b> drawn from a uniform distribution between [0.8, 1.0] to represent \"chocolaty\" items and [0, 0.2] for the \"kaley\" options. \n",
    "- The documents can be different at each step (produced by some \"candidate generation\" process), or fixed throughout the simulation.\n",
    "- The recommendation algorithm observes the D candidate documents.  It then makes a selection (possibly ordered) of k documents and presents them in a \"slate\" to the user. We will focus on **slate size of 1** in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedLongTermSatisfactionRecSimEnv()\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed environment shows the main RecSim environment which follows [gym API](https://www.gymlibrary.dev/api/core/). For more info on how the environment is actually implemented we refer you to the source code acompanied with this tutorial [link](https://github.com/anyscale/academy/blob/main/ray-rllib/acm_recsys_tutorial_2022/rllib_recsim/rllib_recsim.py).\n",
    "\n",
    "Let's use `env.reset()` to begin the interaction with the environment:\n",
    "\n",
    "```\n",
    "gym.Env.reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) ‚Üí Tuple[ObsType, dict]\n",
    "Resets the environment to an initial state and returns the initial observation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Model\n",
    "\n",
    "<img src=\"./images/user_model.png\" width=\"70%\" />\n",
    "\n",
    "In RecSim users are representation by a set of features some of which could be latent hidden variables not observed by our recommendation system during live interaction. In this tutorial we assume that a sampled user from the world does not own any observable features like age, gender, etc. and our AI should infer the latent state of the user from its history of interactions in the current session. \n",
    "- The user examines a \"slate\" of recommended items and makes a choice of one item. After making their choice, the user emits an engagement score which indicates how much that user was engaged with that particular item they chose. The agent has to learn to estimate the latent states of the user that shape their choice model in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observable user features\n",
    "obs['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent user features (HACK: we can cheat and access the user state within the env)\n",
    "pprint(env.get_user_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Choice\n",
    "\n",
    "<img src=\"./images/user_choice_model.png\" width=\"70%\" />\n",
    "\n",
    "This module controls how a particular user would **respond** to a set of recommendations and how its **latent state would evolve** as a function of this interaction. \n",
    "In the environment in this tutorial, engagement is assumed to be a function of two competing phenomenas:\n",
    "-   The love of the user for sweet items ($sweetness(item_t)$)\n",
    "-   The long term satisfaction which cares about healthier options. It is inversely correlated with the sweetness of items suggested so far ($satisfaction_{t-1}$)\n",
    "\n",
    "$$satisfaction_t := satisfaction_{t-1} * \\sigma(-sweetness(item_{t}))$$\n",
    "$$r(item_t) \\propto sweetness(item_t) * satisfaction_{t-1}$$\n",
    "\n",
    "i.e. If a user who loves chocolates have not had chocolate for a while, a chocolate item would be more engaging than a kaley item. On the other hand if we keep recommending chocolate to the same person, they may lose interest and not use our recommendations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space is a dictionary of four keys. \n",
    "- The `user` key is empty, because there is no observable user features. \n",
    "- The `doc` key contains a set of documents presented with numerical keys. Each doc item has a scalar score representing its sweetness. \n",
    "- The `response` key includes a single record of `click` and `engagement` from the immediate previous interaction. \n",
    "- The `time` shows a normalized timestep within the session for this user. -0.5 corresponds to the beginnig of the interaction and +0.5 corresponds to the end of the interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now checkout environment's reward behavior. Let's see what happens if we always pick the sweetest item and plot the reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's checkout the reward space\n",
    "obs = env.reset()\n",
    "rewards = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = int(max(obs['doc'], key=lambda x: obs['doc'][x]))\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('engagement per step for maximum greedy policy')\n",
    "plt.xlabel('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note a couple of things here already:\n",
    "\n",
    "1. The immediate engagement would be high initially when the sweetest item is suggested for the first time.\n",
    "2. As we keep recommending the sweetest items, the user satisfaction significantly tampers off and as a result engagement quickly drops.\n",
    "3. Episodes seem to last for 10 timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise (2 min):\n",
    "Instead of picking the item with the highest feature, pick the item with the lowest feature and see what happens?\n",
    "\n",
    "- What do your observations imply about this environment?\n",
    "- What policy maximizes engagement with the user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's checkout the reward space\n",
    "obs = env.reset()\n",
    "rewards = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = # Write your own strategy here.\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel('engagement per step for minimum greedy policy')\n",
    "plt.xlabel('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting some baselines on trying to maximize engagement\n",
    "Next we will run some simple baselines to get a feeling of the reward we can accumulate in these environments using simple policies.\n",
    "\n",
    "- Greedy minimum feature value (recommending the kaliest option)\n",
    "- Greedy maximum feature value (recommending the chocoletiest option)\n",
    "- random policy (recommending random items from the pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This computes  the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def calc_baseline(baseline_type=\"random\",\n",
    "                  episodes=100):\n",
    "\n",
    "    env = ModifiedLongTermSatisfactionRecSimEnv()\n",
    "    # Reset the env.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    epsiode_satisfaction = []\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "    episode_satisfactions = []\n",
    "    \n",
    "    # Enter while loop (to step through the episode).\n",
    "    time_step = 0\n",
    "    while num_episodes < episodes:\n",
    "        # Produce an action\n",
    "        random_action = env.action_space.sample()\n",
    "        argmax_action = int(max(obs['doc'], key=lambda x: obs['doc'][x]))\n",
    "        argmin_action = int(min(obs['doc'], key=lambda x: obs['doc'][x]))\n",
    "\n",
    "        action_dict = {\n",
    "            'argmax': argmax_action, # greedy choc\n",
    "            'argmin': argmin_action, # greedy kale\n",
    "            'random': random_action,\n",
    "        }\n",
    "\n",
    "        action = action_dict[baseline_type]\n",
    "        \n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Accumulate the rewards\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Append satisfaction to episode_satiscation\n",
    "        epsiode_satisfaction.append(\n",
    "            env.environment._user_model._user_state.satisfaction\n",
    "        )\n",
    "\n",
    "        time_step += 1\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if num_episodes % 99 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 9 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "                \n",
    "            # increment on end of episode\n",
    "            num_episodes += 1\n",
    "            time_step = 0\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "            episode_satisfactions.append(np.mean(epsiode_satisfaction))\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_reward = np.mean(episode_rewards)\n",
    "    env_sd_reward = np.std(episode_rewards)\n",
    "\n",
    "    # Print out the satisfaction over the episodes\n",
    "    env_mean_satisfaction = np.mean(episode_satisfactions)\n",
    "    env_sd_satisfaction = np.std(episode_satisfactions)\n",
    "    \n",
    "    print(f\"\\nMean {baseline_type} baseline reward: {env_mean_reward:.2f}+/-{env_sd_reward:.2f}, satisfaction: {env_mean_satisfaction:.2f}+/-{env_sd_satisfaction:.2f}\")\n",
    "\n",
    "    return env_mean_reward, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "kaliest_baseline, _ = calc_baseline(baseline_type=\"argmin\", episodes=num_episodes)\n",
    "sweetest_baseline,  _ = calc_baseline(baseline_type=\"argmax\", episodes=num_episodes)\n",
    "random_baseline, _ = calc_baseline(baseline_type=\"random\", episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion about the baselines\n",
    "\n",
    "For every baseline we have printed out not only the engagement score of the entire user session but also the average of the satisfaction term over the entire session as well.\n",
    "\n",
    "The question is whether we automatically learn an optimal policy in this recommendation enviornement that is better than random?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions (2 min)\n",
    "\n",
    "- Any questions so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline RL with RecSys <a class=\"anchor\" id=\"offline-rl\"></a>\n",
    "\n",
    "<img src=\"images/offline_rl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### If we don't have a live environment, how do we know, how well our trained policy will perform?\n",
    "\n",
    "One of the challenges in offline RL is the evaluation of the trained policy. In online RL (when a simulator\n",
    "is available), one can either use the data collected for training to compute episode total rewards. Remember\n",
    "that observations, actions, rewards, and done flags are all part of this training data. Alternatively,\n",
    "one could run a separate worker (with the same trained policy) and run it on a fresh evaluation-only environment.\n",
    "In this latter case, we would also have the chance to switch off any exploratory behavior (e.g. stochasticity used\n",
    "for better action entropy).\n",
    "\n",
    "In offline RL, no such data from a live environment is available to us. There are two common ways of addressing this dilemma:\n",
    "\n",
    "1) We deploy the learned policies into production, or maybe just a portion of our production system (similar to A/B testing), and see what happens.\n",
    "\n",
    "2) We use a method called \"off policy evaluation\" (OPE) to compute an estimate on how the new policy would perform if we were to deploy it into a real environment. There are different OPE methods available in RLlib off-the-shelf.\n",
    "\n",
    "3) The third option - which we will use here - is kind of cheating and only possible if you actually do have a simulator available (but you only want to use it for evaluation, not for training, because you want to see how cool offline RL is :) )\n",
    "\n",
    "In this tutorial, we will use the third option to show the effectiveness of offline RL in improving over existing policies running in production. We will also see how much benefit we can get by improving our dataset quality, starting from a totally random policy all the way to 20% expert demonstrations adn 80% random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the currently running policies in production to collect some \"historical data\" that we can use to train RL agents with. Offline RL can be used to improve upon the existing policies deployed in production. We have prepared some datasets in advance for the purpose of this tutorial. They were all generated using `<path to the script>`. In this script we can mix the percentage of the \"expert\" data vs. random data to investigate the effect of dataset quality on the final performance of our models.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an exemplar dataset we have prepared before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"s3://air-example-data/rllib/offline_rl_recsim_data/\"\n",
    "train_data_path = prefix + \"sampled_data_train_random_transitions_small\"\n",
    "\n",
    "dset = data.read_json(train_data_path)\n",
    "df = dset.to_pandas()\n",
    "print('Colimns: ', df.columns)\n",
    "print('Number of rows: ', len(df))\n",
    "print('Value of the first row')\n",
    "print('-'*20)\n",
    "print(df.iloc[0])\n",
    "print('Value of the second row:')\n",
    "print('-'*20)\n",
    "print(df.iloc[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset schema, we can see that RLlib always expects a `type` column that is `SampleBatch`. It will have the normal transition entities per each row (i.e. observation, next_observation, action, reward, done values). It will also contain an episode_id, a timestep indicator, and an action_prob that show the probablity of the action that we chosen at the time of data collection. For random policy the action prob will always be 1/20 (0.05). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understaning of the dataset example format, we can use RLlib to train an offline RL algorithm. RLlib provides several out of the box offline RL algorithms that you can use. Beside those offline-RL-specific algorithms, we can also use any off-policy algorithm (e.g. DQN) to do offline-RL. The only difference between online and offline version is that instead of using an enviornement sampler, we use a dataset sampler to get the data from. In the next section, we will use DQN, with the difference that we pass a dataset path to the input config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Bandit\n",
    "\n",
    "Below is an example script that trains a DQN bandit agent using the dataset we looked at above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQN, DQNConfig\n",
    "\n",
    "register_env(\"modified-lts\", lambda config: ModifiedLongTermSatisfactionRecSimEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedLongTermSatisfactionRecSimEnv()\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "bandit_config_offline = (\n",
    "    DQNConfig()\n",
    "    .framework(\"torch\")\n",
    "    .offline_data(\n",
    "        input_='dataset',\n",
    "        input_config={\n",
    "            'format': 'json',\n",
    "            'paths': train_data_path,\n",
    "        }\n",
    "    )\n",
    "    .environment(\n",
    "        action_space=action_space,\n",
    "        observation_space=observation_space,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_config={\n",
    "            \"input\": \"sampler\",\n",
    "            \"explore\": False,\n",
    "            \"env\": \"modified-lts\",\n",
    "        },\n",
    "    )\n",
    "    .training(gamma=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanations of the script above:\n",
    "\n",
    "- Input is configured by `.offline_data()` API:\n",
    "\n",
    "```python\n",
    "    .offline_data(\n",
    "        input_='dataset',\n",
    "        input_config={\n",
    "            'format': 'json',\n",
    "            'paths': train_data_path\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "- The environment is not passed to the enviornement. Instead we pass in the expected action and observation space to construct the policies. We can create them manually based on our knowledge of our system. In this case we actually cheat and use the environment attributes to get the correct action and observatin spaces.\n",
    "\n",
    "```python \n",
    "    .environment(\n",
    "        action_space=action_space,\n",
    "        observation_space=observation_space,\n",
    "    )\n",
    "```\n",
    "\n",
    "- evaluation config: Since during the evaluation we still need to use the enviroenement simulations, we need to specify it explicitly here.\n",
    "\n",
    "```python\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_config={\n",
    "            \"input\": \"sampler\",\n",
    "            \"explore\": False,\n",
    "            \"env\": \"modified-lts\"\n",
    "        },\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_tuner = tune.Tuner(\n",
    "    DQN,\n",
    "    param_space=bandit_config_offline.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        local_dir=\"./results_notebook/offline_bandits/\",\n",
    "        stop={\"training_iteration\": 30},\n",
    "    )\n",
    ")\n",
    "offline_bandits_results = bandit_tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the offline training results. From the plot below we can see that by running offline RL on randomly collected transitions we can improve over the random policy. This is extremely useful in practical scenarios where our goal is to improve over existing production policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Bandit Episode reward:')\n",
    "offline_bandits_results[0].metrics['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL\n",
    "\n",
    "Now let's make a simple change to train the same DQN recommendation agent using offline RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_config_offline = (\n",
    "    DQNConfig()\n",
    "    .offline_data(\n",
    "        input_='dataset',\n",
    "        input_config={\n",
    "            'format': 'json',\n",
    "            'paths': train_data_path,\n",
    "        }\n",
    "    )\n",
    "    .environment(\n",
    "        action_space=action_space,\n",
    "        observation_space=observation_space,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_config={\n",
    "            \"input\": \"sampler\",\n",
    "            \"explore\": False,\n",
    "            \"env\": \"modified-lts\",\n",
    "        },\n",
    "    )\n",
    "    .training(gamma=0.99)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tuner = tune.Tuner(\n",
    "    DQN,\n",
    "    param_space=dqn_config_offline.to_dict(),\n",
    "    run_config=air.RunConfig(\n",
    "        local_dir=\"./results_notebook/offline_rl/\",\n",
    "        stop={\"training_iteration\": 30},\n",
    "    )\n",
    ")\n",
    "offline_rl_results = dqn_tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean DQN Episode reward:')\n",
    "offline_rl_results[0].metrics['evaluation']['episode_reward_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# plot the results and compare to baselines\n",
    "online_rl_df = pd.read_csv(\"saved_runs/online_rl/progress.csv\")\n",
    "offline_rl_df = pd.read_csv(\"saved_runs/offline_rl/progress.csv\")\n",
    "offline_bandits_df = pd.read_csv(\"saved_runs/offline_bandits/progress.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=online_rl_df, x=\"training_iteration\", y=\"evaluation/episode_reward_mean\", label=\"Online_DQN\")\n",
    "sns.lineplot(data=offline_rl_df, x=\"training_iteration\", y=\"evaluation/episode_reward_mean\", label=\"Offline_DQN\")\n",
    "sns.lineplot(data=offline_bandits_df, x=\"training_iteration\", y=\"evaluation/episode_reward_mean\", label=\"Offline_Bandits\")\n",
    "plt.axhline(random_baseline, color=\"red\", linestyle='--', label=\"random baseline\")\n",
    "plt.legend()\n",
    "plt.title('Offline RL vs. Baselines training performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Bandits converges to a short-sighted solution\n",
    "    -  Optimizes imediate reward. \n",
    "- DQN takes long-term reward into account\n",
    "    -  Achieves a policy better than random.\n",
    "- Offline RL is a viable option that works when we don't have simulators for training\n",
    "    -  Since it doesn't explore freely it won't perform as good as an online RL method. \n",
    "    -  Its performance is bounded to the quality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. üìñ [Ray summit tutorials on RLlib](https://github.com/anyscale/ray-summit-2022-training/tree/main/ray-rllib)\n",
    "2. üë©‚Äç [RLlib Documentation](https://docs.ray.io/en/latest/rllib/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!\n",
    "\n",
    "<a href=\"https://bit.ly/rllib_odsc_west_survey\">Survey</a> - Please let us know how useful you have found this workshop.\n",
    "\n",
    "**We would love to connect with you!**\n",
    "\n",
    "**Twitter** - @anyscalecompute | @raydistributed <br>\n",
    "<b><a href=\"https://github.com/ray-project/ray\">Github</a></b> - üòú give us a star!<br>\n",
    "<b><a href=\"https://www.ray.io/community\">Slack</a></b> - [+invitation link](https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform)<br>\n",
    "<b><a href=\"https://discuss.ray.io/\">Discuss</a></b> - searchable questions <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d33966b1efccf1b078b90327a99b4ed3697643ca9825566d91b27949ee9bb425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
