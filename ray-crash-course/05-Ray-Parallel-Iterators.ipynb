{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Crash Course - Ray Parallel Iterators (Now deprecated in Ray 1.7)\n",
    "\n",
    "**NOTE**: As of Ray release 1.7 `ray.util.iter` has been deprecated\n",
    "\n",
    "© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademyLogo.png)\n",
    "\n",
    "This lesson explores `ray.util.iter` ([documentation](https://docs.ray.io/en/latest/iter.html)), which provides a parallel iterator API for simple data ingest and processing. It can be thought of as syntactic sugar around Ray actors and `ray.wait` loops.\n",
    "\n",
    "Parallel iterators are lazy, so they can operate over infinite sequences of items. Iterator transformations are only executed when the user calls `next()` to fetch the next output item from the iterator.\n",
    "\n",
    "So, parallel iterators provide a simple, yet powerful API for stream processing.\n",
    "\n",
    "> **Tip:** For more about Ray, see [ray.io](https://ray.io) or the [Ray documentation](https://docs.ray.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray, time, sys, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ray Dashboard, if you are running this notebook on a local machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Iterators\n",
    "\n",
    "You can create a `ParallelIterator` object from an existing set of items, range of numbers, set of iterators, or set of worker actors ([reference documentation](https://docs.ray.io/en/latest/iter.html#module-ray.util.iter)). \n",
    "\n",
    "Ray will create a worker actor that produces the data for each shard of the iterator. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator with 2 worker actors over the list [1, 2, 3, 4].\n",
    "it1 = ray.util.iter.from_items([1, 2, 3, 4], num_shards=2)\n",
    "it1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator with 8 worker actors over range(1000000).\n",
    "it2 = ray.util.iter.from_range(1000000, num_shards=8)\n",
    "it2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator over three range(10) generators. How many shards are created?\n",
    "it3 = ray.util.iter.from_iterators([range(10), range(10), range(10)])\n",
    "it3.shards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use Actors, but they must subclass [ParallelIteratorWorker](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIteratorWorker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.iter import ParallelIteratorWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class IterableWorker(ParallelIteratorWorker):\n",
    "    # Must call the parent constructor as shown (see also the docs linked above)\n",
    "    def __init__(self):\n",
    "        super().__init__(item_generator = lambda: np.random.randint(255, 100), repeat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator from existing worker actors. These actors must\n",
    "# implement the ParallelIteratorWorker interface.\n",
    "\n",
    "# Using comprehension, create a list of four instances of\n",
    "# IterableWorker\n",
    "iws = [IterableWorker.remote() for _ in range(4)]\n",
    "\n",
    "# Create a Parallel iterator from list of IterableWorkers\n",
    "it4 = ray.util.iter.from_actors(iws)\n",
    "it4.shards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use some of these iterators in subsequent cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Iterators\n",
    "\n",
    "To read elements from a parallel iterator, it has to be converted to a [LocalIterator](https://docs.ray.io/en/latest/iter.html#ray.util.iter.LocalIterator) by calling [gather_sync](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_sync) or [gather_async](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.gather_async). These correspond to `ray.get` and `ray.wait` loops over the actors respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather items synchronously (deterministic round robin across shards). \n",
    "# Use the it3 = ray.util.iter.from_iterators([range(10), range(10), range(10)]) iterator above:\n",
    "local_it3 = it3.gather_sync()\n",
    "local_it3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local iterators can be used like any other Python iterator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_it3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_it3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you understand what's happening in the two previous cells? To get 10 total elements, the iterator returns elements from every shard, cycling through them as required. Hence, four are returned from one shard and three from the other two shards. We'll discuss ordering and semantics in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Simple transformations can be chained on the iterator, such as mapping, filtering, and batching. These will be executed in parallel on the workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a \"map\" transformation to each element of the iterator.\n",
    "it1_foreach = it1.for_each(lambda x: x ** 2)\n",
    "it1_foreach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the information printed for `it1_foreach`. We'll keep printing these iterators going forward so you can see how each one is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it1_foreach.gather_sync().take(10)  # 10 is too high, there are only 4, but that's fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to batch elements together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch together items into a lists of 5 elements.\n",
    "it2_batch5 = it2.batch(5)#.for_each(lambda list: \"|\".join(list))\n",
    "it2_batch5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_batch5.gather_sync().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order used to sequence operations matters. Notice what's different about the next two pairs of cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_batch5a = it2.batch(5).for_each(lambda list: sum(list))\n",
    "it2_batch5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_batch5a.gather_sync().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_batch5b = it2.for_each(lambda x: 2*x).batch(5)\n",
    "it2_batch5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_batch5b.gather_sync().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter values. Next we filter to _keep_ even values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_evens = it2.filter(lambda x: x % 2 == 0) \n",
    "it2_evens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it2_evens.batch(5).gather_sync().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes:**\n",
    "> \n",
    "> * Transformations used _before_ the call to `gather_sync()` run in parallel on shards using the `ParallelIterator`. \n",
    "> * Transformations used _after_ the call to `gather_sync()` run in the current process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The async gather, `gather_async`, can be used for better performance, but it is non-deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_async = ray.util.iter.from_range(100, 4).gather_async()\n",
    "it_async.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Iterators to Remote Functions\n",
    "\n",
    "Both `ParallelIterator` and `LocalIterator` are serializable. They can be passed to any Ray remote function. However, note that each shard should only be read by one process at a time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can get local iterators representing the shards of a `ParallelIterator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = ray.util.iter.from_range(1000, 3)\n",
    "[s0, s1, s2] = it.shards()\n",
    "[s0, s1, s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterator shards can be passed to remote functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def do_sum(it):\n",
    "    return sum(it)\n",
    "\n",
    "ray.get([do_sum.remote(s) for s in it.shards()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Semantic Guarantees\n",
    "\n",
    "The parallel iterator API guarantees the following semantics:\n",
    "\n",
    "### Fetch Ordering\n",
    "\n",
    "When using `it.gather_sync().for_each(fn)`, `it.gather_async().for_each(fn)`, or any other transformation after a `gather_*sync`, for a sequence of elements served by a source actor, the function `fn` will be called on those elements in order. In particular, it will be called for element _i_ in the sequence before the next element, _i+1_, is fetched from the source actor. This is useful if you need to update the source actor between iterator steps. Note that for async gather, this ordering only applies _per shard_.\n",
    "\n",
    "### Operator State\n",
    "\n",
    "Operator state is preserved for each shard. This means that you can pass a stateful callable to `for_each()`, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumulativeList:\n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.list.append(x)\n",
    "        return (x, self.list)\n",
    "\n",
    "it = ray.util.iter.from_range(5, 1)\n",
    "for x in it.for_each(CumulativeList()).gather_sync():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on Parallel Iterators, see the [API reference](https://docs.ray.io/en/latest/iter.html#module-ray.util.iter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Here are two, more complete examples. \n",
    "\n",
    "### Passing Iterator Shards to Remote Functions\n",
    "\n",
    "Both parallel iterators and local iterators are fully serializable, so once created you can pass them to Ray tasks and actors. This can be useful for distributed training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train(data_shard):\n",
    "    for batch in data_shard:\n",
    "        # simulate performing a model update with a batch of data\n",
    "        print(\"train on\", batch)  \n",
    "\n",
    "def train_model(range, batch, num_shards, repeat):\n",
    "    train_iter = (\n",
    "        ray.util.iter.from_range(range, num_shards=num_shards, repeat=repeat)\n",
    "            .batch(batch)\n",
    "            .for_each(np.array)\n",
    "    )\n",
    "\n",
    "    work = [train.remote(shard) for shard in train_iter.shards()]\n",
    "    ray.get(work)\n",
    "\n",
    "train_model(range=1000, batch=250, num_shards=4, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Word Count\n",
    "\n",
    "_Word count_ was called the \"Hello World\" of Hadoop programming, because it's a conceptually simple algorithm that illustrates the _map/reduce_ paradigm of Hadoop programming very well and it was often the first program written by developers learning about Hadoop. \n",
    "\n",
    "_Word count_ is tedious to write in the original `MapReduce` API, but elegant in [Apache Spark](https://spark.apache.org), which replaced `MapReduce`. Our implementation here is similar to how you would implement it in Spark. The key is to have a powerful, composable set of operations, sometimes called \"operators\" (in the spirit of addition, multiplication, etc.) or [_combinators_](https://en.wikipedia.org/wiki/Combinatory_logic).\n",
    "\n",
    "In _word count_, a corpus of documents is read in parallel processes, where each document is tokenized into words, and the occurrences of each word are counted, then combined into a global word-count dictionary. Usually the results are sorted by frequency of occurrence, descending. The idea is that the most common words are indicative of the major themes of the corpus. Slightly more sophisticated algorithms are [n-grams](https://en.wikipedia.org/wiki/N-gram), which count the short `n`-word phrases, and [inverted index](https://en.wikipedia.org/wiki/Inverted_index), which builds a dictionary of words and the locations the words were found, the basis of a search engine!\n",
    "\n",
    "> **Note:** This code is also available as a complete, standalone example in [word-count.py](word-count.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, gzip, re\n",
    "\n",
    "class WordCount:\n",
    "    \"Wraps a dictionary of words and counts.\"\n",
    "    def __init__(self):\n",
    "        self.counts = {}\n",
    "\n",
    "    def __call__(self, word, increment):\n",
    "        count = increment\n",
    "        if word in self.counts:\n",
    "            count = self.counts[word]+increment\n",
    "        self.counts[word] = count\n",
    "        return (word, count)\n",
    "\n",
    "    def sort_counts(self, descending=True):\n",
    "        \"Returns a generator of word-count pairs sorted by count.\"\n",
    "        return (wc for wc in sorted(self.counts.items(), key = lambda wc: wc[1], reverse=descending))\n",
    "\n",
    "def unzip(f):\n",
    "    if f.endswith(\".gz\"):\n",
    "        return gzip.open(f)\n",
    "    else:\n",
    "        return open(f, 'r')\n",
    "\n",
    "def count_words(file_globs, top_n = 100, batch_window = 1024):\n",
    "    # The working directory of this application may be _different_\n",
    "    # than the Ray cluster's working directory. (In a real cluster,\n",
    "    # the files available will be different, too, but we'll ignore\n",
    "    # the problem here.) So, we need to pass absolute paths or our\n",
    "    # ray.util.iter.from_items won't find the files!\n",
    "    globs = [g for f in file_globs for g in glob.glob(f)]\n",
    "    file_list = list(map(lambda f: os.path.abspath(f), globs))\n",
    "\n",
    "    print(f'Processing {len(file_list)} files: {file_list}')\n",
    "    # See also the combine operation, which is for_each(...).flatten(...).\n",
    "    word_count = (\n",
    "        ray.util.iter.from_items(file_list, num_shards=4)\n",
    "           .for_each(lambda f: unzip(f).readlines())\n",
    "           .flatten()  # converts one record per file with all lines to one record per line.\n",
    "           .for_each(lambda line: re.split('\\W+', line)) # split into words\n",
    "           .flatten()  # flatten lists of words into one word per record\n",
    "           .for_each(lambda word: (word, 1))\n",
    "           .batch(batch_window)\n",
    "    )\n",
    "    # Combine the dictionaries of counts across shards with a sliding window\n",
    "    # of \"batch_window\" lines.\n",
    "    wordCount = WordCount()\n",
    "    for shard_counts in word_count.gather_async():\n",
    "        for word, count in shard_counts:\n",
    "            wordCount(word, count)\n",
    "    sorted_list_iterator = wordCount.sort_counts()\n",
    "    return [sorted_list_iterator.__next__() for i in range(top_n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the current working directory used is actually the root of the project, so we have to use absolute paths or correct relative paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time words_counts = count_words(['*.ipynb'], top_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Let's explore _word count_. Do the first two exercises, then any of the rest of them that interest you.\n",
    "\n",
    "The solutions are in the [solutions](solutions/Ray-Crash-Course-Solutions.ipynb) notebook.\n",
    "\n",
    "As you do these exercises, pay attention to any perceived performance changes, better or worse.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Replace the combinations of `for_each().flatten()` with `combine()` ([documentation](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.combine)). In functional programming languages, this \"operator\" is usually called `flatmap`, because `for_each` as used here is called `map`, where the output of _each_ application of `for_each/map` on a single record returns a collection of new records, and the resulting collection of collections is flattened to a collection of new records.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "The same word may appear with different capitalization, e.g., at the beginning of a sentence. We don't want these occurrences counted separately (although this makes less sense when you are scanning source code!). Convert all words to lower case. When in the pipeline should this be done?\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "The most frequent \"words\" are actually single letters, like `n`, which is the most numerous across the notebooks in this directory. Can you figure out why there are so many `n`s? Hint: try this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('./01-Ray-Tasks.ipynb', 'r')\n",
    "[f.readline() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, the term _stop words_ is used for \"tokens\" that are not very useful for understanding, so they are filtered out. Add a `filter` step ([documentation](https://docs.ray.io/en/latest/iter.html#ray.util.iter.ParallelIterator.filter)) to remove a set of stop words that you define. Where is the best place to put this step? How would you represent the set of stop words and how would you filter them out?\n",
    "\n",
    "Once the stop words are removed, what are the most common words?\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Try running _word count_ on some of your own files. Adjust your list of stop words as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Homework\"\n",
    "\n",
    "Solutions are not provided in the solutions notebook for these suggested exercises.\n",
    "\n",
    "### Better Tokenization\n",
    "\n",
    "Our \"tokenization\" technique is naïve; we simply split on and discard non-alphanumeric characters. Try using a more sophisticated tokenizer. See this [Stack Overflow page](https://stackoverflow.com/questions/21361073/tokenize-words-in-a-list-of-sentences-python) for ideas.\n",
    "\n",
    "### N-Grams\n",
    "\n",
    "Try implementing the [N-grams](https://en.wikipedia.org/wiki/N-gram) algorithm, which counts the short `n`-word phrases. Why are they useful?\n",
    "\n",
    "### Inverted Index\n",
    "\n",
    "Try implementing the [Inverted index](https://en.wikipedia.org/wiki/Inverted_index) algorithm, which builds a dictionary of words and the locations the were found, the basis of a search engine! Takeover Google..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\". Terminate all the processes started in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray parallel iterators isn't a full-featured replacement for powerful systems like [Apache Spark](https://spark.apache.org), but it handles a lot of scenarios without having to leave Ray.\n",
    "\n",
    "The next lesson, [Exploring Ray API Calls](06-Exploring-Ray-API-Calls.ipynb) explores the other Ray API calls for more advanced scenarios, including keyword arguments you can pass to the API calls already learned are explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "382.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
